<!DOCTYPE html>
<html lang="en">

  <!-- Add this block before head.html include -->
<script>
  (function() {
    // Immediately set the theme before any content loads
    const savedTheme = localStorage.getItem('theme') || 'light';
    document.documentElement.setAttribute('data-theme', savedTheme);
    
    // Prevent Flash Of Incorrect Theme (FOIT)
    document.documentElement.style.visibility = 'hidden';
    
    window.addEventListener('DOMContentLoaded', function() {
      document.documentElement.style.visibility = '';
    });
  })();
</script>

<style>
  /* Critical CSS to prevent layout shift */
  html, body {
    overflow-x: hidden;
    width: 100%;
    max-width: 100%;
    margin: 0;
    padding: 0;
  }
  
  .wrapper {
    max-width: 710px;
    margin: 0 auto;
    padding: 0 15px;
    position: relative;
    left: 0;
  }
  
</style><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Evolution of LLMs | Pramod’s Blog</title>
<meta name="generator" content="Jekyll v3.10.0" />
<meta property="og:title" content="Evolution of LLMs" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="The landscape of language models(LMs) has evolved dramatically since the introduction of the Transformer architecture in 2017. Here we will explore the" />
<meta property="og:description" content="The landscape of language models(LMs) has evolved dramatically since the introduction of the Transformer architecture in 2017. Here we will explore the" />
<link rel="canonical" href="https://goyalpramod.github.io/blogs/evolution_of_LLMs/" />
<meta property="og:url" content="https://goyalpramod.github.io/blogs/evolution_of_LLMs/" />
<meta property="og:site_name" content="Pramod’s Blog" />
<meta property="og:image" content="https://goyalpramod.github.io/assets/blog_assets/evolution_of_llms/download.webp" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2025-06-21T06:30:00+00:00" />
<meta name="twitter:card" content="summary_large_image" />
<meta property="twitter:image" content="https://goyalpramod.github.io/assets/blog_assets/evolution_of_llms/download.webp" />
<meta property="twitter:title" content="Evolution of LLMs" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2025-06-21T06:30:00+00:00","datePublished":"2025-06-21T06:30:00+00:00","description":"The landscape of language models(LMs) has evolved dramatically since the introduction of the Transformer architecture in 2017. Here we will explore the","headline":"Evolution of LLMs","image":"https://goyalpramod.github.io/assets/blog_assets/evolution_of_llms/download.webp","mainEntityOfPage":{"@type":"WebPage","@id":"https://goyalpramod.github.io/blogs/evolution_of_LLMs/"},"publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"https://goyalpramod.github.io/assets/images/img.webp"}},"url":"https://goyalpramod.github.io/blogs/evolution_of_LLMs/"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="https://goyalpramod.github.io/feed.xml" title="Pramod&apos;s Blog" /><!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-EW6BCL046P"></script>
<script>
  window['ga-disable-G-EW6BCL046P'] = window.doNotTrack === "1" || navigator.doNotTrack === "1" || navigator.doNotTrack === "yes" || navigator.msDoNotTrack === "1";
  window.dataLayer = window.dataLayer || [];
  function gtag(){window.dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-EW6BCL046P');
</script></head>
<!-- Add favicon right after head.html include -->
  <link rel="icon" type="image/png" href="/assets/icon.png">
  <link rel="shortcut icon" type="image/png" href="/assets/icon.png">
  <link rel="apple-touch-icon" href="/assets/icon.png">

  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Karla:ital,wght@0,200..800;1,200..800&family=Lora:ital,wght@0,400..700;1,400..700&family=Merriweather:ital,wght@0,300;0,400;0,700;0,900;1,300;1,400;1,700;1,900&family=Nunito:ital,wght@0,200..1000;1,200..1000&family=STIX+Two+Text:ital,wght@0,400..700;1,400..700&display=swap" rel="stylesheet">
  <link href="https://fonts.googleapis.com/css2?family=JetBrains+Mono:wght@400;500&display=swap" rel="stylesheet">
  <link href="https://fonts.googleapis.com/css2?family=Fira+Code:wght@400;500&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css" integrity="sha512-DTOQO9RWCH3ppGqcWaEA1BIZOC6xxalwEsw9c2QQeAIftl+Vegovlnee1c9QX4TctnWMn13TZye+giMm8e2LwA==" crossorigin="anonymous" referrerpolicy="no-referrer" />
  
  <style>
    body {
      font-family: 'Nunito', sans-serif;
      font-optical-sizing: auto;
      font-size: 18px;         
      font-weight: 400;        
      font-style: normal;
      line-height: 1.6; 
    }
    @media screen and (max-width: 768px) {
      body {
        font-size: 16px;      // Slightly smaller for mobile
      }
    }

    .post-content,
    .blog-post,
    .thought-post {
      p {
        margin-bottom: 1.5em;
        font-weight: 400;    
      }
      
      h1, h2, h3, h4, h5, h6 {
        font-weight: 600;   
        margin-top: 2em;
        margin-bottom: 0.8em;
      }
    }
  </style>

  <body class="preload"><header class="site-header">
  <!-- Add Google Analytics only in production --><!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-EW6BCL046P"></script>
<script>
  window['ga-disable-G-EW6BCL046P'] = window.doNotTrack === "1" || navigator.doNotTrack === "1" || navigator.doNotTrack === "yes" || navigator.msDoNotTrack === "1";
  window.dataLayer = window.dataLayer || [];
  function gtag(){window.dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-EW6BCL046P');
</script><!-- Add X-Frame-Options meta tag -->
  <meta http-equiv="X-Frame-Options" content="SAMEORIGIN">

  <div class="wrapper">
    <a class="site-title" href="/">Pramod's Blog</a>
    <nav class="site-nav">
      <input type="checkbox" id="nav-trigger" class="nav-trigger" />
      <label for="nav-trigger">
        <span class="menu-icon" aria-label="Menu">
          <svg viewBox="0 0 18 15" width="18px" height="15px" aria-hidden="true">
            <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
          </svg>
        </span>
      </label>

      <div class="trigger">
        <a class="page-link" href="/blog/">Blog</a>
        <a class="page-link" href="/thoughts/">Thoughts</a>
        <a class="page-link" href="/projects/">Projects</a>
        <a class="page-link" href="/resume/">CV</a>
      </div>
    </nav>
  </div>

  
  <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        processEscapes: true
      },
      svg: {
        fontCache: 'global'
      }
    };
  </script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  
</header><main class="page-content" aria-label="Content">
      <div class="wrapper">
        
<div class="featured-image-container">
  <img src="/assets/blog_assets/evolution_of_llms/download.webp" alt="Evolution of LLMs" class="featured-image">
</div>


<article class="blog-post">
  <h1>Evolution of LLMs</h1>
  <p class="meta">June 21, 2025</p><button id="toc-toggle" class="toc-toggle" aria-label="Toggle Table of Contents">
    <i class="fas fa-list-ul"></i>
  </button>
  
  <div class="toc-wrapper">
    <div class="toc">
      <div class="toc-header">
        Table of Contents
        <button class="toc-close" aria-label="Close Table of Contents">
          <i class="fas fa-times"></i>
        </button>
      </div>
      <nav class="toc-nav" id="toc">
        <!-- ToC content will be dynamically inserted here by JavaScript -->
      </nav>
    </div>
  </div>
  <p>The landscape of language models(LMs) has evolved dramatically since the introduction of the Transformer architecture in 2017. Here we will explore the</p>

<ul>
  <li>mathematical foundations</li>
  <li>architectural innovations</li>
  <li>training breakthroughs</li>
</ul>

<p>We will talk about everything the code, math, and ideas that revolutionized NLP.</p>

<p>Additionally you can treat this blog as a sort of part 2, to my original blog on transformers which you can checkout <a href="https://goyalpramod.github.io/blogs/Transformers_laid_out/">here</a>.</p>

<h2 id="how-this-blog-is-structured">How this blog is structured</h2>

<p>We will go year by year, going through the revolutionary ideas introduced by each paper.</p>

<p>In the beginning of each section, I have added the abstract, as well as the authors. I have done this to show you, the people were involved behind each idea. As well as what they felt like was the main contribution of their paper.</p>

<p>Below that I have provided the link to the original paper as well as my own implementation of it, subsequently there is a quick summary section which you can skim over if you feel like you know the crux behind the idea.</p>

<blockquote>
  <p>Note: All the quick summaries are AI generated, and may contain some mistakes. The core content is all human generated though, so it definitely contains mistakes :)</p>
</blockquote>

<p>After that, each section contains intuition, code, and mathematical explanation (wherever required) for each idea. I have tried to add all the prerequisite knowledge wherever possible (Like the PPO section contains derivation of policy gradient methods, as well as explanation for TRPO). I have provided links to resources wherever I have felt I cannot provide enough background or do sufficient justice to the source material.</p>

<p>Additionally there has been a lot of innovation in vision modeling, TTS, Image gen, Video gen etc each of which deserves it’s own blog(And there will be!! I promise you that). As this is primarily an LLM blog, I will just give quick intro and links to some ground breaking innovations involving other ML papers.</p>

<blockquote>
  <p>Note: Do not take for granted all the hardware, data and benchmark innovations. Though I will briefly mention them. I implore you to explore them further if they interest you. This blog is strictly restricted to breakthroughs in Large Language Models, and mostly open source one’s. Even though current models by OpenAI, Anthropic, Google etc are amazing, not much is known about them to the public. So we will only briefly talk about them.</p>
</blockquote>

<h2 id="the-ai-timeline">The AI timeline</h2>

<p>This is a timeline of the most influential work. To read about more architectures that were huge at the time but died down eventually, consider going through the <a href="https://docs.google.com/spreadsheets/d/1ltyrAB6BL29cOv2fSpNQnnq2vbX8UrHl47d7FkIf6t4/edit?gid=0#gid=0">Transformer catalog</a>.</p>

<p>The blog <a href="https://amatria.in/blog/transformer-models-an-introduction-and-catalog-2d1e9039f376/">“Transformer models: an introduction and catalog — 2023 Edition”</a> helped me immensely while making the timeline. Additionally this <a href="https://magazine.sebastianraschka.com/p/understanding-large-language-models">blog</a> was helpful too.</p>

<table>
  <tbody>
    <tr>
      <td>Links post 2019 are broken as it’s still work in progress</td>
    </tr>
  </tbody>
</table>

<details>
<summary>2017</summary>
<div>

    <ul>
      <li><a href="#transformer">Attention is all you need</a> <br /></li>
      <li><a href="#rlhf---reinforcement-learning-from-human-preferences">Deep reinforcement learning from human preferences</a> <br /></li>
      <li><a href="#ppo-proximal-policy-optimization"> Proximal Policy Optimization Algorithms</a> <br /></li>
      <li><a href="#moe--mixture-of-experts">Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer</a> <br /></li>
    </ul>
  </div>
</details>

<details>
<summary>2018</summary>
<div>

    <ul>
      <li><a href="#ulmfit">Universal Language Model Fine-tuning for Text Classification</a> <br /></li>
      <li><a href="#elmo-embeddings-from-language-models">Deep contextualized word representations</a> <br /></li>
      <li><a href="#gpt-1">Improving Language Understanding by Generative Pre-Training </a> <br /></li>
      <li><a href="#sentencepiece">SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing</a> <br /></li>
      <li><a href="#bert">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a> <br /></li>
    </ul>
  </div>
</details>

<details>
<summary>2019</summary>
<div>

    <ul>
      <li><a href="#gpt-2">Language Models are Unsupervised Multitask Learners</a> <br /></li>
      <li><a href="#roberta">RoBERTa: A Robustly Optimized BERT Pretraining Approach</a> <br /></li>
      <li><a href="#distilbert-and-model-compression">DistilBERT, a distilled version of BERT: smaller,faster, cheaper and lighter</a> <br /></li>
      <li><a href="#bart">BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension</a> <br /></li>
      <li><a href="#xlnet">XLNet: Generalized Autoregressive Pretraining for Language Understanding</a> <br /></li>
      <li><a href="#megatron">Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism</a> <br /></li>
      <li><a href="#sparse-attention-patterns">Generating Long Sequences with Sparse Transformers</a> <br /></li>
    </ul>
  </div>
</details>

<details>
<summary>2020</summary>
<div>

    <ul>
      <li><a href="#reformer-the-efficient-transformer">Reformer: The Efficient Transformer</a> <br /></li>
      <li><a href="#longformer-the-long-document-transformer">Longformer: The Long-Document Transformer</a> <br /></li>
      <li><a href="#gshard-scaling-giant-models-with-conditional-computation-and-automatic-sharding">GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding</a> <br /></li>
      <li><a href="#retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks">Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks</a> <br /></li>
      <li><a href="#big-bird-transformers-for-longer-sequences">Big Bird: Transformers for Longer Sequences</a> <br /></li>
      <li><a href="#gpt-3">GPT-3</a> <br /></li>
      <li><a href="#rethinking-attention-with-performers">Rethinking Attention with Performers</a> <br /></li>
      <li><a href="#t5">T5</a> <br /></li>
      <li><a href="#measuring-massive-multitask-language-understanding">Measuring Massive Multitask Language Understanding</a> <br /></li>
      <li><a href="#zero-zero-redundancy-optimizer">ZeRO (Zero Redundancy Optimizer)</a> <br /></li>
      <li><a href="#electra">ELECTRA</a> <br /></li>
      <li><a href="#switch-transformer">Switch Transformer</a> <br /></li>
      <li><a href="#scaling-laws">Scaling Laws</a> <br /></li>
    </ul>
  </div>
</details>

<details>
<summary>2021</summary>
<div>

    <ul>
      <li><a href="#roformer-enhanced-transformer-with-rotary-position-embedding">RoFormer: Enhanced Transformer with Rotary Position Embedding</a> <br /></li>
      <li><a href="#efficient-large-scale-language-model-training-on-gpu-clusters-using-megatron-lm">Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM</a> <br /></li>
      <li><a href="#transcending-scaling-laws-with-01-extra-compute">Transcending Scaling Laws with 0.1% Extra Compute</a> <br /></li>
      <li><a href="#improving-language-models-by-retrieving-from-trillions-of-tokens">Improving language models by retrieving from trillions of tokens</a> <br /></li>
      <li><a href="#clip">CLIP</a> <br /></li>
      <li><a href="#dall-e">Dall-e</a> <br /></li>
      <li><a href="#fsdp">FSDP</a> <br /></li>
      <li><a href="#humaneval">HumanEval</a> <br /></li>
      <li><a href="#lora">LoRA</a> <br /></li>
      <li><a href="#self-instruct-aligning-language-models-with-self-generated-instructions">Self-Instruct: Aligning Language Models with Self-Generated Instructions</a> <br /></li>
      <li><a href="#palm">PaLM</a> <br /></li>
      <li><a href="#gopher-deepmind">Gopher (DeepMind)</a> <br /></li>
      <li><a href="#megatron-turing-nlg">Megatron-Turing NLG</a> <br /></li>
    </ul>
  </div>
</details>
<details>
<summary>2022</summary>
<div>

    <ul>
      <li><a href="#efficiently-scaling-transformer-inference">EFFICIENTLY SCALING TRANSFORMER INFERENCE</a> <br /></li>
      <li><a href="#fast-inference-from-transformers-via-speculative-decoding">Fast Inference from Transformers via Speculative Decoding</a> <br /></li>
      <li><a href="#chinchilla">Chinchilla</a> <br /></li>
      <li><a href="#chain-of-thought-prompting">Chain-of-thought prompting</a> <br /></li>
      <li><a href="#instructgpt">InstructGPT</a> <br /></li>
      <li><a href="#bloom">BLOOM</a> <br /></li>
      <li><a href="#emergent-abilities-of-large-language-models">Emergent Abilities of Large Language Models</a> <br /></li>
      <li><a href="#flash-attention">Flash Attention</a> <br /></li>
      <li><a href="#grouped-query-attention">Grouped-query attention</a> <br /></li>
      <li><a href="#alibi-position-encoding">ALiBi position encoding</a> <br /></li>
      <li><a href="#deepspeed-inference-enabling-efficient-inference-of-transformer-models-at-unprecedented-scale">DeepSpeed Inference: Enabling Efficient Inference of Transformer Models at Unprecedented Scale</a> <br /></li>
      <li><a href="#claude-1">Claude 1</a> <br /></li>
      <li><a href="#flan-fine-tuned-language-net-google">FLAN (Fine-tuned LAnguage Net) (Google)</a> <br /></li>
      <li><a href="#red-teaming-language-models-with-language-models">Red Teaming Language Models with Language Models</a> <br /></li>
      <li><a href="#helm-holistic-evaluation-of-language-models">HELM (Holistic Evaluation of Language Models)</a> <br /></li>
      <li><a href="#dall-e-2-openai">DALL-E 2 (OpenAI)</a> <br /></li>
      <li><a href="#stable-diffusion-stability-ai">Stable Diffusion (Stability AI)</a> <br /></li>
      <li><a href="#gptq">GPTQ</a> <br /></li>
      <li><a href="#beyond-the-imitation-game-quantifying-and-extrapolating-the-capabilities-of-language-models">Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models</a> <br /></li>
      <li><a href="#minerva">Minerva</a> <br /></li>
      <li><a href="#chatgpt">ChatGPT</a> <br /></li>
    </ul>
  </div>
</details>

<details>
<summary>2023</summary>
<div>

    <ul>
      <li><a href="#efficient-memory-management-for-large-language-model-serving-with-pagedattention">Efficient Memory Management for Large Language Model Serving with PagedAttention</a> <br /></li>
      <li><a href="#qlora-efficient-finetuning-of-quantized-llms">QLoRA: Efficient Finetuning of Quantized LLMs</a> <br /></li>
      <li><a href="#parameter-efficient-fine-tuning-methods-for-pretrained-language-models-a-critical-review-and-assessment">Parameter-Efficient Fine-Tuning Methods for Pretrained Language Models: A Critical Review and Assessment</a> <br /></li>
      <li><a href="#flashattention-2-faster-attention-with-better-parallelism-and-work-partitioning">FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning</a> <br /></li>
      <li><a href="#awq-activation-aware-weight-quantization-for-llm-compression-and-acceleration">AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration</a> <br /></li>
      <li><a href="#generative-agents-interactive-simulacra-of-human-behavior">Generative Agents: Interactive Simulacra of Human Behavior</a> <br /></li>
      <li><a href="#voyager-an-open-ended-embodied-agent-with-large-language-models">Voyager: An Open-Ended Embodied Agent with Large Language Models</a> <br /></li>
      <li><a href="#universal-and-transferable-adversarial-attacks-on-aligned-language-models">Universal and Transferable Adversarial Attacks on Aligned Language Models</a> <br /></li>
      <li><a href="#tree-of-thoughts-deliberate-problem-solving-with-large-language-models">Tree of Thoughts: Deliberate Problem Solving with Large Language Models</a> <br /></li>
      <li><a href="#mpt">Mpt</a> <br /></li>
      <li><a href="#wizardlm-empowering-large-language-models-to-follow-complex-instructions">WizardLM: Empowering Large Language Models to Follow Complex Instructions</a> <br /></li>
      <li><a href="#deepspeed-chat-easy-fast-and-affordable-rlhf-training-of-chatgpt-like-models-at-all-scales">DeepSpeed-Chat: Easy, Fast and Affordable RLHF Training of ChatGPT-like Models at All Scales</a> <br /></li>
      <li><a href="#gpt-4">GPT-4</a> <br /></li>
      <li><a href="#mistral-7b">Mistral 7b</a> <br /></li>
      <li><a href="#llama">LLaMA</a> <br /></li>
      <li><a href="#mixtral-8x7b">Mixtral 8x7B</a> <br /></li>
      <li><a href="#llama-2">LLaMA 2</a> <br /></li>
      <li><a href="#vicuna-lmsys">Vicuna (LMSYS)</a> <br /></li>
      <li><a href="#alpaca">Alpaca</a> <br /></li>
      <li><a href="#direct-preference-optimization-dpo">Direct Preference Optimization (DPO)</a> <br /></li>
      <li><a href="#constitutional-ai">Constitutional AI</a> <br /></li>
      <li><a href="#toy-models-of-superposition">Toy Models of Superposition</a> <br /></li>
      <li><a href="#towards-monosemanticity-decomposing-language-models-with-dictionary-learning">Towards Monosemanticity: Decomposing Language Models With Dictionary Learning</a> <br /></li>
      <li><a href="#palm-2">PaLM 2</a> <br /></li>
      <li><a href="#laion-5b-laion">LAION-5B (LAION)</a> <br /></li>
      <li><a href="#lima">LIMA</a> <br /></li>
      <li><a href="#mamba">Mamba</a> <br /></li>
      <li><a href="#llava-visual-instruction-tuning">LLaVA (Visual Instruction Tuning)</a> <br /></li>
      <li><a href="#claude-1claude-2">Claude 1/Claude 2</a> <br /></li>
      <li><a href="#gemini">Gemini</a> <br /></li>
      <li><a href="#qwen">Qwen</a> <br /></li>
      <li><a href="#qwen-vl">Qwen-VL</a> <br /></li>
      <li><a href="#phi-1">Phi-1</a> <br /></li>
      <li><a href="#reinforced-self-training-rest-for-language-modeling">Reinforced Self-Training (ReST) for Language Modeling</a> <br /></li>
      <li><a href="#the-era-of-1-bit-llms-all-large-language-models-are-in-158-bits">The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits</a> <br /></li>
    </ul>
  </div>
</details>

<details>
<summary>2024</summary>
<div>

    <ul>
      <li><a href="#gemma">Gemma</a> <br /></li>
      <li><a href="#gemma-2">Gemma 2</a> <br /></li>
      <li><a href="#chatbot-arena-an-open-platform-for-evaluating-llms-by-human-preference">Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference</a> <br /></li>
      <li><a href="#tinyllama-an-open-source-small-language-model">TinyLlama: An Open-Source Small Language Model</a> <br /></li>
      <li><a href="#mordernbert">MordernBert</a> <br /></li>
      <li><a href="#jamba-a-hybrid-transformer-mamba-language-model">Jamba: A Hybrid Transformer-Mamba Language Model</a> <br /></li>
      <li><a href="#claude-3">Claude 3</a> <br /></li>
      <li><a href="#llama-3">LLaMA 3</a> <br /></li>
      <li><a href="#gemini-15">Gemini 1.5</a> <br /></li>
      <li><a href="#qwen-2">Qwen 2</a> <br /></li>
      <li><a href="#phi-2phi-3">phi-2/phi-3</a> <br /></li>
      <li><a href="#openai-o1">OpenAI o1</a> <br /></li>
      <li><a href="#rso-reinforced-self-training-with-online-feedback">RSO (Reinforced Self-training with Online feedback)</a> <br /></li>
      <li><a href="#spin-self-played-improvement-narration">SPIN (Self-Played Improvement Narration)</a> <br /></li>
      <li><a href="#dbrx">DBRX</a> <br /></li>
      <li><a href="#flashattention-3-fast-and-accurate-attention-with-asynchrony-and-low-precision">FlashAttention-3: Fast and Accurate Attention with Asynchrony and Low-precision</a> <br /></li>
      <li><a href="#qwen-25-alibaba">Qwen 2.5 (Alibaba)</a> <br /></li>
      <li><a href="#deepseek-25-deepseek">DeepSeek 2.5 (DeepSeek)</a> <br /></li>
      <li><a href="#claude-35-sonnet-anthropic">Claude 3.5 Sonnet (Anthropic)</a> <br /></li>
      <li><a href="#deepseek-r1-deepseek">DeepSeek-R1 (DeepSeek)</a> <br /></li>
      <li><a href="#phi-3">Phi 3</a> <br /></li>
      <li><a href="#phi-4">Phi 4</a> <br /></li>
      <li><a href="#self-play-fine-tuning-converts-weak-language-models-to-strong-language-models">Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models</a> <br /></li>
    </ul>
  </div>
</details>

<details>
<summary>2025</summary>
<div>

    <ul>
      <li><a href="#gemma-3">Gemma 3</a> <br /></li>
      <li><a href="#llama-4">Llama 4</a> <br /></li>
      <li><a href="#qwen25">Qwen2.5</a> <br /></li>
      <li><a href="#qwen-25-1m">Qwen 2.5-1M</a> <br /></li>
      <li><a href="#qwen25-omni">Qwen2.5-Omni</a> <br /></li>
      <li><a href="#qwen-3">Qwen 3</a> <br /></li>
      <li><a href="#grok">Grok</a> <br /></li>
      <li><a href="#pixtral">Pixtral</a> <br /></li>
      <li><a href="#large-language-diffusion-models">Large Language Diffusion Models</a> <br /></li>
    </ul>
  </div>
</details>
<p><br /></p>

<blockquote>
  <p>Note: I am releasing this blog early as a preview to get feedback from the community. It is still a work in progress and I plan to explain as well as implement each paper from each year. Do let me know your thoughts through my socials, or in the comments below!!!</p>
</blockquote>

<h2 id="2017-the-foundation-year">2017: The Foundation Year</h2>

<h3 id="transformer">Transformer</h3>

<p><img src="/assets/blog_assets/evolution_of_llms/transformers_abstract.webp" alt="Image of attention is all you need abstract" /></p>

<blockquote>
  <p>Link to paper: <a href="https://arxiv.org/abs/1706.03762">Attention is all you need</a> <br />
Link to implementation: [WORK IN PROGRESS]</p>
</blockquote>

<details>
<summary>Quick Summary</summary>
<div>

    <blockquote>
      <p>This is the famous “Attention Is All You Need” paper by Vaswani et al. that introduced the <strong>Transformer architecture</strong> - a groundbreaking neural network model that revolutionized natural language processing.</p>

      <p><strong>Key Innovation</strong></p>

      <p>The paper proposes replacing traditional recurrent neural networks (RNNs) and convolutional networks with a model based entirely on <strong>attention mechanisms</strong>. The core insight is that self-attention can capture dependencies between words regardless of their distance in a sequence, without needing to process them sequentially.</p>

      <p><strong>Architecture Highlights</strong></p>

      <ul>
        <li><strong>Encoder-Decoder Structure</strong>: 6 layers each, with multi-head self-attention and feed-forward networks</li>
        <li><strong>Multi-Head Attention</strong>: Uses 8 parallel attention heads to capture different types of relationships</li>
        <li><strong>Positional Encoding</strong>: Sine/cosine functions to inject sequence order information</li>
        <li><strong>No Recurrence</strong>: Enables much better parallelization during training</li>
      </ul>

      <p><strong>Results</strong>
The Transformer achieved state-of-the-art performance on machine translation tasks:</p>

      <ul>
        <li><strong>28.4 BLEU</strong> on English-to-German (WMT 2014)</li>
        <li><strong>41.8 BLEU</strong> on English-to-French</li>
        <li>Trained significantly faster than previous models (12 hours vs. days/weeks)</li>
      </ul>

      <p><strong>Impact</strong>
This architecture became the foundation for modern language models like BERT, GPT, and others. The paper’s core principle - that attention mechanisms alone are sufficient for high-quality sequence modeling - fundamentally changed how we approach NLP tasks.</p>

      <p>The work demonstrated superior performance while being more parallelizable and interpretable than previous sequence-to-sequence models.</p>
    </blockquote>

  </div>
</details>
<p><br /></p>

<p>THE foundational paper that introduced some key ideas such as:</p>

<ul>
  <li>Scaled dot-product attention</li>
  <li>Multi-head attention mechanism</li>
  <li>Positional encodings</li>
  <li>Layer normalization</li>
  <li>Masked attention for autoregressive models</li>
</ul>

<p>We have talked deeply about each of these topics previously and I implore you to check that part out <a href="https://goyalpramod.github.io/blogs/Transformers_laid_out/">here</a>.</p>

<p><strong>Problem</strong></p>

<blockquote>
  <p>Sequential models like <a href="https://d2l.ai/chapter_recurrent-neural-networks/rnn.html">RNNs</a> and LSTMs process text word-by-word, creating a fundamental bottleneck: each word must wait for the previous word to be processed. This sequential nature makes training painfully slow and prevents the model from understanding long-range dependencies effectively.</p>
</blockquote>

<p>For example, in the sentence “The cat that lived in the house with the red door was hungry”, by the time the model reaches “was hungry”, it has largely forgotten about “The cat” due to the vanishing gradient problem. The model struggles to connect distant but related words.</p>

<p><img src="/assets/blog_assets/evolution_of_llms/rnn_vs_transformer.webp" alt="Image of rnn vs transformers" /></p>

<p><strong>Solution</strong></p>

<blockquote>
  <p>The Transformer replaced sequential processing with parallel attention mechanisms. Instead of processing words one-by-one, it looks at all words simultaneously and uses attention to determine which words are most relevant to each other, regardless of their distance in the sentence.</p>
</blockquote>

<p>This attention-based approach allows the model to directly connect “The cat” with “was hungry” in a single step, while also enabling massive parallelization during training - turning what used to take weeks into hours.</p>

<h5 id="training-a-transformer">Training a Transformer</h5>

<p>This is one topic that we didn’t talk about extensively so let’s go over it, because that is where the true beauty of GPT lies. How to train over huge amounts of data.</p>

<p>We will build iteratively, first starting small. And going massive as we approach the GPT paper.</p>

<p>This <a href="https://machinelearningmastery.com/training-the-transformer-model/">blog</a> helped me with this section.</p>

<p><strong>Preparing the data</strong></p>

<p>The original Transformer was trained for neural machine translation using English-German sentence pairs. The data preparation involves several crucial steps:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Data preparation
</span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span><span class="p">,</span> <span class="n">Dataset</span>
<span class="kn">from</span> <span class="nn">torch.nn.utils.rnn</span> <span class="kn">import</span> <span class="n">pad_sequence</span>

<span class="k">def</span> <span class="nf">prepare_training_data</span><span class="p">(</span><span class="n">sentences</span><span class="p">):</span>
    <span class="c1"># 1. Add special tokens
</span>    <span class="n">processed_sentences</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">sentence</span> <span class="ow">in</span> <span class="n">sentences</span><span class="p">:</span>
        <span class="n">processed_sentences</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="s">"&lt;START&gt; "</span> <span class="o">+</span> <span class="n">sentence</span> <span class="o">+</span> <span class="s">" &lt;EOS&gt;"</span><span class="p">)</span>

    <span class="c1"># 2. Create vocabulary
</span>    <span class="n">vocab</span> <span class="o">=</span> <span class="n">build_vocab</span><span class="p">(</span><span class="n">processed_sentences</span><span class="p">)</span>
    <span class="n">vocab_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">vocab</span><span class="p">)</span>

    <span class="c1"># 3. Convert to tensor sequences
</span>    <span class="n">sequences</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">sentence</span> <span class="ow">in</span> <span class="n">processed_sentences</span><span class="p">:</span>
        <span class="n">tokens</span> <span class="o">=</span> <span class="n">sentence</span><span class="p">.</span><span class="n">split</span><span class="p">()</span>
        <span class="n">sequence</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">vocab</span><span class="p">[</span><span class="n">token</span><span class="p">]</span> <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">tokens</span><span class="p">])</span>
        <span class="n">sequences</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">sequence</span><span class="p">)</span>

    <span class="c1"># 4. Pad sequences
</span>    <span class="n">padded_sequences</span> <span class="o">=</span> <span class="n">pad_sequence</span><span class="p">(</span><span class="n">sequences</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">padding_value</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">padded_sequences</span><span class="p">,</span> <span class="n">vocab_size</span>
</code></pre></div></div>

<ol>
  <li>
    <p><strong>Special tokens</strong> (<code class="language-plaintext highlighter-rouge">&lt;START&gt;</code> and <code class="language-plaintext highlighter-rouge">&lt;EOS&gt;</code>): These tell the model where sentences begin and end. The <code class="language-plaintext highlighter-rouge">&lt;START&gt;</code> token signals the decoder to begin generation, while <code class="language-plaintext highlighter-rouge">&lt;EOS&gt;</code> teaches it when to stop. Without these, the model wouldn’t know sentence boundaries. As we will move through the years, we will see how the special tokens used in LLMs have evolved as well. For example, think what will happen inside an LLM when it encounters a token that it hasn’t seen during training, like a chinese character etc.</p>
  </li>
  <li>
    <p><strong>Vocabulary creation</strong>: The vocabulary maps every unique word/token in the training data to a number. This is how we convert text (which computers can’t process) into numerical tensors (which they can). The vocabulary size determines the final layer dimension of our model.</p>
  </li>
  <li>
    <p><strong>Tensor conversion</strong>: Neural networks work with numbers, not words. Each word gets replaced by its vocabulary index, creating sequences of integers that can be fed into the model.</p>
  </li>
  <li>
    <p><strong>Padding</strong>: Sentences have different lengths, but neural networks need fixed-size inputs for batch processing. Padding with zeros makes all sequences the same length, enabling efficient parallel computation.</p>
  </li>
</ol>

<p><strong>Key Training Innovations</strong></p>

<p>The Transformer introduced several training techniques that became standard:</p>

<p><strong>Teacher Forcing with Masking</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># During training, decoder sees target sequence shifted by one position
</span><span class="n">encoder_input</span> <span class="o">=</span> <span class="n">source_sequence</span>
<span class="n">decoder_input</span> <span class="o">=</span> <span class="n">target_sequence</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>  <span class="c1"># Remove last token
</span><span class="n">decoder_output</span> <span class="o">=</span> <span class="n">target_sequence</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:]</span>  <span class="c1"># Remove first token
</span>
<span class="c1"># Look-ahead mask prevents seeing future tokens
</span><span class="k">def</span> <span class="nf">create_look_ahead_mask</span><span class="p">(</span><span class="n">seq_len</span><span class="p">):</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">triu</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">ones</span><span class="p">(</span><span class="n">seq_len</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">),</span> <span class="n">diagonal</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">mask</span><span class="p">.</span><span class="nb">bool</span><span class="p">()</span>

<span class="n">mask</span> <span class="o">=</span> <span class="n">create_look_ahead_mask</span><span class="p">(</span><span class="n">decoder_input</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
</code></pre></div></div>

<p><strong>Why this works:</strong> Teacher forcing trains the decoder to predict the next token given all previous tokens, without requiring separate training data. The input-output shift creates a “next token prediction” task from translation pairs. The look-ahead mask ensures the model can’t “cheat” by seeing future tokens during training - it must learn to predict based only on past context, just like during real inference.</p>

<p><strong>Custom Learning Rate Schedule</strong>
The paper introduced a specific learning rate scheduler that warms up then decays:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Learning rate schedule from the paper
</span><span class="kn">import</span> <span class="nn">math</span>

<span class="k">class</span> <span class="nc">TransformerLRScheduler</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">d_model</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">warmup_steps</span><span class="o">=</span><span class="mi">4000</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">optimizer</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">d_model</span> <span class="o">=</span> <span class="n">d_model</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">warmup_steps</span> <span class="o">=</span> <span class="n">warmup_steps</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">step_count</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">step_count</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="n">lr</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">get_lr</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">param_group</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">optimizer</span><span class="p">.</span><span class="n">param_groups</span><span class="p">:</span>
            <span class="n">param_group</span><span class="p">[</span><span class="s">'lr'</span><span class="p">]</span> <span class="o">=</span> <span class="n">lr</span>

    <span class="k">def</span> <span class="nf">get_lr</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">arg1</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">step_count</span> <span class="o">**</span> <span class="o">-</span><span class="mf">0.5</span>
        <span class="n">arg2</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">step_count</span> <span class="o">*</span> <span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">warmup_steps</span> <span class="o">**</span> <span class="o">-</span><span class="mf">1.5</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">d_model</span> <span class="o">**</span> <span class="o">-</span><span class="mf">0.5</span><span class="p">)</span> <span class="o">*</span> <span class="nb">min</span><span class="p">(</span><span class="n">arg1</span><span class="p">,</span> <span class="n">arg2</span><span class="p">)</span>
</code></pre></div></div>

<p><strong>Why this schedule:</strong> The warmup phase gradually increases the learning rate, preventing the model from making drastic weight updates early in training when gradients are noisy. After warmup, the learning rate decays proportionally to the square root of the step number, allowing for fine-tuning as training progresses. This schedule was crucial for training stability with the Transformer’s deep architecture.</p>

<p><strong>Padding Masks for Loss Computation</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="n">F</span>

<span class="k">def</span> <span class="nf">masked_loss</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="n">prediction</span><span class="p">,</span> <span class="n">pad_token</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
    <span class="c1"># Don't compute loss on padding tokens
</span>    <span class="n">mask</span> <span class="o">=</span> <span class="p">(</span><span class="n">target</span> <span class="o">!=</span> <span class="n">pad_token</span><span class="p">).</span><span class="nb">float</span><span class="p">()</span>

    <span class="c1"># Reshape for cross entropy
</span>    <span class="n">prediction</span> <span class="o">=</span> <span class="n">prediction</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">prediction</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">target</span> <span class="o">=</span> <span class="n">target</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="n">mask</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1"># Compute cross entropy loss
</span>    <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">prediction</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s">'none'</span><span class="p">)</span>
    <span class="n">masked_loss</span> <span class="o">=</span> <span class="n">loss</span> <span class="o">*</span> <span class="n">mask</span>

    <span class="k">return</span> <span class="n">masked_loss</span><span class="p">.</span><span class="nb">sum</span><span class="p">()</span> <span class="o">/</span> <span class="n">mask</span><span class="p">.</span><span class="nb">sum</span><span class="p">()</span>
</code></pre></div></div>

<p><strong>Why masking matters:</strong> Padding tokens (zeros) are artificial - they don’t represent real words. Computing loss on them would teach the model incorrect patterns and waste computational resources. The mask ensures we only compute loss on actual content, leading to more meaningful gradients and better learning. This also prevents the model from learning to predict padding tokens, which would be useless during inference.</p>

<p><strong>Training Configuration</strong></p>

<p>The original paper used these hyperparameters:</p>

<ul>
  <li><strong>Model size</strong>: 512 dimensions (base model)</li>
  <li><strong>Attention heads</strong>: 8</li>
  <li><strong>Encoder/Decoder layers</strong>: 6 each</li>
  <li><strong>Feed-forward dimension</strong>: 2048</li>
  <li><strong>Dropout</strong>: 0.1</li>
  <li><strong>Optimizer</strong>: Adam with custom learning rate schedule</li>
  <li><strong>Training time</strong>: ~12 hours on 8 P100 GPUs</li>
</ul>

<p><strong>The Training Loop</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="nn">torch.optim</span> <span class="kn">import</span> <span class="n">Adam</span>

<span class="k">def</span> <span class="nf">train_step</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">scheduler</span><span class="p">,</span> <span class="n">encoder_input</span><span class="p">,</span> <span class="n">decoder_input</span><span class="p">,</span> <span class="n">decoder_output</span><span class="p">):</span>
    <span class="n">model</span><span class="p">.</span><span class="n">train</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">()</span>

    <span class="c1"># Forward pass
</span>    <span class="n">prediction</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">encoder_input</span><span class="p">,</span> <span class="n">decoder_input</span><span class="p">)</span>

    <span class="c1"># Compute masked loss and accuracy
</span>    <span class="n">loss</span> <span class="o">=</span> <span class="n">masked_loss</span><span class="p">(</span><span class="n">decoder_output</span><span class="p">,</span> <span class="n">prediction</span><span class="p">)</span>
    <span class="n">accuracy</span> <span class="o">=</span> <span class="n">masked_accuracy</span><span class="p">(</span><span class="n">decoder_output</span><span class="p">,</span> <span class="n">prediction</span><span class="p">)</span>

    <span class="c1"># Backward pass
</span>    <span class="n">loss</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>
    <span class="n">scheduler</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">loss</span><span class="p">.</span><span class="n">item</span><span class="p">(),</span> <span class="n">accuracy</span><span class="p">.</span><span class="n">item</span><span class="p">()</span>

<span class="c1"># Main training loop
</span><span class="n">model</span> <span class="o">=</span> <span class="n">TransformerModel</span><span class="p">(</span><span class="n">src_vocab_size</span><span class="p">,</span> <span class="n">tgt_vocab_size</span><span class="p">,</span> <span class="n">d_model</span><span class="o">=</span><span class="mi">512</span><span class="p">)</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span> <span class="n">betas</span><span class="o">=</span><span class="p">(</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.98</span><span class="p">),</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-9</span><span class="p">)</span>
<span class="n">scheduler</span> <span class="o">=</span> <span class="n">TransformerLRScheduler</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">d_model</span><span class="o">=</span><span class="mi">512</span><span class="p">)</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">dataloader</span><span class="p">:</span>
        <span class="n">src_batch</span><span class="p">,</span> <span class="n">tgt_batch</span> <span class="o">=</span> <span class="n">batch</span>

        <span class="c1"># Prepare inputs
</span>        <span class="n">encoder_input</span> <span class="o">=</span> <span class="n">src_batch</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:]</span>     <span class="c1"># Remove START token
</span>        <span class="n">decoder_input</span> <span class="o">=</span> <span class="n">tgt_batch</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>    <span class="c1"># Remove EOS token
</span>        <span class="n">decoder_output</span> <span class="o">=</span> <span class="n">tgt_batch</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:]</span>    <span class="c1"># Remove START token
</span>
        <span class="n">loss</span><span class="p">,</span> <span class="n">accuracy</span> <span class="o">=</span> <span class="n">train_step</span><span class="p">(</span>
            <span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">scheduler</span><span class="p">,</span>
            <span class="n">encoder_input</span><span class="p">,</span> <span class="n">decoder_input</span><span class="p">,</span> <span class="n">decoder_output</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="n">step</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">'Epoch </span><span class="si">{</span><span class="n">epoch</span><span class="si">}</span><span class="s">, Step </span><span class="si">{</span><span class="n">step</span><span class="si">}</span><span class="s">, Loss: </span><span class="si">{</span><span class="n">loss</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="s">, Acc: </span><span class="si">{</span><span class="n">accuracy</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="s">'</span><span class="p">)</span>
</code></pre></div></div>

<p><strong>Why This Training Approach Worked</strong></p>

<ol>
  <li><strong>Parallelization</strong>: Unlike RNNs, all positions could be computed simultaneously</li>
  <li><strong>Stable Gradients</strong>: Layer normalization and residual connections prevented vanishing gradients</li>
  <li><strong>Efficient Attention</strong>: Scaled dot-product attention was computationally efficient</li>
  <li><strong>Smart Masking</strong>: Prevented information leakage while enabling parallel training</li>
</ol>

<p>This training methodology laid the groundwork for scaling to the massive language models we see today. The key insight was that with proper masking and attention mechanisms, you could train much larger models much faster than sequential architectures allowed.</p>

<p>While the original Transformer showed the power of attention-based training, it was still limited to translation tasks with paired data. The real revolution came when researchers realized they could use similar training techniques on massive amounts of unlabeled text data - setting the stage for GPT and the era of large language models.</p>

<h3 id="rlhf---reinforcement-learning-from-human-preferences">RLHF - Reinforcement Learning from Human Preferences</h3>

<p><img src="/assets/blog_assets/evolution_of_llms/rlhf_abstract.pdf.webp" alt="Image of RLHF abstract" /></p>

<blockquote>
  <p>Link to paper: <a href="https://arxiv.org/abs/1706.03741">Deep reinforcement learning from human preferences</a> <br />
Link to implementation: [WORK IN PROGRESS]</p>
</blockquote>

<details>
<summary>Quick Summary</summary>
<div>

    <blockquote>
      <p>This paper presents a method for training reinforcement learning (RL) agents using human feedback instead of explicitly refined reward functions. Here’s a high-level overview:</p>

      <p>The authors address a fundamental challenge in RL: for many complex tasks, designing appropriate reward functions is difficult or impossible. Instead of requiring engineers to craft these functions, they develop a system where:</p>

      <ol>
        <li>Humans compare short video clips of agent behavior (1-2 seconds)</li>
        <li>These comparisons train a reward predictor model</li>
        <li>The agent optimizes its policy using this learned reward function</li>
      </ol>

      <p><strong>Key contributions:</strong></p>

      <ul>
        <li>They show this approach can solve complex RL tasks using feedback on less than 1% of the agent’s interactions</li>
        <li>This dramatically reduces the human oversight required, making it practical for state-of-the-art RL systems</li>
        <li>They demonstrate training novel behaviors with just about an hour of human time</li>
        <li>Their approach works across domains including Atari games and simulated robot locomotion</li>
      </ul>

      <p>The technique represents a significant advance in aligning AI systems with human preferences, addressing concerns about misalignment between AI objectives and human values. By having humans evaluate agent behavior directly, the system learns rewards that better capture what humans actually want.</p>
    </blockquote>

  </div>
</details>
<p><br /></p>

<p>As mind boggling as it sounds, the famed algorithm RLHF came out in 2017, the same year attention is all you need came out.
Let us understand the ideas put forth and why it was such a big deal.</p>

<p>(If you are not familiar with the idea of RL, I will recommend checking this small <a href="https://huggingface.co/learn/deep-rl-course/unit0/introduction">course</a> by HuggingFace out)</p>

<p><strong>Problem</strong></p>

<blockquote>
  <p>Designing reward functions for complex behaviors is nearly impossible. How do you mathematically define “write a helpful response” or “be creative but truthful”? Traditional RL requires explicit numerical rewards for every action, but many desirable behaviors are subjective and context-dependent.</p>
</blockquote>

<p>For example, it’s impossible to write code that scores joke quality, but humans can easily compare two jokes and say which is funnier.</p>

<p><strong>Solution</strong> :</p>

<blockquote>
  <p>One possible solution is to allow a human to provide feedback on the agents’s current behavior and use this feedback to define the task. But this poses another problem, this would require hundreds of hours as well as domain experience. It was discovered by the researchers that preference modeling by a human even on a small subset provided great results.</p>
</blockquote>

<p>An ideal solution will</p>

<ol>
  <li>Enable us to solve tasks about which we can tell the desired behavior but not necessarily demonstrate or describe it.</li>
  <li>Allows systems to learn from non-expert users</li>
  <li>Scales to large problems</li>
  <li>Is economical</li>
</ol>

<p>In their experiment, the researchers asked labellers to compare short video clips of the agent’s behavior. They found that by using a small sample of clips they were able to train the system to behave as desired.</p>

<p><img src="/assets/blog_assets/evolution_of_llms/hop.webp" alt="Image of hop" />
<em>Image sourced from <a href="https://arxiv.org/abs/1706.03741">paper</a></em></p>

<p>The human observes the agent acting in the <em>environment</em> he then gives he’s feedback. Which is taken by <em>reward predictor</em> which numerical defines the reward. Which is sent to the <em>RL algorithm</em> this updates the agent based on the feedback and observation from the environment. That then changes the action of the agent.</p>

<p><img src="/assets/blog_assets/evolution_of_llms/1.webp" alt="Image of RLHF" /></p>

<p>This sounds simple enough in principle, but how do you teach a model to learn from these preferences. I.e reward modeling.</p>

<blockquote>
  <p>Note: We will be talking more in depth about RL algorithms in the next section. The topics in RL are rather complicated and usually talked in the end after an LLM is trained. So you can skip this part for now if it is daunting.</p>
</blockquote>

<p><strong>Reward predictor in RLHF</strong></p>

<p>The following blogs helped me while writing this section</p>

<ul>
  <li><a href="https://huggingface.co/blog/rlhf">HF blog on RLHF</a></li>
  <li><a href="https://huyenchip.com/2023/05/02/rlhf.html">Chip Huyen’s blog on RLHF</a></li>
</ul>

<p>The reward predictor is trained to predict which of two given trajectories(σ¹, σ²) will be preferred by a human</p>

<p><strong>Example:</strong>
<img src="/assets/blog_assets/evolution_of_llms/trajector_comparison.webp" alt="Image of trajectories" /></p>

<p>Imagine two robot trajectories:</p>

<ul>
  <li>Trajectory A: Robot goes directly to the goal</li>
  <li>Trajectory B: Robot roams around then goes to the goal</li>
</ul>

<p>A human would prefer A (more efficient). The reward model learns to assign higher values to the observation-action pairs in trajectory A, eventually learning that “efficient movement” correlates with human preference.</p>

<p>Reward predictor equation</p>

\[\hat{P}\left[\sigma^{1} \succ \sigma^{2}\right]=\frac{\exp \sum \hat{r}\left(o_{t}^{1}, a_{t}^{1}\right)}{\exp \sum \hat{r}\left(o_{t}^{1}, a_{t}^{1}\right)+\exp \sum \hat{r}\left(o_{t}^{2}, a_{t}^{2}\right)}\]

<p>It is trained using cross-entropy loss to match human preferences:</p>

\[\operatorname{loss}(\hat{r})=-\sum_{\left(\sigma^{1}, \sigma^{2}, \mu\right) \in D} \mu(1) \log \hat{P}\left[\sigma^{1} \succ \sigma^{2}\right]+\mu(2) \log \hat{P}\left[\sigma^{2} \succ \sigma^{1}\right]\]

<details>
<summary>Mathematical Notation</summary>
<div>

    <ul>
      <li>$\hat{P}\left[\sigma^{1} \succ \sigma^{2}\right]$: Predicted probability that trajectory segment $\sigma^{1}$ is preferred over trajectory segment $\sigma^{2}$</li>
      <li>$\hat{r}$: The learned reward function</li>
      <li>$o_{t}^{i}$: Observation at time $t$ in trajectory segment $i$</li>
      <li>$a_{t}^{i}$: Action at time $t$ in trajectory segment $i$</li>
      <li>$\sigma^{i}$: Trajectory segment $i$ (a sequence of observation-action pairs)</li>
      <li>$\exp$: Exponential function</li>
      <li>$\sum$: Summation over all timesteps in the trajectory segment</li>
      <li>$\operatorname{loss}(\hat{r})$: Cross-entropy loss function for the reward model</li>
      <li>$D$: Dataset of human preference comparisons</li>
      <li>$\mu$: Distribution over ${1,2}$ indicating human preference</li>
      <li>$\mu(1)$: Probability that human preferred segment 1</li>
      <li>$\mu(2)$: Probability that human preferred segment 2</li>
      <li>$\log$: Natural logarithm</li>
    </ul>
  </div>
</details>
<p><br /></p>

<p>Let us understand the Reward Function Fitting Process</p>

<p><strong>The Preference-Predictor Model</strong></p>

<p>The authors instead of directly creating a reward function (which rewards an agent when it does the desired behavior and punishes otherwise), they created a preference predictor. Which predicts which of the two given sequence of actions will be preferred by a human.</p>

<p><strong>The Mathematical Formulation (Equation 1)</strong></p>

<p>The equation P̂[σ¹ ≻ σ²] represents the predicted probability that a human would prefer trajectory segment σ¹ over segment σ².</p>

<p>Breaking down the formula:</p>

<ul>
  <li>$\sigma^{[1]}$ and $\sigma^{[2]}$ are two different trajectory segments (short video clips of agent behavior)</li>
  <li>$o_{t}^{[i]}$ and $a_{t}^{[i]}$ represent the observation and action at time $t$ in trajectory $i$</li>
  <li>$\hat{r}(o_{t}^{[i]}, a_{t}^{[i]})$ is the estimated reward for that observation-action pair</li>
  <li>The formula uses the softmax function (exponential normalization):</li>
</ul>

\[\hat{P}\left[\sigma^{[1]} \succ \sigma^{[2]}\right] = \frac{\exp\left(\sum \hat{r}\left(o_{t}^{[1]}, a_{t}^{[1]}\right)\right)}{\exp\left(\sum \hat{r}\left(o_{t}^{[1]}, a_{t}^{[1]}\right)\right) + \exp\left(\sum \hat{r}\left(o_{t}^{[2]}, a_{t}^{[2]}\right)\right)}\]

<p>This means:</p>

<ol>
  <li>Sum up all the predicted rewards along trajectory 1</li>
  <li>Sum up all the predicted rewards along trajectory 2</li>
  <li>Apply exponential function to both sums</li>
  <li>The probability of preferring trajectory 1 is the ratio of exp(sum1) to the total exp(sum1) + exp(sum2)</li>
</ol>

<p><strong>The Loss Function</strong></p>

<p>The goal is to find parameters for r̂ that make its predictions match the actual human preferences:</p>

\[\operatorname{loss}(\hat{r}) = -\sum_{\left(\sigma^{[1]}, \sigma^{[2]}, \mu\right) \in D} \left[\mu([1])\log \hat{P}\left[\sigma^{[1]} \succ \sigma^{[2]}\right] + \mu([2])\log \hat{P}\left[\sigma^{[2]} \succ \sigma^{[1]}\right]\right]\]

<p>Where:</p>

<ul>
  <li>$\left(\sigma^{[1]}, \sigma^{[2]}, \mu\right) \in D$ means we’re summing over all the comparison data in our dataset $D$</li>
  <li>$\mu$ is a distribution over ${1,2}$ indicating which segment the human preferred</li>
  <li>If the human strictly preferred segment 1, then $\mu([1]) = 1$ and $\mu([2]) = 0$</li>
  <li>If the human strictly preferred segment 2, then $\mu([1]) = 0$ and $\mu([2]) = 1$</li>
  <li>If the human found them equal, then $\mu([1]) = \mu([2]) = 0.5$</li>
</ul>

<p>This is the standard cross-entropy loss function used in classification problems, measuring how well our predicted probabilities match the actual human judgments.</p>

<p>Consider reading this beautiful blog on <a href="https://colah.github.io/posts/2015-09-Visual-Information/">Entropy</a> by Christopher Olah, if you wish to gain a deeper understanding of cross-entropy.</p>

<p><strong>The Bradley-Terry Model Connection</strong></p>

<blockquote>
  <p><strong>Note from Wikipedia:</strong> The Bradley–Terry model is a probability model for the outcome of pairwise comparisons between items, teams, or objects. Given a pair of items $i$ and $j$ drawn from some population, it estimates the probability that the pairwise comparison $i &gt; j$ turns out true, as</p>

\[\Pr(i&gt;j) = \frac{p_i}{p_i + p_j}\]

  <p>where $p_i$ is a positive real-valued score assigned to individual $i$. The comparison $i &gt; j$ can be read as “i is preferred to j”, “i ranks higher than j”, or “i beats j”, depending on the application.</p>
</blockquote>

<p>This approach is based on the <a href="https://en.wikipedia.org/wiki/Bradley%E2%80%93Terry_model">Bradley-Terry model</a>, which is a statistical model for paired comparisons. It’s similar to:</p>

<ol>
  <li>
    <p>The <a href="https://en.wikipedia.org/wiki/Elo_rating_system">Elo rating</a> system in chess: Players have ratings, and the difference in ratings predicts the probability of one player beating another.</p>
  </li>
  <li>
    <p>In this case: Trajectory segments have “ratings” (the sum of rewards), and the difference in ratings predicts the probability of a human preferring one segment over another.</p>
  </li>
</ol>

<p>In essence, the reward function learns to assign higher values to states and actions that humans tend to prefer, creating a preference scale that can be used to guide the agent’s behavior.</p>

<p>The most important breakthrough: <strong>We can align AI systems with human values using comparative feedback from non-experts.</strong> This insight would prove crucial when training language models - instead of trying to define “helpful” or “harmless” mathematically, we can simply ask humans to compare outputs.</p>

<p>This comparative approach scales much better than rating individual responses, making it practical for training large language models on human preferences.</p>

<table>
  <tbody>
    <tr>
      <td>Fun story: One time researchers tried to RL a helicopter and it started <a href="https://www.youtube.com/watch?v=M-QUkgk3HyE&amp;ab_channel=Stanford">flying backwards</a></td>
    </tr>
  </tbody>
</table>

<h3 id="ppo-proximal-policy-optimization">PPO: Proximal Policy Optimization</h3>

<p><img src="/assets/blog_assets/evolution_of_llms/ppo_abstract.pdf.webp" alt="Image of ppo abstract" /></p>

<blockquote>
  <p>Link to paper: <a href="https://arxiv.org/abs/1707.06347">Proximal Policy Optimization Algorithms</a> <br />
Link to implementation: [WORK IN PROGRESS]</p>
</blockquote>

<details>
<summary>Quick Summary</summary>
<div>

    <p>This paper by John Schulman et al. from OpenAI introduces Proximal Policy Optimization (PPO), a family of policy gradient methods for reinforcement learning that achieves the reliability and data efficiency of Trust Region Policy Optimization (TRPO) while being much simpler to implement and more compatible with various neural network architectures.</p>

    <p>Key contributions:</p>

    <ul>
      <li>A novel “clipped” surrogate objective function that provides a pessimistic estimate of policy performance</li>
      <li>An algorithm that alternates between data collection and multiple epochs of optimization on the same data</li>
      <li>Empirical validation showing PPO outperforms other online policy gradient methods across continuous control tasks and Atari games</li>
      <li>A balance between sample complexity, implementation simplicity, and computation time</li>
    </ul>

    <p>The core innovation is their clipped probability ratio approach, which constrains policy updates without requiring the complex second-order optimization techniques used in TRPO. This makes PPO more practical while maintaining performance guarantees.</p>

  </div>
</details>
<p><br /></p>

<p>Another LLM algo that came out in 2017, and that too again by OpenAI. Really goes to show how much they tried to advance AI and be public about it (At least in the early days).</p>

<p>This is going to be math heavy so be prepared (Dw, I will guide you in each step)</p>

<p><strong>Problem</strong></p>

<blockquote>
  <p>However, there is room for improvement in developing a method that is scalable (to
large models and parallel implementations), data efficient, and robust (i.e., successful on a variety
of problems without hyperparameter tuning). Q-learning (with function approximation) fails on
many simple problems and is poorly understood, vanilla policy gradient methods have poor data
effiency and robustness; and trust region policy optimization (TRPO) is relatively complicated,
and is not compatible with architectures that include noise (such as dropout) or parameter sharing
(between the policy and value function, or with auxiliary tasks).</p>
</blockquote>

<p>Essentially there were a lot of RL algorithms, but none of them worked efficiently at scale.</p>

<p><strong>Solution</strong></p>

<blockquote>
  <p>This paper seeks to improve the current state of affairs by introducing an algorithm that attains
the data efficiency and reliable performance of TRPO, while using only first-order optimization.
We propose a novel objective with clipped probability ratios, which forms a pessimistic estimate
(i.e., lower bound) of the performance of the policy. To optimize policies, we alternate between
sampling data from the policy and performing several epochs of optimization on the sampled data</p>
</blockquote>

<p>The authors found a way to take the best RL algorithm of the time (TRPO) and make it work at scale.</p>

<p>The following blogs &amp; articles helped me write this section</p>

<ul>
  <li><a href="https://spinningup.openai.com/en/latest/spinningup/rl_intro.html">Spinning up docs by OpenAI</a>, consider going through this to help understand the nomenclature used throughout this section</li>
  <li><a href="https://jonathan-hui.medium.com/rl-deep-reinforcement-learning-series-833319a95530">RL blogs by jonathan hui</a>, they really simplified the ideas for me</li>
  <li><a href="https://johnwlambert.github.io/policy-gradients/">Understanding Policy Gradients</a>, this blog really helped me understand the math behind the idea</li>
  <li><a href="https://karpathy.github.io/2016/05/31/rl/">These</a> <a href="https://cameronrwolfe.substack.com/p/proximal-policy-optimization-ppo">blogs</a> <a href="https://huggingface.co/blog/NormalUhr/rlhf-pipeline">were</a> <a href="https://lilianweng.github.io/posts/2018-04-08-policy-gradient/">extremely</a> <a href="https://iclr-blogposts.github.io/2024/blog/the-n-implementation-details-of-rlhf-with-ppo/">helpful</a> <a href="https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/">too</a> (each word is a different link)</li>
  <li>The <a href="http://incompleteideas.net/book/the-book-2nd.html">bible</a> of modern RL</li>
</ul>

<h5 id="what-is-reinforcement-learning">What is Reinforcement Learning</h5>

<p><img src="/assets/blog_assets/evolution_of_llms/RL.webp" alt="Image of RL" />
<em>Image taken from <a href="https://huggingface.co/learn/deep-rl-course/en/unit1/rl-framework">HuggingFace Course</a></em></p>

<p>In RL we create an Agent (An ML model like Artificial Neural Network) give it a defined set of Actions $A_t$ (In this case it would be, move left, move right, Press A to shoot).</p>

<p>The agent then chooses an action and interacts with the Environment, which returns a new state as well as reward (positive if we survived or did a favourable outcome, negative if we die or do an unfavourable outcome).</p>

<p>Step by Step it looks something like this:</p>

<ul>
  <li>The agent recieves <em>state $S_0$</em> from the environment (In this that would be the first frame of the game)</li>
  <li>Based on <em>state $S_0$</em>, the agent takes <em>action $A_0$</em> (chooses to move right)</li>
  <li>The environment goes to new frame, new <em>state $S_1$</em>.</li>
  <li>The environment gives the agent, <em>reward $R_t$</em> (still alive!!!).</li>
</ul>

<p>The idea behind RL is based on reward hypothesis, which states that</p>

<table>
  <tbody>
    <tr>
      <td><em>All goals can be described as the maximization of the expected return (expected cumulative reward)</em></td>
    </tr>
  </tbody>
</table>

<p>Which can be mathematically represented as
$R(\tau) = r_{t+1} + r_{t+2} + r_{t+3} + r_{t+4} + \ldots$
($\tau$ read as tau)</p>

<p>Remember this, It will prove useful later.</p>

<h5 id="policy-π-the-agents-brain">Policy π: The Agent’s Brain</h5>

<p><img src="/assets/blog_assets/evolution_of_llms/policy.webp" alt="Image of policy based approach" /></p>

<p>The Policy π is the brain of our Agent, it’s the function that tells an Agent what action it should take at a given state and time.</p>

<p>The policy is what we want to train and make an optimum policy π*, that maximizes expected return when the agent acts according to it. (remember that is the idea behind RL)</p>

<p><img src="/assets/blog_assets/evolution_of_llms/rl_algos.webp" alt="Image of RL" />
<em>Image taken from <a href="https://spinningup.openai.com/en/latest/spinningup/rl_intro2.html">OpenAI Spinning Up</a></em></p>

<p>There are many RL algorithms present that we can use to train the policy as you can see from the image above, But most of them are developed from two central methods:</p>

<ol>
  <li><strong>Policy based methods</strong> : Directly, by teaching the agent to learn which action to take, given the current state</li>
  <li><strong>Value based methods</strong> : Indirectly, teach the agent to learn which state is more valuable and then take the action that leads to the more valuable states</li>
</ol>

<p><img src="/assets/blog_assets/evolution_of_llms/rl_policy_value.webp" alt="Image of RL" />
<em>Image taken from <a href="https://huggingface.co/learn/deep-rl-course/en/unit1/two-methods">HuggingFace Course</a></em></p>

<p>(Don’t get scared by the equations, I will explain them as we move forward. Also, this was a quick recap of RL, for a better deep dive. Consider going through the <a href="https://huggingface.co/learn/deep-rl-course/en/unit0/introduction">HF course</a>)</p>

<p>As this section is dedicated to PPO, I will primarily be talking about the topics concerned with it. It can broadly be put in the following order:</p>

<ol>
  <li>Policy Gradient Methods</li>
  <li>TRPO</li>
  <li>PPO</li>
</ol>

<p>I am skipping over many other interesting and amazing algorithms like <a href="https://en.wikipedia.org/wiki/Q-learning#:~:text=Q%2Dlearning%20can%20identify%20an,taken%20in%20a%20given%20state.">Q-Learning</a>, <a href="https://docs.pytorch.org/tutorials/intermediate/reinforcement_q_learning.html">DQN</a>, <a href="https://en.wikipedia.org/wiki/Actor-critic_algorithm">Actor-critic</a> etc. As they are not relevant to this section. I still implore you to explore them through the links I have provided to get a better, broader and deeper grasp of RL.</p>

<p>Before we move to the next section, I want to talk about a question that baffled me when I started learning about RL.</p>

<blockquote>
  <p>“Why do we need a value based approach”</p>
</blockquote>

<p>Policy based approach seem to work great and are intuitive as well, given a state, choose an action. Then why do we use value based approaches. Needless complexity. Think for a minute then see the answer</p>

<details>
<summary>Answer</summary>
<div>

    <p><strong>Value-based methods shine in scenarios where policy-based methods struggle:</strong></p>

    <p><strong>1. Discrete Action Spaces with Clear Optimal Actions</strong>
In environments like Atari games or grid worlds, there’s often a single best action for each state. Value-based methods (like DQN) can directly learn which action has the highest expected return, making them sample-efficient for these deterministic scenarios.</p>

    <p><strong>2. Exploration Efficiency</strong>
Value functions provide natural exploration strategies. Methods like ε-greedy or UCB can systematically explore based on value estimates. Policy methods often struggle with exploration, especially in sparse reward environments where random policy perturbations rarely discover good behavior.</p>

    <p><strong>3. Off-Policy Learning</strong>
Value-based methods can learn from any data - even old experiences stored in replay buffers. This makes them incredibly sample-efficient. Policy methods traditionally required on-policy data, though modern techniques like importance sampling have bridged this gap.</p>

    <p><strong>4. Computational Efficiency</strong>
In discrete action spaces, value-based methods often require just one forward pass to select an action (argmax over Q-values). Policy methods might need to sample from complex probability distributions or solve optimization problems.</p>

    <p><strong>Where Policy Methods Fail:</strong></p>

    <ul>
      <li><strong>High-dimensional discrete actions</strong>: Computing argmax becomes intractable</li>
      <li><strong>Continuous control</strong>: You can’t enumerate all possible actions to find the maximum</li>
      <li><strong>Stochastic optimal policies</strong>: Sometimes the best strategy is inherently random (like rock-paper-scissors), which value methods can’t represent directly</li>
    </ul>

    <p>The truth is, both approaches are complementary tools for different types of problems.</p>

  </div>
</details>
<p><br /></p>

<h5 id="policy-gradient-methods">Policy Gradient Methods</h5>

<p>Policy gradient methods directly optimize a policy function by adjusting its parameters in the direction of greater expected rewards. They work by:</p>

<ol>
  <li>Collecting experience (state-action pairs and rewards) using the current policy</li>
  <li>Estimating the policy gradient (the direction that would improve the policy)</li>
  <li>Updating the policy parameters using this gradient</li>
</ol>

<p><strong>The Gradient Estimator</strong></p>

<p>In our discussion so far, we talked about deterministic policy based methods. Ie given a state, choose an action $\pi(s) = a$. But when we are talking about policy gradients, we use a stochastic policy based method. Ie given a state, return a probability distribution of actions $\pi(a|s) = P[A|s]$.</p>

<p><img src="/assets/blog_assets/evolution_of_llms/probabilstic_rl.webp" alt="Image of RL" /></p>

<p>We also need to be aware of a few terms and mathematical tricks before moving forward:</p>

<ol>
  <li>
    <p><strong>Trajectory</strong>: A series of state action pair is called a trajectory.</p>

\[\tau = (s_1,a_1,s_2,a_2,\ldots,s_H,a_H)\]
  </li>
  <li>
    <p><strong>Log derivative trick</strong>:</p>

\[\nabla_\theta \log z = \frac{1}{z} \nabla_\theta z\]

    <p>This trick allows us to convert the gradient of a probability into the gradient of its logarithm, which is computationally more stable and easier to work with.</p>

    <p>(To derive it just apply chain rule and know that the derivative of $\log(x)$ = $1/x$)</p>
  </li>
  <li>
    <p><strong>Definition of Expectation</strong>:</p>

    <p>For discrete distributions:
\(\mathbb{E}_{x \sim p(x)}[f(x)] = \sum_x p(x)f(x) \tag{1}\)</p>

    <p>For continuous distributions:
\(\mathbb{E}_{x \sim p(x)}[f(x)] = \int_x p(x)f(x) \, dx \tag{2}\)</p>

    <p>If you are new to the idea of expectation, Consider checking this amazing <a href="https://www.countbayesie.com/blog/2015/2/20/random-variables-and-expectation">blog</a> on the topic.</p>
  </li>
</ol>

<p><strong>Deriving the Policy Gradient</strong></p>

<p>Let $\tau$ be a trajectory (sequence of state-action pairs), $\theta$ be the weights of our neural network policy. Our policy $\pi_\theta$ outputs action probabilities that depend upon the current state and network weights.</p>

<p>We begin with the reward hypothesis: we want to maximize $R(\tau)$ where $\tau$ is a trajectory.</p>

<p>We can write the objective as the <strong>probability of a trajectory being chosen by the policy multiplied by the reward for that trajectory</strong>:</p>

\[J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta}[R(\tau)] = \sum_\tau \pi_\theta(\tau)R(\tau)\]

<p>This formulation is crucial because it connects:</p>

<ul>
  <li>$\pi_\theta(\tau)$: How likely our current policy is to generate trajectory $\tau$</li>
  <li>$R(\tau)$: How much reward we get from that trajectory</li>
</ul>

<p>For continuous trajectory spaces, we can write this as:</p>

\[J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta}[R(\tau)] = \int \pi_\theta(\tau)R(\tau)d\tau\]

<p>Now we can derive the policy gradient by taking the gradient of our objective:</p>

\[\nabla_\theta J(\theta) = \nabla_\theta \int \pi_\theta(\tau)R(\tau)d\tau \tag{3}\]

\[= \int \nabla_\theta \pi_\theta(\tau)R(\tau)d\tau \tag{4}\]

\[= \int \pi_\theta(\tau) \frac{\nabla_\theta \pi_\theta(\tau)}{\pi_\theta(\tau)} R(\tau)d\tau \tag{5}\]

\[= \int \pi_\theta(\tau) \nabla_\theta \log \pi_\theta(\tau) R(\tau)d\tau \tag{6}\]

\[= \mathbb{E}_{\tau \sim \pi_\theta}[\nabla_\theta \log \pi_\theta(\tau) R(\tau)] \tag{7}\]

<p><strong>Step-by-step explanation:</strong></p>

<ul>
  <li><strong>(3)</strong> Start with gradient of our objective function</li>
  <li><strong>(4)</strong> Push gradient inside the integral</li>
  <li><strong>(5)</strong> Multiply and divide by $\pi_\theta(\tau)$</li>
  <li><strong>(6)</strong> Apply the log derivative trick: $\nabla_\theta \log(z) = \frac{1}{z} \nabla_\theta z$</li>
  <li><strong>(7)</strong> Convert back to expectation form</li>
</ul>

<p>The trajectory probability factors as:
\(\pi_\theta(\tau) = \prod_{t=0}^{T} \pi_\theta(a_t|s_t)\)</p>

<p>So the log probability becomes:
\(\log \pi_\theta(\tau) = \sum_{t=0}^{T} \log \pi_\theta(a_t|s_t)\)</p>

<p>What does this mean for us? If you want to maximize your expected reward, you can use gradient ascent. The gradient of the expected reward has an elegant form - it’s simply <strong>the expectation of the trajectory return times the sum of log probabilities of actions taken in that trajectory</strong>.</p>

<p>In reinforcement learning, a trajectory $\tau = (s_1, a_1, s_2, a_2, \ldots, s_T, a_T)$ is generated through a sequential process. The probability of observing this specific trajectory under policy $\pi_\theta$ comes from the <a href="https://en.wikipedia.org/wiki/Chain_rule_(probability)"><strong>chain rule of probability</strong></a>.</p>

<table>
  <tbody>
    <tr>
      <td>This is quite complex to intuitively understand in my opinion. Consider going through this <a href="https://stats.stackexchange.com/questions/585038/question-for-the-derivation-of-the-probability-of-a-trajectory">stack exchange</a>. <br /> <br /> Intuition: Let’s calculate the joint probability of a sequence like $P(\text{sunny weather, white shirt, ice cream})$ - what’s the chance it’s sunny outside, I’m wearing a white shirt, and I chose to eat ice cream all happening together? <br /> <br /> We can break this down step by step: First, what’s the probability it’s sunny outside? That’s $P(\text{sunny})$. Given that it’s sunny, what are the chances I wear a white shirt? That’s $P(\text{white shirt | sunny})$. Finally, given it’s sunny and I’m wearing white, what’s the probability I eat ice cream? That’s $P(\text{ice cream | sunny, white shirt})$.<br /> <br /> \(P(\text{sunny, white shirt, ice cream}) = P(\text{sunny}) \cdot P(\text{white shirt | sunny}) \cdot P(\text{ice cream | sunny, white shirt})\)<br /> <br /> By multiplying these conditional probabilities, we get the full joint probability. In reinforcement learning, trajectories work the same way: $P(s_1, a_1, s_2, a_2, \ldots)$ breaks down into “what state do we start in?” then “what action do we take?” then “where do we transition?” and so on. Each step depends only on what happened before, making complex trajectory probabilities manageable to compute and optimize.</td>
    </tr>
  </tbody>
</table>

<p>The joint probability of a sequence of events can be factored as:
\(P(s_1, a_1, s_2, a_2, \ldots, s_T, a_T) = P(s_1) \cdot P(a_1|s_1) \cdot P(s_2|s_1, a_1) \cdot P(a_2|s_1, a_1, s_2) \cdots\)</p>

<p>However, in the <strong>Markov Decision Process (MDP) setting</strong>, we have two key assumptions:</p>

<ol>
  <li><strong>Markov Property</strong>: Next state depends only on current state and action: $P(s_{t+1}|s_1, a_1, \ldots, s_t, a_t) = P(s_{t+1}|s_t, a_t)$</li>
  <li><strong>Policy Markov Property</strong>: Action depends only on current state: $P(a_t|s_1, a_1, \ldots, s_t) = \pi_\theta(a_t|s_t)$</li>
</ol>

<table>
  <tbody>
    <tr>
      <td>Chapter 3 of <a href="http://incompleteideas.net/book/RLbook2020.pdf">RL book by Sutton and Barto</a> covers the topic well</td>
    </tr>
  </tbody>
</table>

<p>Applying these assumptions:</p>

\[\pi_\theta(\tau) = \pi_\theta(s_1, a_1, \ldots, s_T, a_T) = p(s_1) \prod_{t=1}^{T} \pi_\theta(a_t|s_t)p(s_{t+1}|s_t, a_t)\]

\[\underbrace{p(s_1) \prod_{t=1}^{T} \pi_\theta(a_t|s_t)p(s_{t+1}|s_t, a_t)}_{\pi_\theta(\tau)}\]

<ul>
  <li>$p(s_1)$: Initial state distribution (environment dependent)</li>
  <li>$\pi_\theta(a_t|s_t)$: Policy probability of choosing action $a_t$ in state $s_t$</li>
  <li>$p(s_{t+1}|s_t, a_t)$: Environment transition probability (environment dependent)</li>
</ul>

<p>When we take the log of a product, it becomes a sum:</p>

\[\log \pi_\theta(\tau) = \log p(s_1) + \sum_{t=1}^{T} \log \pi_\theta(a_t|s_t) + \sum_{t=1}^{T} \log p(s_{t+1}|s_t, a_t)\]

<p>The first and last terms do not depend on $\theta$ and can be removed when taking gradients(and this is often done in practice):</p>

<ul>
  <li>$\log p(s_1)$: Initial state is determined by environment, not our policy</li>
  <li>$\log p(s_{t+1}|s_t, a_t)$: Environment dynamics don’t depend on our policy parameters</li>
</ul>

\[\nabla_\theta \left[ \log p(s_1) + \sum_{t=1}^{T} \log \pi_\theta(a_t|s_t) + \sum_{t=1}^{T} \log p(s_{t+1}|s_t, a_t) \right]\]

\[= \nabla_\theta \left[ \cancel{\log p(s_1)} + \sum_{t=1}^{T} \log \pi_\theta(a_t|s_t) + \cancel{\sum_{t=1}^{T} \log p(s_{t+1}|s_t, a_t)} \right]\]

<p>Therefore:
\(\nabla_\theta \log \pi_\theta(\tau) = \nabla_\theta \sum_{t=1}^{T} \log \pi_\theta(a_t|s_t) = \sum_{t=1}^{T} \nabla_\theta \log \pi_\theta(a_t|s_t)\)</p>

<p>So the policy gradient:
\(\nabla_\theta J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta}[\nabla_\theta \log \pi_\theta(\tau) R(\tau)]\)</p>

<p>becomes:
\(\nabla_\theta J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta}\left[\left(\sum_{t=1}^{T} \nabla_\theta \log \pi_\theta(a_t|s_t)\right) R(\tau)\right]\)</p>

<p>The trajectory return $R(\tau)$ is the total reward collected along the trajectory:
\(R(\tau) = \sum_{t=1}^{T} r(s_t, a_t)\)</p>

<p>So our gradient becomes:
\(\nabla_\theta J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta}\left[\left(\sum_{t=1}^{T} \nabla_\theta \log \pi_\theta(a_t\|s_t)\right) \left(\sum_{t=1}^{T} r(s_t, a_t)\right)\right]\)</p>

<p><strong>How do we compute expectations in practice?</strong></p>

<p>We can’t compute the expectation $\mathbb{E}_{\tau \sim \pi{\theta}}[\cdot]$ analytically because:</p>

<ul>
  <li>There are infinitely many possible trajectories</li>
  <li>We don’t know the environment dynamics $p(s_{t+1}|s_t, a_t)$</li>
</ul>

<p>Instead, we use <a href="https://en.wikipedia.org/wiki/Monte_Carlo_method"><strong>Monte Carlo sampling</strong></a>:</p>

<ol>
  <li>Collect $N$ sample trajectories by running our current policy: ${\tau_1, \tau_2, \ldots, \tau_N}$</li>
  <li>Approximate the expectation using the sample average:</li>
</ol>

\[\mathbb{E}_{\tau \sim \pi_\theta}[f(\tau)] \approx \frac{1}{N} \sum_{i=1}^{N} f(\tau_i)\]

<p><strong>Applying Monte Carlo approximation</strong></p>

<p>This is a fabulous <a href="https://www.youtube.com/watch?v=7ESK5SaP-bc&amp;ab_channel=MarbleScience">video</a> to understand Monte Carlo approximation.</p>

<p>Substituting our specific function:
\(f(\tau) = \left(\sum_{t=1}^{T} \nabla_\theta \log \pi_\theta(a_t|s_t)\right) \left(\sum_{t=1}^{T} r(s_t, a_t)\right)\)</p>

<p>We get:
\(\boxed{\nabla_\theta J(\theta) \approx \frac{1}{N} \sum_{i=1}^{N} \left(\sum_{t=1}^{T} \nabla_\theta \log \pi_\theta(a_{i,t}|s_{i,t})\right) \left(\sum_{t=1}^{T} r(s_{i,t}, a_{i,t})\right)}\)</p>

\[\boxed{\theta \leftarrow \theta + \alpha \nabla_\theta J(\theta)}\]

<p>Where:</p>

<ul>
  <li>$i$ indexes the sampled trajectories ($1$ to $N$)</li>
  <li>$t$ indexes time steps within each trajectory ($1$ to $T$)</li>
  <li>$(s_{i,t}, a_{i,t})$ is the state-action pair at time $t$ in trajectory $i$</li>
</ul>

<p>The elegant result is that we only need gradients of our policy’s action probabilities - the environment dynamics completely disappear from our gradient computation! This makes policy gradients model-free and widely applicable.</p>

<p>And we use this policy gradient to update the policy $\theta$.</p>

<p>To get an intuition behind the idea consider reading the intuition part of this <a href="https://jonathan-hui.medium.com/rl-policy-gradients-explained-9b13b688b146">blog</a>.</p>

<p><strong>Policy Gradient for Continuous Space</strong></p>

<p>So far, we’ve been working with discrete action spaces, like our super mad bot game where you can move left, move right, or press A to shoot. But what happens when your agent needs to control a robot arm, steer a car, or even select the “best” next token in language model fine-tuning? Welcome to the world of continuous control!</p>

<p>In discrete spaces, our policy outputs probabilities for each possible action:</p>

<ul>
  <li>Move left: 30%</li>
  <li>Move right: 45%</li>
  <li>Shoot: 25%</li>
</ul>

<p>But in continuous spaces, actions are real numbers. Imagine trying to control a robot arm where the joint angle can be any value between -180° and +180°. You can’t enumerate probabilities for every possible angle, there are infinitely many! (like in real numbers, you cannot even count the numbers present between 179 and 180… Where do you even begin?)</p>

<p>The solution is to make our neural network output <strong>parameters of a probability distribution</strong> (eg mean and standard deviation of a <a href="https://en.wikipedia.org/wiki/Normal_distribution">normal distribution</a>) instead of individual action probabilities. Specifically, we use a Gaussian (normal) distribution.</p>

<p>Here’s how it works:</p>

<p>Instead of: $\pi_\theta(a_t|s_t) = \text{[probability for each discrete action]}$ <br />
We use: $\pi_\theta(a_t|s_t) = \mathcal{N}(f_{\text{neural network}}(s_t); \Sigma)$</p>

<p>Let’s break it down:</p>

<ol>
  <li><strong>Feed the state</strong> $s_t$ into your neural network</li>
  <li><strong>Network outputs the mean</strong> $\mu = f_{\text{neural network}}(s_t)$ - this is the “preferred” action</li>
  <li><strong>Choose a covariance matrix</strong> $\Sigma$ - this controls how much exploration/uncertainty around that mean</li>
  <li><strong>Sample the actual action</strong> from the Gaussian: $a_t \sim \mathcal{N}(\mu, \Sigma)$</li>
</ol>

<p>Now comes the amazing part. Remember our policy gradient formula?</p>

\[\nabla_\theta J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta}\left[\left(\sum_{t=1}^{T} \nabla_\theta \log \pi_\theta(a_t|s_t)\right) R(\tau)\right]\]

<p>The <strong>exact same formula still applies!</strong> We just need to compute $\nabla_\theta \log \pi_\theta(a_t|s_t)$ differently.</p>

<p>Let’s start with what a <a href="https://en.wikipedia.org/wiki/Multivariate_normal_distribution">Multivariant Gaussian distribution</a> actually looks like. For continuous actions, we assume they follow this probability density function:</p>

\[f(x) = \frac{1}{(2\pi)^{d/2} |\Sigma|^{1/2}} \exp\left\{-\frac{1}{2}(x - \mu)^T \Sigma^{-1} (x - \mu)\right\}\]

<p>This looks scary, but it’s just the mathematical way of saying: “actions are most likely to be near the mean $\mu$, with spread determined by covariance $\Sigma$.”</p>

<p>(To understand where this idea comes from, read <a href="http://incompleteideas.net/book/RLbook2020.pdf">13.7</a> from RL by Sutton and Barton)</p>

<p>Now, since our policy $\pi_\theta(a_t|s_t) = \mathcal{N}(f_{\text{neural network}}(s_t); \Sigma)$, we have:</p>

\[\log \pi_\theta(a_t|s_t) = \log f(a_t)\]

<p>Taking the logarithm of our Gaussian:</p>

\[\log \pi_\theta(a_t|s_t) = \log\left[\frac{1}{(2\pi)^{d/2} |\Sigma|^{1/2}} \exp\left\{-\frac{1}{2}(a_t - \mu)^T \Sigma^{-1} (a_t - \mu)\right\}\right]\]

<p>Using properties of logarithms ($\log(AB) = \log A + \log B$ and $\log(e^x) = x$):</p>

\[\log \pi_\theta(a_t|s_t) = \log\left[\frac{1}{(2\pi)^{d/2} |\Sigma|^{1/2}}\right] - \frac{1}{2}(a_t - \mu)^T \Sigma^{-1} (a_t - \mu)\]

<p>The first term is just a constant (doesn’t depend on our neural network parameters $\theta$), so we can ignore it when taking gradients:</p>

\[\log \pi_\theta(a_t|s_t) = -\frac{1}{2}(a_t - \mu)^T \Sigma^{-1} (a_t - \mu) + \text{const}\]

<p>Since $\mu = f_{\text{neural network}}(s_t)$, we can rewrite this as:</p>

\[\log \pi_\theta(a_t|s_t) = -\frac{1}{2}||f(s_t) - a_t||^2_\Sigma + \text{const}\]

<p>Both the above equations are the same, it’s just a shorthand of writing it this way. It is also known as <a href="https://en.wikipedia.org/wiki/Mahalanobis_distance">Mahalanobis distance</a> squared.</p>

<p>Now we can compute the gradient with respect to our network parameters $\theta$:</p>

\[\nabla_\theta \log \pi_\theta(a_t|s_t) = \nabla_\theta \left[-\frac{1}{2}(a_t - f(s_t))^T \Sigma^{-1} (a_t - f(s_t))\right]\]

<p>Let’s define $u = a_t - f(s_t)$ to simplify notation. Our expression becomes:</p>

\[\nabla_\theta \log \pi_\theta(a_t|s_t) = \nabla_\theta \left[-\frac{1}{2} u^T \Sigma^{-1} u\right]\]

<p>Since $a_t$ and $\Sigma^{-1}$ don’t depend on $\theta$, we have:</p>

\[\frac{\partial u}{\partial \theta} = \frac{\partial}{\partial \theta}(a_t - f(s_t)) = -\frac{\partial f(s_t)}{\partial \theta}\]

<p>For the quadratic form $u^T \Sigma^{-1} u$, using the chain rule:</p>

\[\frac{\partial}{\partial \theta}(u^T \Sigma^{-1} u) = \frac{\partial u^T}{\partial \theta} \Sigma^{-1} u + u^T \Sigma^{-1} \frac{\partial u}{\partial \theta}\]

<p>Since $\Sigma^{-1}$ is symmetric, we can write:</p>

\[\frac{\partial}{\partial \theta}(u^T \Sigma^{-1} u) = 2 u^T \Sigma^{-1} \frac{\partial u}{\partial \theta}\]

<p>Substituting back our expressions:</p>

\[\nabla_\theta \log \pi_\theta(a_t|s_t) = -\frac{1}{2} \cdot 2 \cdot u^T \Sigma^{-1} \frac{\partial u}{\partial \theta}\]

\[= -u^T \Sigma^{-1} \left(-\frac{\partial f(s_t)}{\partial \theta}\right)\]

\[= u^T \Sigma^{-1} \frac{\partial f(s_t)}{\partial \theta}\]

<p>Substituting $u = a_t - f(s_t)$ back:</p>

\[\nabla_\theta \log \pi_\theta(a_t|s_t) = (a_t - f(s_t))^T \Sigma^{-1} \frac{\partial f(s_t)}{\partial \theta}\]

<p>Since $\Sigma^{-1}$ is symmetric, $(a_t - f(s_t))^T \Sigma^{-1} = \Sigma^{-1}(a_t - f(s_t))$ when treated as a row vector, so we can write:</p>

\[\nabla_\theta \log \pi_\theta(a_t|s_t) = \Sigma^{-1}(a_t - f(s_t)) \frac{\partial f(s_t)}{\partial \theta}\]

<p>Rearranging to match the original form:</p>

\[\nabla_\theta \log \pi_\theta(a_t|s_t) = -\Sigma^{-1}(f(s_t) - a_t) \frac{\partial f(s_t)}{\partial \theta}\]

<p>This gradient has a beautiful intuitive interpretation:</p>

<ul>
  <li><strong>$(f(s_t) - a_t)$</strong>: The difference between what your network predicted and the action you actually took</li>
  <li><strong>$\frac{df}{d\theta}$</strong>: How to change the network parameters to affect the output</li>
  <li><strong>$\Sigma^{-1}$</strong>: Weighting factor (less weight for high-variance directions)</li>
</ul>

<p>When you collect experience and compute rewards, here’s what happens:</p>

<ol>
  <li><strong>Good action taken</strong> ($R(\tau) &gt; 0$): The gradient pushes $f(s_t)$ closer to the good action $a_t$</li>
  <li><strong>Bad action taken</strong> ($R(\tau) &lt; 0$): The gradient pushes $f(s_t)$ away from the bad action $a_t$</li>
  <li><strong>Standard backpropagation</strong>: This gradient flows back through the network to update $\theta$</li>
</ol>

<p>Our policy gradient update remains:
\(\theta \leftarrow \theta + \alpha \nabla_\theta J(\theta)\)</p>

<p>The <strong>only difference</strong> is how we compute $\nabla_\theta \log \pi_\theta(a_t|s_t)$:</p>

<ul>
  <li><strong>Discrete case</strong>: Gradient of softmax probabilities</li>
  <li><strong>Continuous case</strong>: Gradient of Gaussian log-likelihood (what we just derived!)</li>
</ul>

<p>Everything else stays identical - collect trajectories, compute returns, update parameters. The same core algorithm seamlessly handles both discrete and continuous control problems!</p>

<p><strong>Policy Gradient Improvements</strong></p>

<p>There are two methods in which RL is trained</p>

<ol>
  <li>Monte Carlo Learning: Cummulative reward of the entire episode (Entire run of the enviorment)</li>
  <li>Temporal Difference Learning: Reward is used to update policy in every step</li>
</ol>

<p><img src="/assets/blog_assets/evolution_of_llms/mc_vs_td.webp" alt="Image of MC vs TD" />
<em>Image taken from <a href="https://www.researchgate.net/publication/364732848_Reinforcement_Learning_and_Bandits_for_Speech_and_Language_Processing_Tutorial_Review_and_Outlook">Reinforcement Learning and Bandits for Speech and Language Processing: Tutorial, Review and Outlook</a></em></p>

<p>Policy Gradient (PG) uses MC this causes it to have low bias (Expected reward is close to actual reward, as the same policy is used throughout the run) but high variance (Some runs produce great results, some really bad).</p>

<table>
  <tbody>
    <tr>
      <td>A <a href="https://ai.stackexchange.com/questions/22118/what-is-the-bias-variance-trade-off-in-reinforcement-learning">stack exchange</a> on bias &amp; variance in RL</td>
    </tr>
  </tbody>
</table>

<p>Remember, our policy gradient formula is:</p>

\[\nabla_\theta J(\theta) \approx \frac{1}{N} \sum_{i=1}^{N} \left(\sum_{t=1}^{T} \nabla_\theta \log \pi_\theta(a_{i,t}|s_{i,t})\right) \left(\sum_{t=1}^{T} r(s_{i,t}, a_{i,t})\right)\]

<p>We can rewrite this more compactly as:</p>

\[\nabla_\theta J(\theta) \approx \frac{1}{N} \sum_{i=1}^{N} \sum_{t=1}^{T} \nabla_\theta \log \pi_\theta(a_{i,t}|s_{i,t}) \cdot Q(s_{i,t}, a_{i,t})\]

<p>Where $Q(s,a)$ represents the <strong>total reward we get from taking action $a$ in state $s$</strong> (this is called the Q-function or action-value function).</p>

<p><strong>The Baseline Trick</strong></p>

<p>Here’s a mathematical insight: <strong>we can subtract any term from our gradient as long as that term doesn’t depend on our policy parameters $\theta$.</strong></p>

<p>Why? Because:
\(\nabla_\theta [f(\theta) - c] = \nabla_\theta f(\theta) - \nabla_\theta c = \nabla_\theta f(\theta) - 0 = \nabla_\theta f(\theta)\)</p>

<p>So instead of using $Q(s,a)$ directly, we can use $Q(s,a) - V(s)$, where $V(s)$ is some baseline function.</p>

<p>The most natural choice for baseline is $V(s) =$ <strong>the expected reward from state $s$</strong> (The value function). This represents “how good is this state on average?”</p>

<p>Our new gradient becomes:
\(\nabla_\theta J(\theta) \approx \frac{1}{N} \sum_{i=1}^{N} \sum_{t=1}^{T} \nabla_\theta \log \pi_\theta(a_{i,t}|s_{i,t}) \cdot (Q(s_{i,t}, a_{i,t}) - V(s_{i,t}))\)</p>

<p>This is defined as the <strong>Advantage Function</strong>:
\(A^{\pi}(s,a) = Q^{\pi}(s,a) - V^{\pi}(s)\)</p>

<p>The advantage function answers the question: <strong>“How much better is taking action $a$ in state $s$ compared to the average action in that state?”</strong></p>

<ul>
  <li><strong>$A(s,a) &gt; 0$</strong>: Action $a$ is better than average → increase its probability</li>
  <li><strong>$A(s,a) &lt; 0$</strong>: Action $a$ is worse than average → decrease its probability</li>
  <li><strong>$A(s,a) = 0$</strong>: Action $a$ is exactly average → no change needed</li>
</ul>

<p>Our final policy gradient becomes:
\(\boxed{\nabla_\theta J(\theta) \approx \frac{1}{N} \sum_{i=1}^{N} \sum_{t=1}^{T} \nabla_\theta \log \pi_\theta(a_{i,t}|s_{i,t}) \cdot A^{\pi}(s_{i,t}, a_{i,t})}\)</p>

<p>Let’s understand why this reduces variance with an example:</p>

<p><strong>Situation 1</strong>: Trajectory A gets +10 rewards, Trajectory B gets -10 rewards</p>

<ul>
  <li>If average performance is 0: $A_A = +10$, $A_B = -10$</li>
  <li>Result: Increase A’s probability, decrease B’s probability ✓</li>
</ul>

<p><strong>Situation 2</strong>: Trajectory A gets +10 rewards, Trajectory B gets +1 rewards</p>

<ul>
  <li>If average performance is +5.5: $A_A = +4.5$, $A_B = -4.5$</li>
  <li>Result: Increase A’s probability, decrease B’s probability ✓</li>
</ul>

<p>Even when both trajectories have positive rewards, the advantage function correctly identifies which one is relatively better!</p>

<p>In deep learning, we want input features to be zero-centered. The advantage function does exactly this for our rewards:</p>

<ul>
  <li><strong>Without baseline</strong>: All positive rewards → always increase probabilities</li>
  <li><strong>With advantage</strong>: Rewards centered around zero → increase good actions, decrease bad ones</li>
</ul>

<p>This gives our policy gradient much clearer, less conflicting signals, significantly reducing variance and improving convergence.</p>

<p><strong>Vanilla Policy Gradient Algorithm</strong></p>

<p>Now that we understand the advantage function, let’s see how it all comes together in the complete algorithm:</p>

\[\nabla U(\theta) \approx \hat{g} = \frac{1}{m} \sum_{i=1}^{m} \nabla_\theta \log P(\tau^{(i)}; \theta)(R(\tau^{(i)}) - b)\]

<p>(The notation may change from paper to paper, but the core idea remains the same)</p>

<p><img src="/assets/blog_assets/evolution_of_llms/vanilla_pg.webp" alt="Image of policy based approach" />
<em>Image taken from <a href="https://jonathan-hui.medium.com/rl-policy-gradients-explained-9b13b688b146">RL — Policy Gradient Explained</a></em></p>

<p><strong>Reward Discount</strong></p>

<p>There’s one more important technique that further reduces variance: <strong>reward discounting</strong>.</p>

<p>Reward discount reduces variance by reducing the impact of distant actions. The intuition is that actions taken now should have more influence on immediate rewards than on rewards received far in the future.</p>

<p>You can think of it in terms of money, would rather have money right now, or have it later.</p>

<p>Instead of using the raw cumulative reward, we use a <strong>discounted return</strong>:</p>

\[Q^{\pi,\gamma}(s, a) \leftarrow r_0 + \gamma r_1 + \gamma^2 r_2 + \cdots | s_0 = s, a_0 = a\]

<p>Where:</p>

<ul>
  <li>$\gamma \in [0,1]$ is the <strong>discount factor</strong></li>
  <li>$\gamma = 0$: Only immediate rewards matter</li>
  <li>$\gamma = 1$: All future rewards are equally important</li>
  <li>$\gamma \approx 0.99$: Common choice that slightly prioritizes near-term rewards</li>
</ul>

<p>The corresponding objective function becomes:</p>

\[\nabla_\theta J(\theta) \approx \frac{1}{N} \sum_{i=1}^{N} \sum_{t=1}^{T} \nabla_\theta \log \pi_\theta(a_{i,t}|s_{i,t}) \left(\sum_{t'=t}^{T} \gamma^{t'-t} r(s_{i,t'}, a_{i,t'})\right)\]

<p>Why Discounting Helps:</p>

<ul>
  <li><strong>Reduces variance</strong>: Distant rewards have less influence, so random events far in the future don’t dominate the gradient</li>
  <li><strong>Focuses learning</strong>: The agent learns to optimize for more predictable, near-term outcomes</li>
  <li><strong>Mathematical stability</strong>: Prevents infinite returns in continuing tasks</li>
</ul>

<p>All of this comprises the complete Vanila Policy Gradient Algorithm which serves as the foundation for more advanced methods like PPO, TRPO, and GRPO, which we’ll explore in subsequent sections.</p>

<h5 id="trpo">TRPO</h5>

<p><strong>The Sample Efficiency Problem</strong></p>

<p>Our vanilla policy gradient algorithm works, but it has a critical flaw that makes it impractical for real-world applications. Let’s examine what happens during training:</p>

<ol>
  <li><strong>Collect trajectories</strong> using current policy π_θ</li>
  <li><strong>Compute gradients</strong> from these trajectories</li>
  <li><strong>Update policy</strong> θ → θ_new</li>
  <li><strong>Throw away all previous data</strong> and start over</li>
</ol>

<p>This last step is the problem. Imagine training a robot to walk - every time you make a small adjustment to the policy, you must collect entirely new walking data and discard everything you learned before. For complex tasks requiring thousands of timesteps per trajectory, this becomes computationally prohibitive.</p>

<p>Recall our policy gradient formula:</p>

\[\nabla_\theta J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta}\left[\sum_{t=1}^{T} \nabla_\theta \log \pi_\theta(a_t|s_t) \cdot A(s_t, a_t)\right]\]

<p>The expectation $\mathbb{E}_{\tau \sim \pi \theta}$ means we must sample trajectories using the current policy π_θ. When we update θ, this distribution changes, invalidating all our previous samples.</p>

<p><strong>Importance Sampling</strong></p>

<p>What if we could reuse old data to estimate the performance of our new policy? This is exactly what <a href="https://stats.stackexchange.com/questions/254114/what-is-importance-sampling">importance sampling</a> enables. The core idea is beautifully simple:</p>

<table>
  <tbody>
    <tr>
      <td><strong>If you want to compute an expectation under distribution p, but you have samples from distribution q, you can reweight the samples by the ratio p/q.</strong></td>
    </tr>
  </tbody>
</table>

<p>For any function f(x), the expectation under distribution p can be computed as:</p>

\[\mathbb{E}_{x \sim p}[f(x)] = \sum_x p(x)f(x)\]

<p>But using importance sampling, we can compute this same expectation using samples from a different distribution q:</p>

\[\mathbb{E}_{x \sim p}[f(x)] = \sum_x p(x)f(x) = \sum_x \frac{p(x)}{q(x)} \cdot q(x)f(x) = \mathbb{E}_{x \sim q}\left[\frac{p(x)}{q(x)} f(x)\right]\]

<p>The magic happens in that middle step - we multiply and divide by q(x), creating a ratio p(x)/q(x) that reweights our samples.</p>

<p>Let’s see this in action with an example. Suppose we want to compute the expected value of f(x) = x under two different distributions:</p>

<p><strong>Distribution p</strong>: P(x=1) = 0.5, P(x=3) = 0.5<br />
<strong>Distribution q</strong>: P(x=1) = 0.8, P(x=3) = 0.2</p>

<p><strong>Direct calculation under p:</strong>
\(\mathbb{E}_{x \sim p}[f(x)] = 0.5 \times 1 + 0.5 \times 3 = 2.0\)</p>

<p><strong>Using importance sampling with samples from q:</strong></p>

<p>If we sample from q and get samples [1, 1, 1, 3], we can estimate the expectation under p by reweighting:</p>

<p>For x=1: weight = p(1)/q(1) = 0.5/0.8 = 0.625<br />
For x=3: weight = p(3)/q(3) = 0.5/0.2 = 2.5</p>

\[\mathbb{E}_{x \sim p}[f(x)] \approx \frac{1}{4}[0.625 \times 1 + 0.625 \times 1 + 0.625 \times 1 + 2.5 \times 3] = 2.0\]

<p>The reweighted result matches our direct calculation!</p>

<p>Now we can revolutionize our policy gradient approach. Instead of:</p>

\[\mathbb{E}_{\tau \sim \pi_\theta}[f(\tau)]\]

<p>We can use:</p>

\[\mathbb{E}_{\tau \sim \pi_{\theta_{old}}}\left[\frac{\pi_\theta(\tau)}{\pi_{\theta_{old}}(\tau)} f(\tau)\right]\]

<p>Remember that trajectory probabilities factor as:
\(\pi_\theta(\tau) = {p(s_1) \prod_{t=1}^{T} \pi_\theta(a_t|s_t)p(s_{t+1}|s_t, a_t)}\)</p>

<p>The environment dynamics $p(s_{t+1}|s_t, a_t)$ abd $p(s_1)$ are the same for both policies, so they cancel out in the ratio:</p>

\[\frac{\pi_\theta(\tau)}{\pi_{\theta_{old}}(\tau)} = \frac{\prod_{t=1}^{T} \pi_\theta(a_t\|s_t)}{\prod_{t=1}^{T} \pi_{\theta_{old}}(a_t\|s_t)} = \prod_{t=1}^{T} \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}\]

<p>Our objective becomes:</p>

\[J(\theta) = \mathbb{E}_{\tau \sim \pi_{\theta_{old}}}\left[\prod_{t=1}^{T} \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)} \cdot R(\tau)\right]\]

<p>This is huge! We can now:</p>

<ul>
  <li><strong>Collect data</strong> with policy ${\pi_{\theta_{old}}}$</li>
  <li><strong>Reuse this data</strong> multiple times to evaluate different policies ${\pi_{\theta}}$</li>
  <li><strong>Dramatically improve sample efficiency</strong></li>
</ul>

<p>But there’s a catch. Importance sampling works well only when the two distributions are similar. If π<em>θ becomes very different from π</em>θ_old, the probability ratios can explode or vanish:</p>

<ul>
  <li><strong>Ratio » 1</strong>: New policy assigns much higher probability to some actions</li>
  <li><strong>Ratio « 1</strong>: New policy assigns much lower probability to some actions</li>
  <li><strong>Ratio ≈ 0</strong>: Catastrophic - new policy never takes actions the old policy preferred</li>
</ul>

<p>Consider what happens if one action has ratio = 100 while others have ratio = 0.01. A single high-ratio sample can dominate the entire gradient estimate, leading to:</p>

<ul>
  <li><strong>Unstable training</strong>: Gradients vary wildly between batches</li>
  <li><strong>Poor convergence</strong>: The algorithm makes erratic updates</li>
  <li><strong>Sample inefficiency</strong>: We need many more samples to get reliable estimates</li>
</ul>

<p><strong>Constrained Policy Updates</strong></p>

<p>The breakthrough insight: <strong>constrain how much the policy can change</strong> to keep importance sampling ratios well-behaved. This leads us naturally to the concept of trust regions - regions where we trust our importance sampling approximation to be accurate.</p>

<p>But, we must also ask. How do we guarantee that our policy updates always improve performance?</p>

<p>These observations bring us to two key concepts:</p>

<ul>
  <li>The Minorize-Maximization (MM) algorithm</li>
  <li>Trust regions</li>
</ul>

<p><strong>Minorize-Maximization (MM) Algorithm</strong></p>

<p>Can we guarantee that any policy update always improves the expected rewards? This seems impossible, but it’s theoretically achievable through the MM algorithm.</p>

<p>The idea: Instead of directly optimizing the complex true objective η(θ), we iteratively optimize simpler lower bound functions M(θ) that approximate η(θ) locally.</p>

<p>The MM algorithm follows this iterative process:</p>

<ol>
  <li><strong>Find a lower bound</strong> M that approximates the expected reward η locally at the current guess θ_i</li>
  <li><strong>Optimize</strong> the lower bound M to find the next policy guess θ_{i+1}</li>
  <li><strong>Repeat</strong> until convergence</li>
</ol>

<p>For this to work, M must be:</p>

<ul>
  <li><strong>A lower bound</strong>: M(θ) ≤ η(θ) for all θ</li>
  <li><strong>Tight at current point</strong>: M(θ_i) = η(θ_i)</li>
  <li><strong>Easier to optimize</strong>: M should be simpler than η (typically quadratic)</li>
</ul>

<p>The lower bound function has the form:
$M(\theta) = g \cdot (\theta - \theta_{old}) - \frac{1}{2}(\theta - \theta_{old})^T F (\theta - \theta_{old})$</p>

<p>This is a quadratic approximation where:</p>

<ul>
  <li>g is the gradient at θ_old</li>
  <li>F is a positive definite matrix (often related to the <a href="https://en.wikipedia.org/wiki/Hessian_matrix">Hessian</a>)</li>
</ul>

<p><img src="/assets/blog_assets/evolution_of_llms/9.webp" alt="Image of Minorize Maximization algorithm" />
<em>Image taken from <a href="https://jonathan-hui.medium.com/rl-trust-region-policy-optimization-trpo-explained-a6ee04eeeee9">RL — Trust Region Policy Optimization (TRPO) Explained</a></em></p>

<p>If M is a lower bound that never crosses η, then maximizing M must improve η.</p>

<p><strong>Proof sketch</strong>:</p>

<ul>
  <li>Since $M(\theta_{\text{old}}) = \eta(\theta_{\text{old}})$ and $M(\theta) \leq \eta(\theta)$ everywhere</li>
  <li>If we find $\theta_{\text{new}}$ such that $M(\theta_{\text{new}}) &gt; M(\theta_{\text{old}})$</li>
  <li>Then $\eta(\theta_{\text{new}}) \geq M(\theta_{\text{new}}) &gt; M(\theta_{\text{old}}) = \eta(\theta_{\text{old}})$</li>
  <li>Therefore $\eta(\theta_{\text{new}}) &gt; \eta(\theta_{\text{old}})$ ✓</li>
</ul>

<p>In simpler terms, we have a function $\eta(\theta)$ parameterized by $\theta$ (the weights of our neural network). It is not computationally tractable to optimize this function directly. Hence we create a close approximation function $M(\theta)$ using the lower bound function form described above. This approximation comes from the general theory of Minorize-Maximization algorithms (see <a href="https://doi.org/10.1198/016214504000000113">Hunter &amp; Lange, 2004</a>).</p>

<p>This approximation $M(\theta)$ is computationally feasible and easier to optimize. What we have proved here is that as we improve $M(\theta)$, that improvement guarantees we also improve $\eta(\theta)$.</p>

<table>
  <tbody>
    <tr>
      <td><strong>By optimizing a lower bound function approximating η locally, MM guarantees policy improvement every iteration and leads us to the optimal policy eventually.</strong></td>
    </tr>
  </tbody>
</table>

<p><strong>Trust Regions</strong></p>

<p>There are two major optimization paradigms:</p>

<ol>
  <li><strong>Line Search</strong> (like gradient descent): Choose direction first, then step size</li>
  <li><strong>Trust Region</strong>: Choose maximum step size first (the size of the trust region), then find optimal point within that region</li>
</ol>

<p><img src="/assets/blog_assets/evolution_of_llms/line_search_vs_trust_region.webp" alt="Image of Line search vs Trust Region" /></p>

<p>In trust region methods, we:</p>

<ol>
  <li><strong>Define a trust region</strong> of radius δ around current policy θ_old</li>
  <li><strong>Find the optimal policy</strong> within this constrained region</li>
  <li><strong>Adapt the radius</strong> based on how well our approximation worked</li>
</ol>

<p>The optimization problem becomes:
$\max_{\theta} \; M(\theta)$
$\text{subject to} \; |\theta - \theta_{old}| \leq \delta$</p>

<p>Adaptive Trust Region Sizing</p>

<p>The trust region radius δ can be dynamically adjusted:</p>

<ul>
  <li><strong>If approximation is good</strong>: Expand δ for next iteration</li>
  <li><strong>If approximation is poor</strong>: Shrink δ for next iteration</li>
  <li><strong>If policy diverges too much</strong>: Shrink δ to prevent importance sampling breakdown</li>
</ul>

<p>Why Trust Regions Work for RL</p>

<p>In reinforcement learning, trust regions serve a dual purpose:</p>

<ol>
  <li><strong>Mathematical</strong>: Keep our quadratic approximation M valid</li>
  <li><strong>Statistical</strong>: Prevent importance sampling ratios from exploding</li>
</ol>

<p>When policies change too much, both our lower bound approximation AND our importance sampling become unreliable. Trust regions keep us in the safe zone for both.</p>

<details>
<summary>Mathematical Notation Reference</summary>
<div>

    <table>
      <thead>
        <tr>
          <th>Symbol</th>
          <th>Meaning</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>$\pi_\theta(a|s)$</td>
          <td>Policy probability of action a given state s</td>
        </tr>
        <tr>
          <td>$\pi_{\theta_{old}}(a|s)$</td>
          <td>Old policy probability</td>
        </tr>
        <tr>
          <td>$\tau$</td>
          <td>Trajectory $(s_1, a_1, s_2, a_2, \ldots)$</td>
        </tr>
        <tr>
          <td>$\pi_\theta(\tau)$</td>
          <td>Probability of trajectory under policy $\pi_\theta$</td>
        </tr>
        <tr>
          <td>$\frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}$</td>
          <td>Importance sampling ratio for single timestep</td>
        </tr>
        <tr>
          <td>$\prod_{t=1}^{T} \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}$</td>
          <td>Importance sampling ratio for full trajectory</td>
        </tr>
        <tr>
          <td>$R(\tau)$</td>
          <td>Total reward of trajectory</td>
        </tr>
        <tr>
          <td>$A(s_t, a_t)$</td>
          <td>Advantage function</td>
        </tr>
        <tr>
          <td>$\eta(\theta)$</td>
          <td>Expected reward under policy $\pi_\theta$</td>
        </tr>
        <tr>
          <td>$M(\theta)$</td>
          <td>Lower bound function in MM algorithm</td>
        </tr>
        <tr>
          <td>$\theta_{old}$</td>
          <td>Current policy parameters</td>
        </tr>
        <tr>
          <td>$\delta$</td>
          <td>Trust region radius</td>
        </tr>
        <tr>
          <td>$F$</td>
          <td>Positive definite matrix (approximating curvature)</td>
        </tr>
        <tr>
          <td>$g$</td>
          <td>Policy gradient vector</td>
        </tr>
      </tbody>
    </table>

  </div>
</details>
<p><br /></p>

<p><strong>Trust Region Policy Optimization (TRPO)</strong></p>

<p>Now we can finally understand how TRPO elegantly combines all the concepts we’ve explored:</p>

<ol>
  <li><strong>Importance Sampling</strong> - to reuse old data efficiently</li>
  <li><strong>MM Algorithm</strong> - to guarantee policy improvement</li>
  <li><strong>Trust Regions</strong> - to constrain policy changes and keep approximations valid</li>
</ol>

<p>TRPO is a culmination of these ideas into a practical, theoretically-grounded algorithm.</p>

<p>Recall that our original objective was:</p>

\[J(\pi) = \mathbb{E}_{\tau \sim \pi}[R(\tau)]\]

<p>This is the expected return (total reward) when following policy π. Instead of maximizing absolute performance $J(\pi’)$, TRPO maximizes the <strong>policy improvement</strong>:</p>

\[\max_{\pi'} J(\pi') - J(\pi)\]

<p>This is mathematically equivalent to maximizing $J(\pi’)$ (since $J(\pi)$ is constant), but conceptually important - we’re explicitly measuring progress from our current policy.</p>

<p><strong>Why focus on improvement?</strong> Because we can construct better approximations for the improvement $J(\pi’) - J(\pi)$ than for the absolute performance $J(\pi’)$. The MM algorithm works by finding lower bounds for this improvement.</p>

<p>To apply the MM algorithm, TRPO constructs a lower bound function ℒ that uses importance sampling:</p>

\[\mathcal{L}_\pi(\pi') = \frac{1}{1-\gamma} \mathbb{E}_{s\sim d^\pi} \left[ \frac{\pi'(a|s)}{\pi(a|s)} A^\pi(s,a) \right] = \mathbb{E}_{\tau \sim \pi} \left[ \sum_{t=0}^{\infty} \gamma^t \frac{\pi'(a_t|s_t)}{\pi(a_t|s_t)} A^\pi(s_t, a_t) \right]\]

<p>ℒ looks complex, but let’s break this down piece by piece to understand what’s really happening here.</p>

<p>The <strong>discounted state visitation distribution</strong> $d^\pi(s)$ tells us how often we expect to visit each state when following policy π:</p>

\[d^\pi(s) = (1-\gamma) \sum_{t=0}^{\infty} \gamma^t P(s_t = s|\pi)\]

<p>Think of this as a “popularity contest” for states. If γ = 1, this becomes just the regular state visit frequency under policy π. But when γ &lt; 1, we care more about states we visit early in episodes than those we reach later. It’s like asking: “If I run my policy many times, which states will I spend most of my time in, giving more weight to earlier visits?”</p>

<p>The advantage function $A^\pi(s,a)$ we’ve already met - it tells us how much better taking action $a$ in state $s$ is compared to what the policy would do on average in that state.</p>

<p>But here’s where the magic happens. The function ℒ is essentially asking a clever question using importance sampling: “If I reweight all the actions my current policy π took according to how likely my new policy π’ would be to take them, what would my expected advantage be?”</p>

<p>This is brilliant because it lets us estimate how well policy π’ would perform without actually running it in the environment. We just take all our old experience from policy π and reweight it according to the probability ratio $\frac{\pi’(a|s)}{\pi(a|s)}$. When the new policy is more likely to take an action than the old one, we give that experience more weight. When it’s less likely, we give it less weight.</p>

<p>This importance sampling approach is what allows TRPO to reuse old data efficiently - a huge computational win over vanilla policy gradients that throw away all previous experience after each update.</p>

<p>The theoretical foundation comes from this crucial bound (proven in Appendix 2 of the <a href="https://arxiv.org/pdf/1502.05477">TRPO paper</a>):</p>

\[J(\pi') - J(\pi) \geq \mathcal{L}_\pi(\pi') - C\sqrt{\mathbb{E}_{s\sim d^\pi}[D_{KL}(\pi' \| \pi)[s]]}\]

<p>This tells us:</p>

<ul>
  <li><strong>Left side</strong>: True policy improvement</li>
  <li><strong>Right side</strong>: Our lower bound estimate minus a penalty term</li>
</ul>

<p>The penalty term grows with KL divergence, so the bound becomes loose when policies differ too much.</p>

<table>
  <tbody>
    <tr>
      <td>Consider reading this <a href="https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained">blog</a> to get a better idea about KLD</td>
    </tr>
  </tbody>
</table>

<p><img src="/assets/blog_assets/evolution_of_llms/10.webp" alt="Image of KLD" />
<em>Image taken from <a href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence">Wikipedia</a></em></p>

<p>The KL divergence measures how different two probability distributions are:</p>

\[D_{KL}(P \| Q) = \sum_x P(x) \log \frac{P(x)}{Q(x)}\]

<p>For continuous distributions, this becomes:</p>

\[D_{KL}(P \| Q) = \int P(x) \log \frac{P(x)}{Q(x)} dx\]

<p>Think of KL divergence as asking: “If I have samples from distribution P, how surprised would I be if I thought they came from distribution Q instead?” When the distributions are identical, KL divergence is zero. As they become more different, the divergence grows.</p>

<p>TRPO can be formulated in two mathematically equivalent ways:</p>

<p><strong>KL-Penalized (Unconstrained):</strong>
\(\max_{\pi'} \mathcal{L}_\pi(\pi') - C\sqrt{\mathbb{E}_{s\sim d^\pi}[D_{KL}(\pi' \| \pi)[s]]}\)</p>

<p><strong>KL-Constrained:</strong>
\(\max_{\pi'} \mathcal{L}_\pi(\pi')\)
\(\text{subject to } \mathbb{E}_{s\sim d^\pi}[D_{KL}(\pi'||\pi)[s]] \leq \delta\)</p>

<p>These formulations arise directly from the theoretical bound we mentioned earlier:</p>

\[J(\pi') - J(\pi) \geq \mathcal{L}_\pi(\pi') - C\sqrt{\mathbb{E}_{s\sim d^\pi}[D_{KL}(\pi'||\pi)[s]]}\]

<p>The unconstrained version simply maximizes this lower bound directly. The constrained version takes a different approach: instead of penalizing large KL divergences, it prevents them entirely by adding a hard constraint.</p>

<p>These are mathematically equivalent due to <a href="https://en.wikipedia.org/wiki/Duality_(optimization)"><strong>Lagrangian duality</strong></a> - a beautiful result from optimization theory. For every penalty coefficient C in the unconstrained problem, there exists a constraint threshold δ in the constrained problem that gives the same optimal solution. You can think of it like this: instead of saying “I’ll pay a penalty for going over the speed limit,” you’re saying “I absolutely won’t go over the speed limit.” Both approaches can lead to the same driving behavior, just with different enforcement mechanisms.</p>

<p>The lower bound is what we try to maximize to find the optimum $\theta$</p>

<p><img src="/assets/blog_assets/evolution_of_llms/11.webp" alt="Image of lower bound of constrained problem" />
<em>Image taken from <a href="https://jonathan-hui.medium.com/rl-trust-region-policy-optimization-trpo-part-2-f51e3b2e373a">here</a></em></p>

<p>However, in practice, the constrained formulation wins by a landslide. Here’s why: the penalty coefficient C becomes a nightmare to tune when the discount factor γ gets close to 1. As γ approaches 1, the coefficient explodes, making the algorithm incredibly sensitive to small changes in γ. Imagine trying to tune a parameter that changes by orders of magnitude when you adjust γ from 0.99 to 0.995 - it’s practically impossible.</p>

\[C \propto 1/(1-\gamma)^2\]

<p>The constrained version, on the other hand, gives you direct, interpretable control. The parameter δ simply says “don’t let the policy change too much,” which is much easier to understand and tune across different environments. It’s the difference between having a thermostat that directly controls temperature versus one that requires you to calculate complex equations involving heat transfer coefficients.</p>

<table>
  <tbody>
    <tr>
      <td>This practical insight would later inspire PPO’s breakthrough innovation. PPO took the unconstrained formulation and made it work brilliantly by replacing the complex second-order penalty with a simple first-order clipping mechanism. Instead of computing expensive Fisher Information Matrices, PPO just clips the importance sampling ratios directly - achieving similar performance with a fraction of the computational cost.</td>
    </tr>
  </tbody>
</table>

<p>The beauty of TRPO lies in its theoretical guarantee. Since we have the fundamental bound:</p>

\[J(\pi') - J(\pi) \geq \mathcal{L}_\pi(\pi') - C\sqrt{\mathbb{E}_{s\sim d^\pi}[D_{KL}(\pi'(·|s) \| \pi(·|s))]}\]

<p>TRPO’s algorithm ensures three key things happen:</p>

<ol>
  <li><strong>Optimize</strong> $\mathcal{L}_\pi(\pi’)$ using importance sampling</li>
  <li><strong>Constrain</strong> the KL divergence to stay small</li>
  <li><strong>Rely</strong> on the fact that $\mathcal{L}_\pi(\pi) = 0$ when $\pi’ = \pi$</li>
</ol>

<p>This last point is crucial and deserves explanation.</p>

<p>Why is ℒ_π(π) = 0? At the current policy, the importance sampling ratio becomes $\frac{\pi(a|s)}{\pi(a|s)} = 1$ for all actions. So we get:</p>

\[\mathcal{L}_\pi(\pi) = \mathbb{E}_{s\sim d^\pi} \left[ \mathbb{E}_{a \sim \pi} \left[ 1 \cdot A^\pi(s,a) \right] \right] = \mathbb{E}_{s\sim d^\pi} \left[ \mathbb{E}_{a \sim \pi} \left[ A^\pi(s,a) \right] \right]\]

<p>But by definition, the advantage function has zero expectation under the policy - $\mathbb{E}_{a \sim \pi}[A^\pi(s,a)] = 0$ because it measures how much better each action is compared to the average. This means if we can make ℒ_π(π’) &gt; 0 while keeping KL divergence small, we’re guaranteed that J(π’) &gt; J(π). <strong>TRPO never moves backwards.</strong></p>

<table>
  <tbody>
    <tr>
      <td>You can read more about the proof <a href="https://jonathan-hui.medium.com/rl-proof-for-trpo-ppo-f18056fd6594">here</a></td>
    </tr>
  </tbody>
</table>

<p>$\mathcal{L}_\pi(\pi’) \geq 0$ implies $J(\pi’) \geq J(\pi)$ (Our new policy will always be better or equal to our current policy)</p>

<blockquote>
  <p><strong>TRPO’s guarantee: Every policy update improves performance or leaves it unchanged. We never move backwards.</strong></p>
</blockquote>

<p>Think of TRPO this way:</p>

<ol>
  <li><strong>Sample trajectories</strong> with current policy $\pi$</li>
  <li><strong>Estimate</strong> how well any nearby policy $\pi’$ would do on these same trajectories (importance sampling)</li>
  <li><strong>Find the best</strong> nearby policy within our trust region (constrained optimization)</li>
  <li><strong>Verify</strong> the policy is actually better before committing (safety check)</li>
</ol>

<p>The trust region ensures our importance sampling estimates remain accurate, while the MM algorithm structure guarantees we always improve.
The constrained optimization problem:
\(\max_{\pi'} \mathcal{L}_\pi(\pi')\)
\(\text{subject to } \mathbb{E}_{s\sim d^\pi}[D_{KL}(\pi'||\pi)[s]] \leq \delta\)</p>

<p>looks intimidating, but we can solve it elegantly using a <a href="https://en.wikipedia.org/wiki/Taylor_series"><strong>Taylor expansion</strong></a> around our current policy parameters θ_k. This is where the mathematical beauty of TRPO really shines through.</p>

<p><img src="/assets/blog_assets/evolution_of_llms/12.webp" alt="Image of Taylor Series expansion" />
<em>Definition from Wikipedia</em>
Let’s expand both the objective function and the constraint to second order around $\theta_k$. For the objective function $\mathcal{L}$:</p>

\[\mathcal{L}_{\theta_k}(\theta) \approx \mathcal{L}_{\theta_k}(\theta_k) + g^T (\theta - \theta_k) + \frac{1}{2}(\theta - \theta_k)^T H_{\mathcal{L}} (\theta - \theta_k)\]

<p>Where:</p>

<ul>
  <li>g = ∇<em>θ L</em>θₖ(θ) |_θₖ (the gradient of the objective at θₖ)</li>
  <li>H<em>L = ∇²</em>θ L<em>θₖ(θ) |</em>θₖ (the Hessian of the objective at θₖ)</li>
</ul>

<table>
  <tbody>
    <tr>
      <td>We can skip the terms beyond second order because they become negligibly small when $\theta$ is close to $\theta_k$. This is the fundamental assumption of trust region methods - we’re making small enough steps that higher-order terms don’t significantly affect our approximation quality.</td>
    </tr>
  </tbody>
</table>

<p>For the KL constraint:
\(\overline{D}_{KL}(\theta|\theta_k) \approx \overline{D}_{KL}(\theta_k|\theta_k) + \nabla_\theta \overline{D}_{KL}(\theta|\theta_k)|_{\theta_k}^T (\theta - \theta_k) + \frac{1}{2}(\theta - \theta_k)^T H_{KL} (\theta - \theta_k)\)</p>

<p>Now comes the key insight that simplifies everything. At the current policy θ_k, several terms vanish:</p>

<ul>
  <li>$\mathcal{L}_{\theta_k}(\theta_k) = 0$ (we showed this earlier - the advantage has zero expectation)</li>
  <li>$\overline{D}_{KL}(\theta_k|\theta_k) = 0$ (KL divergence of a distribution with itself is always zero)</li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>∇*θ D̄_KL(θ</td>
          <td> </td>
          <td>θₖ)</td>
          <td>*θₖ = 0 (the gradient of KL divergence at the reference point is zero)</td>
        </tr>
      </tbody>
    </table>
  </li>
</ul>

<p>This leaves us with a beautifully clean quadratic optimization problem:</p>

<p>\(\max_\theta g^T (\theta - \theta_k)\)
\(\text{subject to } \frac{1}{2}(\theta - \theta_k)^T H_{KL} (\theta - \theta_k) \leq \delta\)</p>

<p>where g is the <strong>policy gradient</strong>:
\(g = \nabla_\theta \mathcal{L}_{\theta_k}(\theta) |_{\theta_k}\)</p>

<p>and $H_{KL}$ is the <strong>Hessian of the KL divergence</strong>, which has a special name: the <strong>Fisher Information Matrix (FIM)</strong>:</p>

\[H_{KL} = \nabla^2_\theta \overline{D}_{KL}(\theta|\theta_k) |_{\theta_k} = F = \mathbb{E}_{s,a \sim \pi_k} \left[ \nabla_\theta \log \pi_\theta(a|s) \nabla_\theta \log \pi_\theta(a|s)^T \right]\]

<p>This constrained quadratic optimization problem has a closed-form solution that can be derived using <strong>Lagrange multipliers</strong>. Setting up the Lagrangian:</p>

\[\mathcal{L}(\theta, \lambda) = g^T (\theta - \theta_k) - \lambda \left( \frac{1}{2}(\theta - \theta_k)^T F (\theta - \theta_k) - \delta \right)\]

<p>Taking the gradient with respect to θ and setting it to zero:
\(\nabla_\theta \mathcal{L} = g - \lambda F (\theta - \theta_k) = 0\)</p>

<p>Solving for the optimal step:
\(\theta - \theta_k = \frac{1}{\lambda} F^{-1} g\)</p>

<p>To find λ, we substitute back into the constraint:
\(\frac{1}{2} \left( \frac{1}{\lambda} F^{-1} g \right)^T F \left( \frac{1}{\lambda} F^{-1} g \right) = \delta\)</p>

\[\frac{1}{2\lambda^2} g^T F^{-1} g = \delta\]

\[\lambda = \sqrt{\frac{g^T F^{-1} g}{2\delta}}\]

<p>Putting it all together, we get the <strong>Natural Policy Gradient</strong> update:</p>

\[\theta_{k+1} = \theta_k + \sqrt{\frac{2\delta}{g^T F^{-1} g}} F^{-1} g\]

<p><strong>Why “Natural”?</strong> This is where things get philosophically beautiful. Regular gradient descent uses the Euclidean distance in parameter space - it treats all parameter changes as equal. But this is fundamentally wrong for probability distributions!</p>

<p>Consider two neural networks that represent the same policy but with different parameterizations. Vanilla gradient descent would give them different updates, even though they’re the same policy. The Natural Policy Gradient fixes this by using the Fisher Information Matrix to measure distance in the space of probability distributions rather than parameter space.</p>

<p>The Fisher Information Matrix captures the “curvature” of the log-likelihood surface. Areas where small parameter changes cause big changes in the policy get more weight in the distance metric. This makes the algorithm <strong>reparameterization invariant</strong> - the actual policy updates remain the same regardless of how you parameterize your neural network.</p>

<p>Think of it like this: if you’re navigating on a curved surface, you shouldn’t use straight-line distances to plan your path. The Fisher Information Matrix gives you the “natural” metric for that curved space, ensuring you take the most efficient steps toward better policies.</p>

<p><img src="/assets/blog_assets/evolution_of_llms/13.webp" alt="Image of Euclidean gradient descent vs natural gradient descent" /></p>

<p><strong>The Backtracking Line Search</strong></p>

<p>Our elegant mathematical derivation gives us the Natural Policy Gradient update:</p>

\[\theta_{k+1} = \theta_k + \sqrt{\frac{2\delta}{g^T F^{-1} g}} F^{-1} g\]

<p>But here’s the rub: this assumes our quadratic approximation is perfect. In reality, neural networks are highly nonlinear, and our Taylor expansion is only valid in a small neighborhood around $\theta_k$. The computed step might violate our KL constraint or even decrease performance when the approximation breaks down.</p>

<p>TRPO’s solution is beautifully practical: <strong>backtracking line search</strong>. Instead of blindly taking the full computed step, TRPO modifies the update to:</p>

\[\theta_{k+1} = \theta_k + \alpha^j \sqrt{\frac{2\delta}{g^T F^{-1} g}} F^{-1} g\]

<p>where $\alpha \in (0, 1)$ is the backtracking coefficient (typically 0.5), and $j$ is the smallest nonnegative integer such that the new policy satisfies both:</p>

<ol>
  <li>The KL constraint: 𝔼<em>{s∼d^π}[D_KL(π</em>{θ<em>{k+1}}(·|s) ‖ π</em>{θₖ}(·|s))] ≤ δ</li>
  <li>Positive improvement: ℒ<em>{θₖ}(θ</em>{k+1}) ≥ 0</li>
</ol>

<p>This conservative verification ensures TRPO never violates its theoretical guarantees, even when the quadratic approximation becomes inaccurate. It’s the algorithm’s safety net - systematically reducing the step size until both conditions are met.</p>

<p>However, there’s a computational nightmare lurking here. The Natural Policy Gradient requires computing $F^{-1}g$, which means inverting the Fisher Information Matrix:</p>

\[F = \mathbb{E}_{s,a \sim \pi_k} \left[ \nabla_\theta \log \pi_\theta(a|s) \nabla_\theta \log \pi_\theta(a|s)^T \right]\]

<p>For modern deep networks with millions of parameters, F is a massive n×n matrix. Computing its inverse is O(n³) - completely impractical. Even storing the full matrix requires O(n²) memory, which quickly becomes impossible for large networks.</p>

<p>This computational bottleneck is what led to the development of <strong>Truncated Natural Policy Gradient</strong>, and ultimately to TRPO’s clever use of conjugate gradient methods to approximate the matrix inversion without ever computing F⁻¹ explicitly.</p>

<p><strong>Truncated Natural Policy Gradient</strong></p>

<p>The solution to our computational nightmare is elegantly simple: instead of computing F⁻¹g directly, we use the <a href="https://en.wikipedia.org/wiki/Conjugate_gradient_method"><strong>Conjugate Gradient</strong></a> method to solve the linear system:</p>

\[F x = g\]

<p>This iterative approach finds x ≈ F⁻¹g without ever computing the matrix inverse, requiring only matrix-vector products Fv. It’s like finding the solution to a puzzle without having to understand every piece - we just need to know how the pieces interact.</p>

<p>Conjugate Gradient works by generating search directions that are “conjugate” - meaning they’re orthogonal after being transformed by matrix F. This ensures that each new search direction doesn’t undo progress from previous iterations. For quadratic problems like ours, CG guarantees finding the exact solution in at most n steps (where n is the number of parameters), but typically converges much faster in practice.</p>

<p><img src="/assets/blog_assets/evolution_of_llms/14.webp" alt="Image of Taylor Series expansion" /></p>

<p>The key insight is that we never need to compute or store the full Fisher Information Matrix. Instead, we only need the matrix-vector product $Fx$ for any vector $x$. This can be computed efficiently using automatic differentiation:</p>

\[Fx = \nabla_\theta \left( \left( \nabla_\theta \overline{D}_{KL}(\theta||\theta_k) \right)^T x \right)\]

<p>This <strong>Hessian-vector product</strong> gives us exactly what conjugate gradient needs without ever materializing the massive $F$ matrix.</p>

<p><strong>The Complete TRPO Algorithm</strong></p>

<p>TRPO weaves together all these concepts into a surprisingly elegant algorithm that carefully balances theoretical guarantees with practical implementation:</p>

<p><strong>Step 1: Data Collection</strong></p>

<ul>
  <li>Collect trajectories using the current policy $\pi_k$</li>
  <li>Estimate the advantage function $A^{\pi_k}$ using any method (GAE, Monte Carlo returns, or temporal difference learning)</li>
</ul>

<p><strong>Step 2: Gradient Computation</strong></p>

<ul>
  <li>Compute the policy gradient \(g = \nabla_\theta \mathcal{L}_{\theta_k}(\theta) \|_{\theta_k}\)</li>
  <li>Set up the Fisher Information Matrix function for conjugate gradient operations</li>
</ul>

<p><strong>Step 3: Search Direction</strong></p>

<ul>
  <li>Solve $Fx = g$ using Conjugate Gradient to get the search direction $x$</li>
  <li>This gives us the natural gradient direction without explicitly inverting $F$</li>
</ul>

<p><strong>Step 4: Step Size Calculation</strong></p>

<ul>
  <li>Compute the initial step size to satisfy the trust region constraint</li>
  <li>Calculate $\alpha = \sqrt{\frac{2\delta}{g^T F^{-1} g}}$</li>
</ul>

<p><strong>Step 5: Conservative Verification (Line Search with Exponential Backoff)</strong></p>

<ul>
  <li>Propose an update: $\theta’ = \theta_k + \alpha \cdot x$</li>
  <li>Verify two critical conditions:</li>
  <li>KL divergence constraint: \(\mathbb{E}_{s\sim d^\pi}[D_{KL}(\pi'(·\|s) \| \pi(·\|s))] \leq \delta\)</li>
  <li>Surrogate improvement: $\mathcal{L}_{\theta_k}(\theta’) \geq 0$</li>
  <li>If either verification fails: reduce $\alpha$ (typically by half) and try again</li>
  <li>Only commit to the policy update after both conditions are satisfied</li>
</ul>

<p>This conservative approach guarantees the theoretical properties we derived, but it also reveals TRPO’s fundamental tension between theory and practice. The algorithm is theoretically beautiful but computationally demanding, requiring multiple verification steps and potential backtracking that can make each update quite expensive.</p>

<p><strong>TRPO’s Limitations</strong></p>

<p>Despite its theoretical elegance, TRPO faces several practical challenges that motivated simpler alternatives:</p>

<ul>
  <li><strong>Computational Overhead</strong>: Computing Fisher Information Matrices and running conjugate gradient makes each update significantly more expensive than first-order methods like Adam</li>
  <li><strong>Sample Inefficiency</strong>: Requires large batch sizes to accurately estimate the FIM - small batches lead to noisy estimates and unstable training</li>
  <li><strong>Scalability Issues</strong>: Second-order computations become impractical for very large neural networks where first-order methods excel</li>
</ul>

<p>TRPO’s story represents a classic tension in machine learning: the trade-off between theoretical rigor and practical utility. While TRPO provided crucial theoretical insights about policy optimization - principled policy updates, trust region concepts, and guaranteed improvement - its computational complexity limited its real-world impact.</p>

<p>This limitation sparked a natural question: could we achieve similar performance guarantees with a much simpler algorithm? The answer would come in the form of Proximal Policy Optimization (PPO), which took TRPO’s core insights and packaged them into an algorithm so simple and effective that it would become the workhorse of modern policy optimization.</p>

<p>As we noted earlier from the PPO paper: <em>“Q-learning (with function approximation) fails on many simple problems and is poorly understood, vanilla policy gradient methods have poor data efficiency and robustness; and trust region policy optimization (TRPO) is relatively complicated, and is not compatible with architectures that include noise (such as dropout) or parameter sharing.”</em></p>

<p>PPO’s breakthrough was recognizing that you don’t need complex second-order methods to implement trust regions effectively. Instead of computing Fisher Information Matrices and running conjugate gradient, PPO simply clips the importance sampling ratios directly. This first-order approach achieves similar practical performance while being orders of magnitude simpler to implement and debug.</p>

<blockquote>
  <p>Note: TRPO in a crux is simple, it is a constrained optimization problem. To solve which we need second order derivatives. Which is computationally expensive and no current ML framework solves it without significant overhead. But do know, it is a tough topic to truly understand. One needs to be well-versed with many prerequisite mathematical knowledge. Do not be dishearted if it takes you time to understand it thorougly. Read slowly, daily, iteratively.</p>
</blockquote>

<p><strong>Proximal Policy Optimization (PPO)</strong></p>

<p>PPO emerged from a simple observation: what if we could achieve TRPO’s stability guarantees without all the computational complexity? The genius of PPO lies in replacing TRPO’s hard KL constraint with a clever objective function that naturally prevents large policy updates.</p>

<p>Let’s first understand the core innovation. Remember TRPO’s constrained optimization problem:</p>

\[\max_{\theta} \mathcal{L}_{\theta_{old}}(\theta) \quad \text{subject to } \mathbb{E}_{s \sim d^{\pi_{old}}}[D_{KL}(\pi_{old}(\cdot|s) || \pi_\theta(\cdot|s))] \leq \delta\]

<p>PPO asks: instead of explicitly constraining the KL divergence, what if we modify the objective function itself to penalize large policy changes? This transforms a constrained optimization problem into an unconstrained one that standard optimizers like Adam can handle.</p>

<p><strong>The Clipped Surrogate Objective</strong></p>

<p>PPO introduces a brilliantly simple idea. Define the probability ratio:</p>

\[r_t(\theta) = \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}\]

<p>This ratio tells us how much more (or less) likely the new policy is to take the same action compared to the old policy. When $r_t(\theta) = 1$, the policies agree perfectly. When $r_t(\theta) = 2$, the new policy is twice as likely to take that action.</p>

<p>The vanilla policy gradient objective using importance sampling would be:</p>

\[L^{IS}(\theta) = \mathbb{E}_t[r_t(\theta) \cdot A_t]\]

<p>But this can lead to destructively large policy updates when $r_t(\theta)$ becomes very large or very small. PPO’s innovation is to clip this ratio:</p>

\[L^{CLIP}(\theta) = \mathbb{E}_t[\min(r_t(\theta) \cdot A_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon) \cdot A_t)]\]

<p><img src="/assets/blog_assets/evolution_of_llms/15.webp" alt="Image of Taylor Series expansion" />
<em>Image taken from paper</em></p>

<p>Let’s unpack this equation with concrete examples to build intuition.</p>

<p><strong>Case 1: Positive Advantage (A_t &gt; 0)</strong></p>

<p>When an action led to better-than-expected rewards, we want to increase its probability. Let’s say $A_t = 2$ and $\epsilon = 0.2$.</p>

<ul>
  <li>
    <p>If $r_t(\theta) = 0.5$ (new policy half as likely):</p>

    <ul>
      <li>Unclipped objective: $0.5 \times 2 = 1$</li>
      <li>Clipped objective: $\min(1, 0.8 \times 2) = 1$</li>
      <li>No clipping occurs since we’re making the policy worse for a good action</li>
    </ul>
  </li>
  <li>
    <p>If $r_t(\theta) = 1.5$ (new policy 50% more likely):</p>
    <ul>
      <li>Unclipped objective: $1.5 \times 2 = 3$</li>
      <li>Clipped objective: $\min(3, 1.2 \times 2) = 2.4$</li>
      <li>Clipping kicks in! We cap the improvement to prevent overconfidence</li>
    </ul>
  </li>
</ul>

<p>The key insight: for positive advantages, clipping prevents us from changing the policy too aggressively in the “good” direction. Once $r_t(\theta) &gt; 1 + \epsilon$, there’s no benefit to increasing it further.</p>

<p><strong>Case 2: Negative Advantage (A_t &lt; 0)</strong></p>

<p>When an action led to worse-than-expected rewards, we want to decrease its probability. Let’s say $A_t = -2$ and $\epsilon = 0.2$.</p>

<ul>
  <li>
    <p>If $r_t(\theta) = 0.5$ (new policy half as likely):</p>

    <ul>
      <li>Unclipped objective: $0.5 \times (-2) = -1$</li>
      <li>Clipped objective: $\min(-1, 0.8 \times (-2)) = -1.6$</li>
      <li>Clipping makes the objective more negative, encouraging further reduction</li>
    </ul>
  </li>
  <li>
    <p>If $r_t(\theta) = 1.5$ (new policy 50% more likely):</p>
    <ul>
      <li>Unclipped objective: $1.5 \times (-2) = -3$</li>
      <li>Clipped objective: $\min(-3, 1.2 \times (-2)) = -3$</li>
      <li>No clipping since we’re increasing probability of a bad action</li>
    </ul>
  </li>
</ul>

<p>For negative advantages, clipping prevents us from reducing the probability too aggressively. Once $r_t(\theta) &lt; 1 - \epsilon$, there’s no benefit to decreasing it further.</p>

<p><strong>The Mathematical Beauty of PPO’s Objective</strong></p>

<p>The clipped objective creates a “trust region” implicitly. When the policy changes too much (beyond $1 \pm \epsilon$), the gradient of the clipped objective becomes zero with respect to $\theta$. This elegant mechanism automatically prevents destructive updates without requiring second-order optimization.</p>

<p>To see this mathematically, consider the gradient when $A_t &gt; 0$ and $r_t(\theta) &gt; 1 + \epsilon$:</p>

\[\frac{\partial L^{CLIP}}{\partial \theta} = \frac{\partial}{\partial \theta}[\text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon) \cdot A_t] = 0\]

<p>The gradient vanishes because the clipped value $(1+\epsilon)$ doesn’t depend on $\theta$. This creates a “flat” region in the loss landscape that prevents further movement in that direction.</p>

<p><strong>PPO with Adaptive KL Penalty</strong></p>

<p>Before arriving at the clipped objective, the PPO paper explored a KL penalty approach that directly connects to TRPO:</p>

\[L^{KLPEN}(\theta) = \mathbb{E}_t[r_t(\theta) \cdot A_t - \beta \cdot D_{KL}(\pi_{\theta_{old}}(\cdot|s_t) || \pi_\theta(\cdot|s_t))]\]

<p>This is exactly the unconstrained version of TRPO’s problem! The Lagrangian of TRPO’s constrained optimization:</p>

\[\max_{\theta} \mathcal{L}_{\theta_{old}}(\theta) - \lambda(\mathbb{E}[D_{KL}] - \delta)\]

<p>becomes PPO’s KL penalty objective when we fix $\beta = \lambda$. The key difference: PPO adapts $\beta$ dynamically during training:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">if</span> <span class="n">kl_divergence</span> <span class="o">&gt;</span> <span class="mf">1.5</span> <span class="o">*</span> <span class="n">target_kl</span><span class="p">:</span>
    <span class="n">beta</span> <span class="o">*=</span> <span class="mi">2</span>  <span class="c1"># Increase penalty
</span><span class="k">elif</span> <span class="n">kl_divergence</span> <span class="o">&lt;</span> <span class="n">target_kl</span> <span class="o">/</span> <span class="mf">1.5</span><span class="p">:</span>
    <span class="n">beta</span> <span class="o">/=</span> <span class="mi">2</span>  <span class="c1"># Decrease penalty
</span></code></pre></div></div>

<p>However, this adaptive mechanism proved finicky in practice. The clipped objective achieved similar goals with fixed hyperparameters, making it the preferred approach.</p>

<p><strong>Why Multiple Epochs Work: The Importance Sampling Perspective</strong></p>

<p>A subtle but crucial aspect of PPO is performing multiple epochs of updates on the same data. This seems to violate our earlier concern about importance sampling breaking down when policies diverge. The clipping mechanism is precisely what makes this safe.</p>

<p>Consider what happens over multiple epochs:</p>

<ol>
  <li><strong>Epoch 1</strong>: Policy changes slightly, ratios stay near 1</li>
  <li><strong>Epoch 2</strong>: For trajectories where policy already changed, clipping prevents further movement</li>
  <li><strong>Epochs 3-10</strong>: Most gradients are zero due to clipping, only “unexploited” trajectories contribute</li>
</ol>

<p>The clipping essentially creates a curriculum where different parts of the data become “active” in different epochs, naturally preventing overfitting to any particular trajectory.</p>

<p>PPO’s clipping prevents the fine-tuned model from diverging too far from the base model’s distribution, maintaining fluency while optimizing for human preferences. This is why responses from RLHF models feel coherent - they’re constrained to stay within a trust region of the original model’s behavior.</p>

<p>The journey from policy gradients through TRPO to PPO represents a beautiful example of how complex theoretical insights can be distilled into simple, practical algorithms. PPO takes TRPO’s guarantee of monotonic improvement and approximates it with a first-order method that captures the essential insights: prevent destructive updates, enable data reuse, and maintain computational simplicity.</p>

<h3 id="moe">MoE</h3>

<p><img src="/assets/blog_assets/evolution_of_llms/moe_abstract.webp" alt="Image of MoE paper abstract" /></p>

<blockquote>
  <p>Link to paper: <a href="https://arxiv.org/abs/1701.06538">Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer</a> <br />
Link to implementation: [WORK IN PROGRESS]</p>
</blockquote>

<details>
<summary>Quick Summary</summary>
<div>

    <p>This 2017 paper by Shazeer et al. introduces a novel approach to dramatically increase neural network capacity without proportionally increasing computational costs. The core innovation is the Sparsely-Gated Mixture-of-Experts (MoE) layer, which contains thousands of feed-forward neural networks (experts), with a trainable gating network that selectively activates only a small subset of experts for each input example.</p>

    <p>Key highlights:</p>

    <ul>
      <li>The authors achieve over 1000x improvements in model capacity while maintaining computational efficiency</li>
      <li>Their approach addresses several challenges of conditional computation, including GPU utilization and load balancing</li>
      <li>When applied to Language Modeling and machine translation tasks, their MoE models significantly outperform state-of-the-art models with lower computational cost</li>
      <li>Their largest model contains up to 137 billion parameters and demonstrates continued performance improvements with increased capacity</li>
    </ul>

    <p>This paper represents a significant advancement in scaling neural networks efficiently, presaging some of the techniques that would later become important in very large language models.</p>

  </div>
</details>
<p><br /></p>

<p>Another explosive paper, in 2017. Talk about being a crazy year right. Well to be perfectly honest MOE was actually introduced in 1991 in the paper <a href="https://www.cs.toronto.edu/~fritz/absps/jjnh91.pdf">Adaptive Mixture of Local Experts</a>. But Noam et al introduced the idea to LSTMs, which really blew up.</p>

<p><strong>Problem</strong></p>

<blockquote>
  <p>The capacity of a neural network to absorb information is limited by its number of
parameters.</p>
</blockquote>

<p><strong>Solution</strong></p>

<blockquote>
  <p>Conditional computation, where parts of the network are active on a
per-example basis, has been proposed in theory as a way of dramatically increasing model capacity without a proportional increase in computation.</p>
</blockquote>

<p>The following blogs helped me immensely while writing this section</p>

<ul>
  <li><a href="https://huggingface.co/blog/moe">Mixture of Experts Explained</a></li>
  <li><a href="https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-mixture-of-experts">A Visual Guide to Mixture of Experts (MoE)</a></li>
</ul>

<p><img src="/assets/blog_assets/evolution_of_llms/2.webp" alt="Image of MoE intuition" /></p>

<p>A simple intuition behind MoE can be seen as above, A single dense neural network is like a big student. Who has general knowledge about a lot of things without being particularly great at any one topic. When you ask him a question he takes his time to think and answers you, He also eats a lot because he is big.</p>

<p>But with a MoE layer, a smart router reads the question and directs it to the right expert. That expert gives a focused answer since they only need to worry about their specialty. As we’re only activating one small expert instead of the entire large model, we use much less computation while still having access to lots of specialized knowledge.</p>

<p>The above visualization is good for intuition point of view, but that is not how MoEs actually work in practice. For starter each expert is not an expert in a topic but expert in tokens, some can be punctuation experts, some can be noun experts etc.(More on this later)</p>

<p>This work introduced MoEs to LSTMs, so let us proceed forward in understanding that.
Consider reading the following blog <a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/">Understanding LSTM Networks</a> by <a href="https://x.com/ch402?lang=en">Christopher Olah</a> &amp; <a href="https://www.youtube.com/watch?v=AsNTP8Kwu80">Recurrent Neural Networks (RNNs), Clearly Explained!!!</a> by <a href="https://x.com/joshuastarmer?lang=en">Josh Starmer</a> if you need a refresher on the topic.</p>

<p><img src="/assets/blog_assets/evolution_of_llms/3.webp" alt="Image of MoE for RNNs" /></p>

<p>In an MoE model, the Fully connected neural network(FCNN) (Or the hiddens state in case of RNNs &amp; LSTMs) is replaces with an MoE layer. The MoE layer consists of a gating function which outputs a probability distribution of likely experts. The experts themselves are smaller FCNN. The output of the experts is multiplied with their probability after which it is finally summed over.</p>

<p><img src="/assets/blog_assets/evolution_of_llms/moe_layer.webp" alt="Image of MoE layer" /></p>

<p>The idea seems simple enough, but there are multiple complexities like:</p>

<ul>
  <li>How do you create a fair gating function?</li>
  <li>How many experts do you choose?</li>
  <li>How many tokens do you send to each expert?</li>
</ul>

<p>Let’s us go through each question one by one.</p>

<blockquote>
  <p>Note: We will see many changes that were made on this idea as we progress, but this was the foundational paper on MoEs for large models. So it is crucial that you understand it well.</p>
</blockquote>

<h5 id="sparse-vs-dense-networks">Sparse vs Dense Networks</h5>

<p><img src="/assets/blog_assets/evolution_of_llms/sparse_vs_dense_moe.webp" alt="Image of sparse and dense MoE" /></p>

<p><strong>Dense Networks</strong>: Every parameter is used for every input</p>

<ul>
  <li>High computational cost that scales linearly with parameters</li>
  <li>Limited by memory and compute constraints</li>
  <li>Parameters must be “generalists” handling all types of inputs</li>
</ul>

<p><strong>Sparse Networks (MoE)</strong>: Only a subset of parameters are used per input</p>

<ul>
  <li>Computational cost scales with active experts, not total experts</li>
  <li>Can have 1000x more parameters with similar compute budget</li>
  <li>Parameters can be highly specialized for specific patterns</li>
</ul>

<p><strong>conditional computation</strong> allows us to scale model capacity without proportional compute scaling. It’s like having a library with thousands of specialized books, but only reading the few relevant ones for each question.</p>

<h5 id="the-gating-network">The Gating Network</h5>

<p>First let us understand how the output is calculated in a sparse MoE.</p>

<p><img src="/assets/blog_assets/evolution_of_llms/6.webp" alt="Image of MoE paper abstract" /></p>

<p>We begin with an input matrix X, multiple that by the router weights W. We take the softmax of this output to get the probability distribution $G(x)$. This is the likelihood of which experts are best for the given input.</p>

<p>Depending on how many experts we choose, we take the output of those experts and multiply that with the probability of that output begin chosen (This is done distriute the importance of the output based on which expert is most likely to be chosen). That gives us the output.</p>

<p>When we put it all together, this is how it looks.</p>

<p><img src="/assets/blog_assets/evolution_of_llms/7.webp" alt="Image of MoE paper abstract" /></p>

<p>The original paper introduced two key innovations:</p>

<p><strong>Softmax Gating (Dense Baseline)</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Simple dense gating - activates ALL experts with different weights
</span><span class="n">G</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">=</span> <span class="n">Softmax</span><span class="p">(</span><span class="n">x</span> <span class="err">·</span> <span class="n">W_g</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">Σ</span> <span class="n">G</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="n">_i</span> <span class="o">*</span> <span class="n">Expert_i</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># All experts contribute
</span></code></pre></div></div>

<p><strong>Noisy Top-K Gating (The Sparse Innovation)</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Step 1: Add trainable noise for load balancing
</span><span class="n">H</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="n">_i</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span> <span class="err">·</span> <span class="n">W_g</span><span class="p">)</span><span class="n">_i</span> <span class="o">+</span> <span class="n">StandardNormal</span><span class="p">()</span> <span class="o">*</span> <span class="n">Softplus</span><span class="p">((</span><span class="n">x</span> <span class="err">·</span> <span class="n">W_noise</span><span class="p">)</span><span class="n">_i</span><span class="p">)</span>

<span class="c1"># Step 2: Keep only top K experts, set others to -∞
</span><span class="n">KeepTopK</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span><span class="n">_i</span> <span class="o">=</span> <span class="p">{</span>
    <span class="n">v_i</span>    <span class="k">if</span> <span class="n">v_i</span> <span class="ow">is</span> <span class="ow">in</span> <span class="n">top</span> <span class="n">k</span> <span class="n">elements</span>
    <span class="o">-</span><span class="err">∞</span>     <span class="n">otherwise</span>
<span class="p">}</span>

<span class="c1"># Step 3: Apply softmax (experts with -∞ get probability 0)
</span><span class="n">G</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">=</span> <span class="n">Softmax</span><span class="p">(</span><span class="n">KeepTopK</span><span class="p">(</span><span class="n">H</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">k</span><span class="p">))</span>
</code></pre></div></div>

<p><strong>Why the noise?</strong> The Gaussian noise helps with load balancing. Without it, the same few experts would always dominate, creating a “rich get richer” problem where popular experts get more training and become even more popular.</p>

<p><strong>Why Top-K?</strong> By keeping only the top K experts (typically K=2 or K=4), we achieve:</p>

<ul>
  <li><strong>Sparsity</strong>: Most experts are inactive, saving computation</li>
  <li><strong>Specialization</strong>: Each expert focuses on specific patterns</li>
  <li><strong>Scalability</strong>: We can add thousands of experts without proportional compute increase</li>
</ul>

<h5 id="addressing-performance-challenges">Addressing Performance Challenges</h5>

<p>The paper identified several critical challenges that needed solving for MoE to work in practice:</p>

<p><strong>The Shrinking Batch Problem</strong></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Original batch: 1024 examples
With 256 experts, k=4: Each expert sees only ~16 examples
Small batches = inefficient GPU utilization
</code></pre></div></div>

<p><strong>Solution:</strong></p>

<p>Mix Data and Model Parallelism</p>

<ul>
  <li>Combine batches from multiple GPUs before sending to experts</li>
  <li>Each expert gets larger effective batch size: <code class="language-plaintext highlighter-rouge">(batch_size * num_devices * k) / num_experts</code></li>
  <li>Achieves factor of <code class="language-plaintext highlighter-rouge">d</code> improvement in expert batch size with <code class="language-plaintext highlighter-rouge">d</code> devices</li>
</ul>

<p><strong>Network Bandwidth Bottleneck</strong></p>

<p>Modern GPUs have computational power thousands of times greater than network bandwidth. Meaning most time is spent between I/O operations.</p>

<p><strong>Solution:</strong></p>

<ul>
  <li>Keep experts stationary on devices (don’t move the experts)</li>
  <li>Only send inputs/outputs across network (much smaller)</li>
  <li>Use larger hidden layers to improve computation-to-communication ratio</li>
</ul>

<table>
  <tbody>
    <tr>
      <td>To understand this better, consider reading <a href="https://horace.io/brrr_intro.html">Making Deep Learning Go Brrrr From First Principles</a></td>
    </tr>
  </tbody>
</table>

<p><strong>Load Balancing Problem</strong></p>

<p>Without intervention, a few experts dominate while others are rarely used. This creates a vicious cycle: popular experts get more training data, become better, and thus get selected even more often. Meanwhile, neglected experts remain undertrained and essentially become dead weight.</p>

<p>Think of it like a classroom where only the brightest students get called on - they get more practice and become even brighter, while others stagnate.</p>

<p><strong>The Dual Challenge</strong></p>

<p>The paper identifies that we need to balance two distinct but related problems:</p>

<ol>
  <li><strong>Importance Imbalance</strong>: Some experts get high gating weights but few examples</li>
  <li><strong>Load Imbalance</strong>: Some experts get many examples but low individual weights</li>
</ol>

<p>Both scenarios are problematic. An expert with few high-weight examples overfits to specific patterns, while an expert with many low-weight examples receives weak learning signals.</p>

<p><strong>Mathematical Solution: Auxiliary Loss</strong></p>

<p>The authors introduce a load balancing loss that uses the <strong>coefficient of variation (CV)</strong> to measure and penalize imbalance:</p>

\[CV = \frac{\sigma}{\mu} = \frac{\text{standard deviation}}{\text{mean}}\]

<p>The CV is beautiful because it’s scale-invariant - it measures relative variability regardless of the absolute magnitudes. A CV of 0 means perfect balance, while higher values indicate increasing imbalance.</p>

<p><strong>Step 1: Measuring Importance</strong></p>

<p><img src="/assets/blog_assets/evolution_of_llms/17.webp" alt="Image of MoE paper abstract" /></p>

<p>For each expert $i$, we sum its gating probabilities across the entire batch:</p>

\[\text{Importance}(X)_i = \sum_{x \in X} G(x)_i\]

<p>This gives us the “importance scores” - how much each expert contributes regardless of which specific examples it processes.</p>

<p><strong>Step 2: Computing the Importance Loss</strong></p>

\[\mathcal{L}_{\text{importance}} = w_{\text{importance}} \cdot CV(\text{Importance}(X))^2\]

<p>Where:
\(CV(\text{Importance}(X)) = \frac{\sigma(\text{Importance}(X))}{\mu(\text{Importance}(X))}\)</p>

<p><strong>Why square the CV?</strong> This creates a stronger penalty for large imbalances and makes the gradient more well-behaved during optimization.</p>

<p><img src="/assets/blog_assets/evolution_of_llms/18.webp" alt="Image of MoE paper abstract" /></p>

<p><strong>Step 3: Measuring Load</strong></p>

<p>Load measures how many examples each expert actually processes:</p>

\[\text{Load}(X)_i = \sum_{x \in X} \mathbf{1}[\text{expert } i \text{ is in top-k for } x]\]

<p>In practice, this uses a smooth differentiable approximation rather than the hard indicator function.</p>

<p><strong>Step 4: Computing the Load Loss</strong></p>

\[\mathcal{L}_{\text{load}} = w_{\text{load}} \cdot CV(\text{Load}(X))^2\]

<p><strong>The Complete Auxiliary Loss</strong></p>

\[\mathcal{L}_{\text{auxiliary}} = w_{\text{importance}} \cdot CV(\text{Importance}(X))^2 + w_{\text{load}} \cdot CV(\text{Load}(X))^2\]

<p><strong>Final Training Objective</strong></p>

\[\mathcal{L}_{\text{total}} = \mathcal{L}_{\text{main}} + \mathcal{L}_{\text{auxiliary}}\]

<p><strong>Why Both Losses Matter</strong></p>

<p>Consider these scenarios:</p>

<ul>
  <li><strong>Expert A</strong>: Gets selected for 100 examples with average weight 0.01 each</li>
  <li><strong>Expert B</strong>: Gets selected for 2 examples with average weight 0.5 each</li>
  <li><strong>Expert C</strong>: Gets selected for 50 examples with average weight 0.02 each</li>
</ul>

<p>All have similar total importance (≈ 1.0), but vastly different training dynamics:</p>

<ul>
  <li>Expert A gets many weak signals → slow learning</li>
  <li>Expert B gets few strong signals → overfitting risk</li>
  <li>Expert C gets balanced signal → healthy learning</li>
</ul>

<p>The dual loss ensures both the total contribution (importance) and the number of training examples (load) are balanced across experts.</p>

<p><strong>Practical Impact</strong></p>

<p>With proper load balancing:</p>

<ul>
  <li>All experts receive sufficient training signal</li>
  <li>No expert dominates the computation</li>
  <li>Model capacity is fully utilized</li>
  <li>Training stability improves dramatically</li>
</ul>

<p>This auxiliary loss was crucial for making MoE work at scale - without it, the models would collapse to using only a handful of experts, defeating the entire purpose of conditional computation.</p>

<p><strong>Expert Capacity</strong></p>

<p><img src="/assets/blog_assets/evolution_of_llms/19.webp" alt="Image of MoE paper abstract" /></p>

<p>This wasn’t introduced in this paper, but let’s talk about it too since it’s crucial for modern MoE implementations. Even with perfect load balancing, there’s another challenge: <strong>token overflow</strong>. In the example above, FFNN 1 receives the majority of tokens. To prevent any single expert from being overwhelmed, we set an <strong>Expert Capacity</strong> - a maximum number of tokens each expert can process per batch. When an expert reaches capacity, additional tokens that would have been routed to it are either sent to the next-best expert or bypass the MoE layer entirely (called <strong>token overflow</strong>). This capacity mechanism ensures balanced computational load across experts and prevents memory bottlenecks, though it can sometimes mean tokens don’t get processed by their optimal expert. The trade-off between perfect routing and practical constraints is a key engineering challenge in scaling MoE systems.</p>

<h5 id="training-the-moe-model">Training the MoE Model</h5>

<p><strong>Key Challenge</strong>: How do experts specialize without explicit supervision?</p>

<p>The specialization emerges through training dynamics:</p>

<ol>
  <li><strong>Initial randomness</strong>: All experts start random and perform similarly</li>
  <li><strong>Noise-induced preferences</strong>: The noise in gating creates slight preferences</li>
  <li><strong>Reinforcement loop</strong>: Experts that perform well for certain inputs get selected more</li>
  <li><strong>Emergent specialization</strong>: Through this process, experts develop distinct capabilities</li>
</ol>

<p><strong>What do experts actually learn?</strong> (From the paper’s analysis)</p>

<p><img src="/assets/blog_assets/evolution_of_llms/16.webp" alt="Image of tokens in MoE layre" />
<em>Image taken from <a href="https://arxiv.org/pdf/2401.04088">Mistral paper</a></em></p>

<p>Unlike the intuitive “biology expert” or “math expert”, real MoE experts learn much more fine-grained patterns:</p>

<ul>
  <li><strong>Syntactic specialization</strong>: Expert 381 specializes in contexts with “researchers”, “innovation”, and “technology”</li>
  <li><strong>Positional patterns</strong>: Expert 752 handles phrases where indefinite article “a” introduces important concepts</li>
  <li><strong>Semantic clustering</strong>: Expert 2004 focuses on contexts involving speed and rapid change</li>
</ul>

<p>This emergent specialization is what makes MoE powerful - experts automatically discover useful divisions of labor without being explicitly told what to specialize in.</p>

<h5 id="revolutionary-results">Revolutionary Results</h5>

<p><strong>Language Modeling (1B Word Benchmark)</strong>:</p>

<ul>
  <li>4096-expert MoE: 24% better perplexity than dense baseline</li>
  <li>Same computational cost as much smaller dense models</li>
  <li>Up to 137B parameters (1000x parameter increase) with minimal compute overhead</li>
  <li>Training time: 12 hours vs weeks for equivalent dense models</li>
</ul>

<p><strong>Machine Translation (WMT’14)</strong>:</p>

<ul>
  <li>En→Fr: 40.56 BLEU (vs 39.22 for GNMT)</li>
  <li>En→De: 26.03 BLEU (vs 24.91 for GNMT)</li>
  <li>Achieved new state-of-the-art with lower computational cost</li>
  <li>Faster training than dense models with better quality</li>
</ul>

<p><strong>Computational Efficiency</strong>:</p>

<ul>
  <li>MoE models achieved 0.74-1.56 TFLOPS/GPU</li>
  <li>Significant fraction of theoretical maximum (4.29 TFLOPS/GPU)</li>
  <li>Only 37-46% of operations were in expert computations</li>
</ul>

<p><strong>The Breakthrough</strong>: This was the first time conditional computation delivered on its theoretical promise at scale. Previous attempts had struggled with the practical challenges that this paper solved.</p>

<h5 id="from-lstms-to-modern-transformers">From LSTMs to Modern Transformers</h5>

<p>While this paper applied MoE to LSTMs (the dominant architecture in 2017), the core insights proved even more powerful when later applied to Transformers, about which we will learn more about in the later sections.</p>

<p>The path from this 2017 paper to modern LLMs shows how foundational ideas can have delayed but massive impact. Key lessons that influenced later work:</p>

<ol>
  <li><strong>Sparsity enables scale</strong>: The core insight that you can have orders of magnitude more parameters without proportional compute increase</li>
  <li><strong>Load balancing is crucial</strong>: Without proper load balancing, MoE models fail to train effectively</li>
  <li><strong>Engineering matters</strong>: Success required solving practical challenges like communication costs and batch sizing</li>
  <li><strong>Specialization emerges</strong>: Given proper training dynamics, experts will naturally develop useful specializations</li>
</ol>

<p>Today’s largest language models increasingly rely on MoE architectures, making this paper’s contributions more relevant than ever. The ability to scale to trillion-parameter models while maintaining reasonable training costs has become essential for pushing the boundaries of AI capabilities.</p>

<h2 id="2018-bert-and-early-innovations">2018: BERT and Early Innovations</h2>

<h3 id="ulmfit">ULMFiT</h3>

<p><img src="/assets/blog_assets/evolution_of_llms/ULM_abstract.webp" alt="Image of ULMFiT" /></p>

<blockquote>
  <p>Link to paper: <a href="https://arxiv.org/pdf/1801.06146">Universal Language Model Fine-tuning for Text Classification</a></p>
</blockquote>

<details>
<summary>Quick Summary</summary>
<div>

    <p>This 2018 paper by Jeremy Howard and Sebastian Ruder introduces ULMFiT (Universal Language Model Fine-tuning), a method for transfer learning in NLP tasks. The authors present an approach that mirrors the success of transfer learning in computer vision by using a pre-trained language model and fine-tuning it for specific text classification tasks.</p>

    <p>Key contributions:</p>

    <ol>
      <li>
        <p>A three-stage approach to transfer learning for NLP tasks:</p>

        <ul>
          <li>General-domain language model pretraining</li>
          <li>Target task language model fine-tuning</li>
          <li>Target task classifier fine-tuning</li>
        </ul>
      </li>
      <li>
        <p>Novel fine-tuning techniques to prevent catastrophic forgetting:</p>

        <ul>
          <li>Discriminative fine-tuning (using different learning rates for different layers)</li>
          <li>Slanted triangular learning rates (a specific learning rate schedule)</li>
          <li>Gradual unfreezing (progressively unfreezing layers from last to first)</li>
        </ul>
      </li>
      <li>
        <p>State-of-the-art results on six text classification datasets with significant error reductions (18-24%)</p>
      </li>
      <li>
        <p>Impressive sample efficiency - with just 100 labeled examples, the method matches the performance of training from scratch with 10-100x more data</p>
      </li>
    </ol>

    <p>The paper demonstrates that effective transfer learning is possible in NLP without task-specific modifications or architecture changes, using a standard 3-layer LSTM model with careful fine-tuning techniques.</p>

  </div>
</details>
<p><br /></p>

<p>It is unfortunate that such an amazing paper which was a direct influence on the way GPT-1 was trained is often not talked about enough when it comes to LLM. Let’s do justice to that and understand more about Universal Language Model Fine-tuning or ULMFiT!!!</p>

<p><strong>Problem</strong></p>

<blockquote>
  <p>Inductive transfer learning has greatly impacted computer vision, but existing approaches in NLP still require task-specific
modifications and training from scratch.</p>
</blockquote>

<p>Before ULMFiT, transfer learning in NLP had a fatal flaw: catastrophic forgetting. When you fine-tuned a pretrained model on a new task, the model would rapidly “forget” its pretrained knowledge and overfit to the small target dataset. It was like teaching a polyglot to speak a new dialect, only to watch them forget all their other languages in the process.</p>

<p>This made transfer learning frustrating and often counterproductive. Researchers would get excited about pretrained models, only to find that fine-tuning destroyed the very knowledge they wanted to leverage.</p>

<p><strong>Solution</strong></p>

<blockquote>
  <p>ULMFiT, an effective transfer learning method that can be applied to
any task in NLP, and introduce techniques
that are key for fine-tuning a language
model.</p>
</blockquote>

<p>This paper laid crucial groundwork for the LLM revolution we see today.</p>

<p><img src="/assets/blog_assets/evolution_of_llms/4.webp" alt="Image of ULMFiT training" /></p>

<p>The first stage is pretty basic and nothing innovative, but the second &amp; third stage is where the innovation lies.</p>

<p>So far noone had been able to fine tune a general purpose model to perform well on target task which was not present in the original modeling of the original model</p>

<blockquote>
  <p><strong>Why Transfer Learning Failed in NLP</strong> <br />
Unlike computer vision, where you could take ImageNet features and achieve great results on new tasks, NLP models seemed to resist transfer. The problem wasn’t the models themselves but how we fine-tuned them. Traditional approaches used the same learning rate for all layers and froze nothing, causing rapid degradation of learned representations.
ULMFiT’s breakthrough was realizing that different layers need different treatment during fine-tuning.</p>
</blockquote>

<p>Let us understand each stage one by one</p>

<h5 id="1st-stage-general-domain-lm-pretraining">1st stage: General-domain LM pretraining</h5>

<p>This is the simplest albeit the most expensive stage. Take a large dataset that has general information on many topics and train your model on it.</p>

<blockquote>
  <p>“We pretrain the language model on Wikitext-103 (Merity et al., 2017b) consisting of 28,595 preprocessed Wikipedia articles and 103 million words.”</p>
</blockquote>

<p>This helps the model learn general language properties, At this stage it is nothing more than a really good next token predictor.</p>

<p>For example if you asked it the question</p>

<p>“What is the capital of France?”</p>

<p>Instead of getting Paris as the answer you will get. “What is the capital of India ?What is the captial of China” and so on.</p>

<p>That is where the innovative “fine-tuning” part comes in.</p>

<h5 id="2nd-stage-target-task-language-model-fine-tuning">2nd Stage: Target task Language Model fine-tuning</h5>

<p>Fine-tuning let us teach the LLM to follow our task specific requirements and control it’s behaviour in our target dataset.</p>

<p><strong>Discriminative fine-tuning</strong></p>

<p>In any deep neural network, different layers capture different parts of the dataset (This is a very popular <a href="https://poloclub.github.io/cnn-explainer/">article</a> visualizing different parts of a CNN model). So it is reasonable to think that it won’t be a good idea to use the same learning rate to fine-tune each layer.</p>

<p>That is the idea behind discriminative fine-tuning. In this we have a different learning rate for each layer.</p>

<p>Stochastic Gradient Descent</p>

\[\theta_t = \theta_{t-1} - \eta \cdot \nabla_\theta J(\theta)\]

<p>Stochastic Gradient Descent with different learning rate for each layers</p>

\[\theta_t^l = \theta_{t-1}^l - \eta^l \cdot \nabla_{\theta^l} J(\theta)\]

<p>The authors found that it’s best to first find the optimum $\eta^L$ for the last layer, then go downstream following this rule $\eta^{l-1} = \eta^l/2.6$</p>

<p><strong>Slanted triangular learning rates</strong></p>

<p>When fine-tuning a pretrained model, we face a fundamental tension. We want the model to quickly adapt to our target task, but we also need to preserve the valuable knowledge it learned during pretraining. Fixed learning rates can’t solve this dilemma.</p>

<p>To solve that the authors introduced STLR, an adaptive learning rate that changes with number of iterations.</p>

<p>The idea is, start with a higher learning rate to rapidly adapt the model to the target task’s patterns, then gradually decrease it to fine-tune without destroying pretrained representations.</p>

<p><img src="/assets/blog_assets/evolution_of_llms/20.webp" alt="Image of STLR" /></p>

<p>Think of it like learning to drive in a new city. Initially, you drive faster to quickly get oriented and find the general area you need. Once you’re close to your destination, you slow down to carefully navigate the final streets and parking.</p>

<p>This was achieved using the following formula</p>

\[\text{cut} = \lfloor T \cdot \text{cut\_frac} \rfloor\]

\[p = \begin{cases}
t/\text{cut}, &amp; \text{if } t &lt; \text{cut} \\
1 - \frac{t-\text{cut}}{\text{cut} \cdot (1/\text{cut\_frac} - 1)}, &amp; \text{otherwise}
\end{cases}\]

\[\eta_t = \eta_{\max} \cdot \frac{1 + p \cdot (\text{ratio} - 1)}{\text{ratio}}\]

<p>Where:</p>

<ul>
  <li>$T$ is the total number of training iterations</li>
  <li>$\text{cut_frac}$ is the fraction of iterations for learning rate increase (typically 0.1)</li>
  <li>$\text{cut}$ is the iteration where we switch from increasing to decreasing</li>
  <li>$p$ is the fraction of iterations in current phase</li>
  <li>$\text{ratio}$ specifies how much smaller the lowest LR is from maximum (typically 32)</li>
  <li>$\eta_{\max}$ is the maximum learning rate (typically 0.01)</li>
  <li>$\eta_t$ is the learning rate at iteration $t$</li>
</ul>

<h5 id="3rd-stage-target-task-classifier-fine-tuning">3rd Stage: Target task classifier fine-tuning</h5>

<p>The final stage adds a classifier head to the fine-tuned language model and trains it for the specific task. This stage introduces several crucial techniques that prevent the model from forgetting its pretrained knowledge.</p>

<p><strong>Gradual Unfreezing</strong></p>

<p><img src="/assets/blog_assets/evolution_of_llms/44.webp" alt="Image of Gradual Unfreezing" /></p>

<p>Rather than fine-tuning all layers at once, which risks catastrophic forgetting, ULMFiT gradually unfreezes layers starting from the last layer. The intuition is elegant: the last layers contain the most task-specific knowledge, while early layers capture universal language features that should change slowly.</p>

<p>The process works like this: First, unfreeze only the classifier head and train for one epoch. Then unfreeze the next layer down and continue training. Repeat this process until all layers are unfrozen. This gradual approach gives the model time to adapt each layer’s representations smoothly without destroying the foundation built in earlier layers.</p>

<p><strong>Concat Pooling</strong></p>

<p><img src="/assets/blog_assets/evolution_of_llms/45.webp" alt="Image of Concat Pooling" /></p>

<p>ULMFiT also introduced concat pooling for text classification. Instead of using only the final hidden state, it concatenates the last hidden state with both max-pooled and mean-pooled representations across all timesteps. This captures information from the entire document, not just the end.</p>

<p><strong>BPTT for Text Classification (BPT3C)</strong></p>

<p><img src="/assets/blog_assets/evolution_of_llms/46.webp" alt="Image of BPT3C" /></p>

<p>For handling long documents, ULMFiT adapts backpropagation through time by dividing documents into fixed-length batches while maintaining hidden state continuity between batches.</p>

<p><strong>Revolutionary Results</strong></p>

<p>ULMFiT’s results were unprecedented for their time. On six text classification datasets, the method achieved 18-24% error reduction compared to state-of-the-art approaches. More impressively, with just 100 labeled examples, ULMFiT matched the performance of training from scratch with 10-100× more data.</p>

<p>This sample efficiency was the real game-changer. Suddenly, high-quality NLP became accessible to domains with limited labeled data, democratizing the field in ways that hadn’t been possible before.</p>

<h3 id="elmo-embeddings-from-language-models">ELMo: Embeddings from Language Models</h3>

<p><img src="/assets/blog_assets/evolution_of_llms/ELMO_abstract.webp" alt="Image of ELMo" /></p>

<blockquote>
  <p>Link to paper: <a href="https://arxiv.org/abs/1802.05365">Deep contextualized word representations</a></p>
</blockquote>

<details>
<summary>Quick Summary</summary>
<div>

    <p>This paper introduces ELMo (Embeddings from Language Models), a new approach to creating word representations that capture both complex word characteristics (syntax and semantics) and how those characteristics change across different contexts (addressing polysemy). Unlike traditional word embeddings that assign a single vector per word, ELMo derives representations from a bidirectional language model (biLM) pre-trained on a large text corpus.</p>

    <p>The key innovation is that ELMo representations are deep - they’re a function of all internal layers of the biLM, not just the final layer. The authors show that different layers capture different linguistic properties (lower layers capture syntax, higher layers capture semantics). By learning task-specific weightings of these layers, models can access both types of information simultaneously.</p>

    <p>The authors demonstrate that adding ELMo to existing models significantly improves performance across six diverse NLP tasks, including question answering, textual entailment, sentiment analysis, and named entity recognition - achieving state-of-the-art results in all cases, with relative error reductions ranging from 6-20%.</p>

  </div>
</details>
<p><br /></p>

<p>Embeddings are a vital part of LLMs, because they determine the very understanding of tokens. Think of them as the language that allows machines to understand and work with human text. Just like how we might describe a person using various characteristics (height, age, personality), embeddings describe words using hundreds of numerical features that capture their meaning, relationships, and usage patterns.</p>

<p><strong>Problem</strong></p>

<p>Before ELMo, word embeddings had a major limitation: each word had only one representation regardless of context. The word “bank” would have the same vector whether you’re talking about a financial institution or the side of a river. This is called the polysemy problem - one word, multiple meanings, but only one embedding.</p>

<blockquote>
  <p>learning high quality representations can be challenging. They should ideally
model both (1) complex characteristics of word
use (e.g., syntax and semantics), and (2) how these
uses vary across linguistic contexts (i.e., to model
polysemy).</p>
</blockquote>

<p><strong>Solution</strong></p>

<p>ELMo’s breakthrough was making embeddings contextual. Instead of giving “bank” the same representation everywhere, ELMo creates different representations based on the surrounding words. When “bank” appears near “loan” and “mortgage,” it gets a finance-related representation. When it appears near “river” and “water,” it gets a geography-related representation.</p>

<blockquote>
  <p>Our representations differ from traditional word
type embeddings in that each token is assigned a
representation that is a function of the entire input
sentence. We use vectors derived from a bidirectional LSTM that is trained with a coupled language model (LM) objective on a large text corpus.</p>
</blockquote>

<p>Before we start talking about ELMo we have to understand how word embeddings work and what they are. You can skip this section if you have an extensive understanding of the topic at hand</p>

<p>This <a href="https://pythonandml.github.io/dlbook/content/word_embeddings/traditional_word_embeddings.html">book</a> proved to be extremely helpful while writing this section.</p>

<h5 id="traditional-word-embeddings">Traditional Word Embeddings</h5>

<p>Machines do not understand text but rather numbers. So researchers have come up with ways to represent words as numbers that still captures their complexity, semantics, meaning etc. Let’s go one by one.</p>

<p><strong>One-Hot Vectors</strong></p>

<p>One of the simplest solution is to create <a href="https://en.wikipedia.org/wiki/One-hot">one hot encoding</a> for each word.</p>

<p><img src="/assets/blog_assets/evolution_of_llms/OHV.webp" alt="Image of One Hot Vector" /></p>

<p>Here, every word has been assigned a unique vector and the length of our one-hot encoded vector would be equal to the size of $V$ ($|V| = 3$).</p>

<blockquote>
  <p>Note:</p>

  <ul>
    <li>In OHE words are independant of each other and hence do not capture any relationship between them</li>
    <li>OHE is computationally expensive as in reality the size of vocabulary can be in billions</li>
  </ul>
</blockquote>

<p><strong>Bag-of-Words (BOW)</strong></p>

<p>Think of BOW as the most straightforward way to convert text into numbers - it’s like counting how many times each word appears in a document, completely ignoring the order.</p>

<p>BOW creates a document-term matrix where each row represents a document and each column represents a word from our vocabulary. Each cell contains the frequency of that word in that specific document.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Simple BOW implementation
</span><span class="n">documents</span> <span class="o">=</span> <span class="p">[</span><span class="s">'this is the first document'</span><span class="p">,</span>
             <span class="s">'this document is the second document'</span><span class="p">,</span>
             <span class="s">'this is the third one'</span><span class="p">,</span>
             <span class="s">'is this the first document'</span><span class="p">]</span>

<span class="c1"># Create vocabulary
</span><span class="n">vocab</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">documents</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">doc</span><span class="p">.</span><span class="n">split</span><span class="p">():</span>
        <span class="k">if</span> <span class="n">word</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">vocab</span><span class="p">:</span>
            <span class="n">vocab</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">word</span><span class="p">)</span>

<span class="c1"># Create BOW matrix
</span><span class="n">bow_matrix</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">documents</span><span class="p">:</span>
    <span class="n">word_count</span> <span class="o">=</span> <span class="p">[</span><span class="n">doc</span><span class="p">.</span><span class="n">split</span><span class="p">().</span><span class="n">count</span><span class="p">(</span><span class="n">word</span><span class="p">)</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">vocab</span><span class="p">]</span>
    <span class="n">bow_matrix</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">word_count</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s">"Vocab:"</span><span class="p">,</span> <span class="n">vocab</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"BOW Matrix:"</span><span class="p">,</span> <span class="n">bow_matrix</span><span class="p">)</span>

<span class="c1"># Vocab:
# ['this', 'is', 'the', 'first', 'document', 'second', 'third', 'one']
</span>
<span class="c1">#BOW Matrix:
# [[1, 1, 1, 1, 1, 0, 0, 0],
#  [1, 1, 1, 0, 2, 1, 0, 0],
#  [1, 1, 1, 0, 0, 0, 1, 1],
#  [1, 1, 1, 1, 1, 0, 0, 0]]
</span></code></pre></div></div>

<p>The problem? BOW treats “The cat sat on the mat” and “The mat sat on the cat” as identical because it only cares about word counts, not context or order. Plus, common words like “the” and “is” get the same weight as meaningful words.</p>

<p><strong>Co-occurrence Matrix</strong></p>

<p>Instead of just counting words in documents, what if we count how often words appear <em>together</em>? That’s exactly what co-occurrence matrices do.</p>

<p>A co-occurrence matrix shows how frequently word pairs appear within a defined window (like within the same sentence). If “learning” and “machine” often appear together, they’ll have a high co-occurrence score.</p>

<p><img src="/assets/blog_assets/evolution_of_llms/35.webp" alt="Image of One Hot Vector" /></p>

<p>The mathematical representation: For words $w_i$ and $w_j$, the co-occurrence count $C_{ij}$ represents how many times they appear together within a context window.</p>

\[C_{ij} = \sum_{k=1}^{N} \mathbb{I}(w_i, w_j \text{ co-occur in context } k)\]

<p>Where $\mathbb{I}$ is an indicator function and $N$ is the total number of contexts.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Simple co-occurrence matrix
</span><span class="k">def</span> <span class="nf">build_cooccurrence_matrix</span><span class="p">(</span><span class="n">documents</span><span class="p">,</span> <span class="n">window_size</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    <span class="c1"># Flatten all documents into one list
</span>    <span class="n">all_words</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">documents</span><span class="p">:</span>
        <span class="n">all_words</span><span class="p">.</span><span class="n">extend</span><span class="p">(</span><span class="n">doc</span><span class="p">.</span><span class="n">split</span><span class="p">())</span>

    <span class="c1"># Create vocabulary
</span>    <span class="n">vocab</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">all_words</span><span class="p">))</span>
    <span class="n">vocab_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">vocab</span><span class="p">)</span>

    <span class="c1"># Initialize matrix
</span>    <span class="n">cooc_matrix</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">)]</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">)]</span>

    <span class="c1"># Count co-occurrences
</span>    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">all_words</span><span class="p">)):</span>
        <span class="n">target_word</span> <span class="o">=</span> <span class="n">all_words</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="n">target_idx</span> <span class="o">=</span> <span class="n">vocab</span><span class="p">.</span><span class="n">index</span><span class="p">(</span><span class="n">target_word</span><span class="p">)</span>

        <span class="c1"># Check words within window
</span>        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">i</span><span class="o">-</span><span class="n">window_size</span><span class="p">),</span> <span class="nb">min</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">all_words</span><span class="p">),</span> <span class="n">i</span><span class="o">+</span><span class="n">window_size</span><span class="o">+</span><span class="mi">1</span><span class="p">)):</span>
            <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="n">j</span><span class="p">:</span>
                <span class="n">context_word</span> <span class="o">=</span> <span class="n">all_words</span><span class="p">[</span><span class="n">j</span><span class="p">]</span>
                <span class="n">context_idx</span> <span class="o">=</span> <span class="n">vocab</span><span class="p">.</span><span class="n">index</span><span class="p">(</span><span class="n">context_word</span><span class="p">)</span>
                <span class="n">cooc_matrix</span><span class="p">[</span><span class="n">target_idx</span><span class="p">][</span><span class="n">context_idx</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="k">return</span> <span class="n">cooc_matrix</span><span class="p">,</span> <span class="n">vocab</span>
</code></pre></div></div>

<p>The beauty of co-occurrence matrices is that they capture some semantic relationships - words that often appear together likely have related meanings.</p>

<p><strong>N-Gram</strong></p>

<p>N-grams extend the BOW concept by considering sequences of $n$ consecutive words instead of individual words. This helps capture some word order and context.</p>

<ul>
  <li><strong>Unigrams (n=1)</strong>: Individual words → [“this”, “is”, “the”, “first”]</li>
  <li><strong>Bigrams (n=2)</strong>: Word pairs → [“this is”, “is the”, “the first”]</li>
  <li><strong>Trigrams (n=3)</strong>: Word triplets → [“this is the”, “is the first”]</li>
</ul>

<p>The mathematical formulation for n-gram probability:
\(P(w_n|w_1, w_2, ..., w_{n-1}) \approx P(w_n|w_{n-k+1}, ..., w_{n-1})\)</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">generate_ngrams</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">n</span><span class="p">):</span>
    <span class="n">words</span> <span class="o">=</span> <span class="n">text</span><span class="p">.</span><span class="n">split</span><span class="p">()</span>
    <span class="n">ngrams</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">words</span><span class="p">)</span> <span class="o">-</span> <span class="n">n</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
        <span class="n">ngram</span> <span class="o">=</span> <span class="s">' '</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">words</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="n">n</span><span class="p">])</span>
        <span class="n">ngrams</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">ngram</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">ngrams</span>
</code></pre></div></div>

<p>The trade-off? Higher n-values capture more context but create exponentially more features and become computationally expensive.</p>

<p><strong>TF-IDF (Term Frequency-Inverse Document Frequency)</strong></p>

<p>TF-IDF is the smart cousin of BOW. It doesn’t just count words - it considers how important a word is to a specific document relative to the entire collection.</p>

<p>The intuition: Words that appear frequently in one document but rarely across all documents are more significant for that specific document.</p>

\[\text{TF-IDF}(t,d) = \text{TF}(t,d) \times \text{IDF}(t)\]

<p>Where:</p>

<ul>
  <li>$\text{TF}(t,d) = \frac{\text{count of term } t \text{ in document } d}{\text{total words in document } d}$</li>
  <li>$\text{IDF}(t) = \log\left(\frac{\text{total documents}}{\text{documents containing term } t}\right)$</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>

<span class="n">documents_name</span> <span class="o">=</span> <span class="p">[</span><span class="s">"Document-1"</span><span class="p">,</span> <span class="s">"Document-2"</span><span class="p">,</span> <span class="s">"Document-3"</span><span class="p">,</span> <span class="s">"Document-4"</span><span class="p">]</span>

<span class="n">documents</span> <span class="o">=</span> <span class="p">[</span><span class="s">'this is the first document'</span><span class="p">,</span>
             <span class="s">'this document is the second document'</span><span class="p">,</span>
             <span class="s">'this is the third one'</span><span class="p">,</span>
             <span class="s">'is this the first document'</span><span class="p">]</span>

<span class="n">word_tokens</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">document</span> <span class="ow">in</span> <span class="n">documents</span><span class="p">:</span>
    <span class="n">words</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">document</span><span class="p">.</span><span class="n">split</span><span class="p">(</span><span class="s">" "</span><span class="p">):</span>
        <span class="n">words</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">word</span><span class="p">)</span>
    <span class="n">word_tokens</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">words</span><span class="p">)</span>

<span class="n">vocab</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>

<span class="k">for</span> <span class="n">document</span> <span class="ow">in</span> <span class="n">word_tokens</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">document</span><span class="p">:</span>
        <span class="n">vocab</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">word</span><span class="p">)</span>

<span class="n">vocab</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">vocab</span><span class="p">)</span>

<span class="n">TF</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span> <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">vocab</span><span class="p">))]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">word_tokens</span><span class="p">))]</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">document</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">word_tokens</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">document</span><span class="p">:</span>
        <span class="n">j</span> <span class="o">=</span> <span class="n">vocab</span><span class="p">.</span><span class="n">index</span><span class="p">(</span><span class="n">word</span><span class="p">)</span>
        <span class="n">TF</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">word_tokens</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>

<span class="n">idf</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">vocab</span><span class="p">))]</span>
<span class="n">D</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">word_tokens</span><span class="p">)</span>

<span class="k">for</span> <span class="n">j</span><span class="p">,</span> <span class="n">word_vocab</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">vocab</span><span class="p">):</span>
    <span class="n">df</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">document</span> <span class="ow">in</span> <span class="n">word_tokens</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">document</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">word_vocab</span> <span class="o">==</span> <span class="n">word</span><span class="p">:</span>
                <span class="n">df</span> <span class="o">+=</span> <span class="mi">1</span>
                <span class="k">break</span>
    <span class="n">idf</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">D</span><span class="o">/</span><span class="n">df</span><span class="p">)</span>

<span class="n">TFIDF</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span> <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">vocab</span><span class="p">))]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">word_tokens</span><span class="p">))]</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">word_tokens</span><span class="p">)):</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">vocab</span><span class="p">)):</span>
        <span class="n">TFIDF</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">TF</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span> <span class="o">*</span> <span class="n">idf</span><span class="p">[</span><span class="n">j</span><span class="p">]</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">TFIDF</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">vocab</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">documents_name</span><span class="p">).</span><span class="nb">round</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
<span class="n">data</span>
</code></pre></div></div>

<p><em>Code taken from <a href="https://pythonandml.github.io/dlbook/content/word_embeddings/traditional_word_embeddings.html#">here</a></em></p>

<p>if we calculate tf-idf of the same example as before we get an output like below</p>

<p><img src="/assets/blog_assets/evolution_of_llms/47.webp" alt="Image of One Hot Vector" /></p>

<p>Notice how common words like “this”, “is”, “the” get lower TF-IDF scores because they appear in most documents, while specific words like “second” or “third” get higher scores.</p>

<p><strong>The Limitation of Traditional Embeddings</strong></p>

<p>All these methods share a fundamental flaw: they assign the same representation to a word regardless of context. The word “bank” gets the same vector whether we’re talking about a river bank or a financial institution. This is where contextual embeddings like ELMo come to the rescue.</p>

<h5 id="static-word-embeddings">Static Word Embeddings</h5>

<p><strong>Word2Vec</strong></p>

<p>The following are excellect sources to understand Word2Vec in a deeper level. Consider going through them</p>

<ul>
  <li><a href="https://jalammar.github.io/illustrated-Word2Vec/">The Illustrated Word2Vec</a></li>
  <li><a href="https://mccormickml.com/2016/04/19/Word2Vec-tutorial-the-skip-gram-model/">Word2Vec Tutorial - The Skip-Gram Model</a></li>
</ul>

<p><strong>Intuition</strong></p>

<p>We are going to start with the old and cheesy explanation of Word2Vec. Talking about similarity of vector representation in space. (If you do not understand what I am talking about, Most explanations of Word2Vec use the explanation I am about to give. Fret not, for we will dive deeper too!)</p>

<p>I absolutely love cheese. And I have scale in which I measure how cheesy a piece of cheese is.
<img src="/assets/blog_assets/evolution_of_llms/21.webp" alt="Image of Word2Vec explanation" /></p>

<p>Anytime I find a new cheese, I taste it and put it on my scale. I use this scale to choose which cheese to eat based on my mood.</p>

<p><img src="/assets/blog_assets/evolution_of_llms/22.webp" alt="Image of Word2Vec ex5planation" /></p>

<p>But one day I ran into a cheese (yellow cheese) which had the same cheesiness as the white cheese. Now how do I differentiate between the two? well cheese has many other properties (or features from the ML perspective). Like protein!!</p>

<p><img src="/assets/blog_assets/evolution_of_llms/23.webp" alt="Image of Word2Vec explanation" /></p>

<p>I can add that as another axis and I can use that as another metric to choose the kind of cheese I want to eat.</p>

<p><img src="/assets/blog_assets/evolution_of_llms/24.webp" alt="Image of Word2Vec explanation" /></p>

<p>This way of plotting cheese based on different properties provides with another amazing way. One day a friend of mine came and said he really liked red cheese, but I was out of red cheese :(
because I love it too.</p>

<p>So I can just find the cheese which is most similar to it, using cosine similarity!!</p>

<p><img src="/assets/blog_assets/evolution_of_llms/25.webp" alt="Image of Word2Vec explanation" /></p>

<p>That is essentially the idea of Word2Vec, We plot multiple words in an n dimensional space (I used 2 dimensional because I can plot it. I can’t plot a 7d space, I will love to meet you if you can tho!!!). And find similar words based on cosine similarity.</p>

<p>The cosine similarity between two vectors A and B is calculated as:</p>

\[\cos(\theta) = \frac{A \cdot B}{||A|| ||B||} = \frac{\sum_{i=1}^{n} A_i B_i}{\sqrt{\sum_{i=1}^{n} A_i^2} \sqrt{\sum_{i=1}^{n} B_i^2}}\]

<p>This gives us a value between -1 and 1, where 1 means the vectors point in exactly the same direction (very similar), 0 means they’re perpendicular (unrelated), and -1 means they point in opposite directions (very different).</p>

<p>There is a popular example that shows the distance between king and woman is same as the distance between man and woman. This essentially shows that both the pair of words share very similar ideas with only a few differences (maybe in royalty).</p>

<p><img src="/assets/blog_assets/evolution_of_llms/26.webp" alt="Image of Word2Vec explanation" /></p>

<p>In reality Word2Vec looks something more like the below image (ouch!!).</p>

<p><img src="/assets/blog_assets/evolution_of_llms/27.webp" alt="Image of Word2Vec explanation" />
<em>Image taken from <a href="https://projector.tensorflow.org/">Embedding Projector</a></em></p>

<p><strong>Skip gram model</strong></p>

<p>Now that we understand the idea behind Word2Vec, it’s time to understand how it is implemented. I will be skipping Continous Bag of words, an idea introduced in the original paper as it is not essential in my opinion. You can read more about it in the sources I have provided.</p>

<table>
  <tbody>
    <tr>
      <td>Skip gram in 1 sentence -&gt; Given a word, find it’s nearby words (context words).</td>
    </tr>
    <tr>
      <td>CBOW in 1 sentence -&gt; Given context words, find the target word.</td>
    </tr>
  </tbody>
</table>

<p>One of the many beautiful thing about NLP is, you can create a labelled dataset using an unlabelled dataset. Our objective is to build a model that finds words nearby a target word.</p>

<p>Let us understand how the training dataset is created using a sample sentence. We first define a window (here it’s 2), we have our target word with the relevant context words around it.</p>

<p><img src="/assets/blog_assets/evolution_of_llms/28.webp" alt="Image of Word2Vec explanation" /></p>

<p>Now that we have our training data, let us understand how the model itself is constructed. Remember how we talked about cheesiness as a feature for our cheese, then protein, well both of them can be described as their own individual neuron. So in the example below we have an embedding model with 300 as it’s dimension (In modern embedding model when you talk about dimensions, it is essentially talking about how many features can be used to describe one particular token, as a rule of thumb higher the dimension, more complex is it’s representation)</p>

<p>So for any given word, we can create a one hot encoding, then pass it through our embedding model. Which is then passed to the final Softmax layer which gives the probability of all the words which are likely to be a context word.</p>

<blockquote>
  <p>I have a question for you, What is the difference between nn.Embedding and nn.Sequential?</p>
</blockquote>

<p><img src="/assets/blog_assets/evolution_of_llms/29.webp" alt="Image of Word2Vec explanation" /></p>

<p>One thing I found interesting was, we talk about “Embedding Matrix” but the above image describes a typical neural network model, what is going on over here?</p>

<p>Well if we look below, it becomes much easier to understand. The word vector represents the OHE encoding of the chosen work.</p>

<p>Each row of the embedding matrix represents the weights of every word in that neuron. So when we do a matrix multiplication, we get the weight for that word from different neurons.</p>

<p>It’s easier to visualize as different embedding vectors for each token of a fixed dimension.</p>

<p><img src="/assets/blog_assets/evolution_of_llms/30.webp" alt="Image of Word2Vec explanation" /></p>

<p>Now we just run a training loop and voila, the hidden layer, is our embedding matrix. But we have a problem. As this was an example we used a vocabulary of a 100 words, but in reality the vocabulary is 100x times larger. And calculating softmax of so many tokens is a very expensive operation.</p>

<p>imagine a vocabulary of 10,000 tokens (quite small from modern vocabs) with 500 dimensional embedding model. That’s 5 million training parameter!!!</p>

<p>There are a few solutions proposed by the researchers to overcome this problem. Interestingly the <a href="https://arxiv.org/pdf/1301.3781">original paper</a> only mentions Hierarical Softmax, but the <a href="https://code.google.com/archive/p/Word2Vec/">code</a> shared by researches talks about sub-sampling and Negative Sampling. So let’s talk about all three!</p>

<blockquote>
  <p>[UPDATE] I later found out that they introduced these ideas in there follow up paper <a href="https://arxiv.org/pdf/1310.4546">here</a>. (Have a look at the authors… it is something haha)</p>
</blockquote>

<p>This <a href="https://www.ruder.io/word-embeddings-softmax/#negativesampling">blog</a> covered the mentioned topics quite well and I have taken inspiration from it while writing this section.</p>

<p><strong>Sub-Sampling</strong></p>

<p><img src="/assets/blog_assets/evolution_of_llms/31.webp" alt="Image of Word2Vec explanation" /></p>

<p>If we look at one particular example from our dataset generation step, particularly the 3rd line. We will see that <code class="language-plaintext highlighter-rouge">is</code> is a word that must be quite common in sentences. And hence we can expect to run into many pairs of <code class="language-plaintext highlighter-rouge">is, ...</code></p>

<p>To fix this problem, the authors introduced a sampling rate.</p>

\[P(w_i) = 1 - \sqrt{t/f(w_i)}\]

<p>Where $f(w_i)$ is the frequency of the word and $t$ is a chosen threshold.</p>

<p><strong>Negative Sampling (better for frequent words, better with low dimensional vectors)</strong></p>

<p>The idea is quite interesting, If we go back to the example that we started with. I.e training on a vocab of 100 words. Our main problem was that it was very expensive to calculate the softmax of so many tokens.</p>

<p>So what if instead of training on all the tokens, we took a small subset of the negative samples. I.e tokens which are not related to our target word, and take a bunch of context words. And train it using logistic regression. In other words, tell if two words are neighbours or not.</p>

<p><img src="/assets/blog_assets/evolution_of_llms/32.webp" alt="Image of Word2Vec explanation" /></p>

<p>This greatly simplifies training, and reduces the cost significantly too. One interesting thing to note is the authors used a <code class="language-plaintext highlighter-rouge">unigram distribution</code> to pull the negative samples out of.</p>

\[P(w_i) = {f(w_i)}^{3/4}/\sum({f(w_i)}^{3/4})\]

<p>If you wish to learn more about negative sampling consider reading <a href="https://mccormickml.com/2017/01/11/Word2Vec-tutorial-part-2-negative-sampling/">this</a>.</p>

<p>I find “why this even works” more interesting than the fact that it works (And this produces better results than our original method). It’s actual name is noise contrastive estimation and the way I imagine it is, we are pulling similar words together while pushing dissimilar words away. Now if we pushed all of the dissimilar words away as we were doing we inevitably pushed away similar words for different pairs too.</p>

<p>Consider reading about contrastive loss. (This is a good <a href="https://medium.com/@maksym.bekuzarov/losses-explained-contrastive-loss-f8f57fe32246">medium article</a> on the topic, Consider looking at the author of contrastive loss too… it’s somehting haha)</p>

<p><strong>Hierarical Softmax (better for infrequent words)</strong></p>

<p>Let’s start with an example, because that really drives the idea home.</p>

<p>“I can have pizza every day”</p>

<p>If we used the standard softmax to calculate the probability of “pizza” given some context, we would do something like:</p>

\[P(pizza|context) = \frac{e^{v'_{pizza} \cdot h}}{\sum_{w \in V} e^{v'_w \cdot h}}\]

<p>Where $h$ is the hidden layer output (context vector) and $v’_w$ are the output embeddings.</p>

<p>But Hierarchical Softmax constructs a binary tree and we get a structure like below:</p>

<p><img src="/assets/blog_assets/evolution_of_llms/33.webp" alt="Image of Word2Vec explanation" /></p>

<blockquote>
  <p>Understanding how the binary tree is constructed is beyond the scope of this article, but if you understand how <a href="https://en.wikipedia.org/wiki/Huffman_coding">Huffman Encoding</a> works, that is one way of creating the tree. More frequent words get shorter paths, less frequent words get longer paths.</p>
</blockquote>

<p>The equation then becomes:</p>

\[P(pizza|context) = \sigma(v'_{root} \cdot h) \times \sigma(-v'_{n_1} \cdot h) \times \sigma(v'_{n_4} \cdot h)\]

<p>Where:</p>

<ul>
  <li>$\sigma(x) = \frac{1}{1 + e^{-x}}$ is the sigmoid function</li>
  <li>$v’_{n_i}$ are the learned vectors for internal nodes</li>
  <li>The negative sign appears when we take the left branch at a node</li>
</ul>

<p>This reduces the computation from $O(|V|)$ to $O(\log|V|)$, where $V$ is the size of the vocabulary.</p>

<p>If you wish to learn more about Hierarchical Softmax consider reading this <a href="https://talbaumel.github.io/blog/softmax/">blog</a>.</p>

<p><strong>GloVe</strong></p>

<p>This <a href="https://wandb.ai/authors/embeddings-2/reports/An-Introduction-to-the-Global-Vectors-GloVe-Algorithm--VmlldzozNDg2NTQ">blog</a> helped me considerably while writing this section.</p>

<p>GloVe stands for Global Vectors for Word Representation, it is seemingly an improvement over Word2Vec as it considers global statistics over local statistics.</p>

<p>Well what does that mean? Put simply, we leverage the global co-occurrence matrix instead of using a local context window like we did in Word2Vec.</p>

<p>The main innovation behind GloVe is the idea that we only need to calculate the ratio of probability of the occurrence of two words to capture their semantic relation. This ratio-based approach helps filter out noise from non-discriminative words and highlights meaningful relationships.</p>

<p>First we create a co-occurrence matrix $X$ based on the available corpus. The notation $X_{ij}$ refers to number of times word j has appeared in the context of word i. We calculate the probability of a word j occurring given i as $P(j|i) = X_{ij}/X_i$, where $X_i$ is the sum of all co-occurrence counts for word i (i.e., $X_i = \sum_k X_{ik}$).</p>

<p><img src="/assets/blog_assets/evolution_of_llms/35.webp" alt="Image of Word2Vec explanation" /></p>

<p>Let’s understand it with an example.</p>

<p>We created the co-occurrence matrix for two sentences “Pizza is the best” and “Margherita is my favorite” using a window size of 1 (immediate neighbors only).</p>

<p>Let’s calculate the probability of “Pizza” given “is” (this is from a very small corpus only to show how it is calculated, it is not reminiscent of the actual results).</p>

<p>From our matrix, “is” co-occurs with three words: “Pizza” (1 time), “the” (1 time), and “Margherita” (1 time). So the total count for “is” is 3.</p>

\[P(\text{Pizza}|\text{is}) = \frac{X_{\text{is,Pizza}}}{X_{\text{is}}} = \frac{1}{3} = 0.33\]

<p><img src="/assets/blog_assets/evolution_of_llms/34.webp" alt="Image of Word2Vec explanation" />
<em>Image taken from the original paper</em></p>

<p>If we look at the example provided by the authors from a real corpus, the power of ratios becomes clear. It is pretty intuitive that $P(solid|ice)$ will have a higher value than $P(solid|steam)$, because “solid” is more likely to appear in the context of “ice” than “steam”. Hence their ratio $P(solid|ice)/P(solid|steam) = 8.9$ has a large value, indicating “solid” is discriminative for “ice”.</p>

<p>For “gas”, we see the opposite: $P(gas|ice)/P(gas|steam) = 0.085$, a small ratio indicating “gas” is discriminative for “steam”.</p>

<p>Whereas for “water”, both ice and steam are likely to co-occur with it, so the ratio $P(water|ice)/P(water|steam) = 1.36$ is close to 1, indicating “water” doesn’t discriminate between ice and steam. More interesting is “fashion” with a ratio of 0.96 ≈ 1, because both ice and steam are unlikely to be related to fashion. This ratio-based approach elegantly filters out such neutral co-occurrences that don’t provide semantic information.</p>

<p>This insight - that ratios of co-occurrence probabilities capture semantic relationships better than raw probabilities - forms the mathematical foundation of GloVe’s log-bilinear regression model.</p>

<p>To understand more about the implementation and training details, consider reading the original <a href="https://nlp.stanford.edu/pubs/glove.pdf">paper</a> and this <a href="https://cran.r-project.org/web/packages/text2vec/vignettes/glove.html">article</a>.</p>

<blockquote>
  <p>I have skipped a lot of the different parts, like training, eval, results etc. Because this is ultimately a section on ELMo and not GloVe. But the idea is fascinating enough to garner some time spent on it.</p>
</blockquote>

<h5 id="contextual-word-embeddings">Contextual Word Embeddings</h5>

<p><strong>Embeddings from Language Models (ELMo)</strong></p>

<p>We more or less now have a complete understanding of how different embedding models work, so it’s time to understand the model of the hour: ELMo.</p>

<p>There are two things we need to understand: how it’s trained and how it’s used.</p>

<p>Training is quite simple - we train a two-layer bi-directional LSTM on a Language Modeling task.</p>

<p>The Language Modeling task basically means: given all these words, what’s the most likely word that comes next? It is exactly how GPT-1 was trained, which we will be covering next.</p>

<p><img src="/assets/blog_assets/evolution_of_llms/36.webp" alt="Image of Word2Vec explanation" />
<em>image inspired from this <a href="https://jalammar.github.io/illustrated-bert/">blog</a></em></p>

<p>Now I had a question while reading this: ELMo is an embedding model, right? But what we are doing is next token prediction here. How is it used with other NLP models then? If you have the same question, you are on the right track. It is quite an innovative solution in my opinion.</p>

<p>Let us first start with the very essence of any NLP task: We begin with a sentence, right? We can use this sentence, pass it to our trained ELMo model, and extract representations from different layers. <strong>The key insight is that we don’t just use the final layer - we combine representations from all layers (character embeddings + both LSTM layers) using learned weights.</strong></p>

<p><img src="/assets/blog_assets/evolution_of_llms/37.webp" alt="Image of Word2Vec explanation" /></p>

<p>We can then give these combined embeddings to any other NLP model. Voila! This understanding will prove to be extremely useful as we move to bigger LLMs. While modern embedding models don’t use ELMo’s specific bidirectional LSTM approach, ELMo’s key innovation of contextual embeddings and the concept of using pre-trained language models for embeddings laid the groundwork for today’s transformer-based embedding models.</p>

<h3 id="gpt-1">GPT-1</h3>

<p><img src="/assets/blog_assets/evolution_of_llms/gpt1_abstract.webp" alt="Image of GPT-1" /></p>

<blockquote>
  <p>Link to paper: <a href="https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf">Improving Language Understanding by Generative Pre-Training</a>
, <a href="https://openai.com/index/language-unsupervised/">blog</a></p>
</blockquote>

<details>
<summary>Quick Summary</summary>
<div>

    <p>This seminal 2018 paper from OpenAI researchers (Radford, Narasimhan, Salimans, and Sutskever) introduces a powerful semi-supervised approach to natural language understanding that combines unsupervised pre-training with supervised fine-tuning.</p>

    <p>The key innovation lies in training a large transformer-based language model on unlabeled text data, then leveraging the learned representations by fine-tuning this model on specific downstream tasks. This approach addresses a fundamental challenge in NLP: the scarcity of labeled data for various language understanding tasks.</p>

    <p>The authors demonstrate that their method significantly outperforms task-specific architectures across 9 out of 12 NLP tasks, including natural language inference, question answering, semantic similarity, and text classification. Notable improvements include:</p>

    <ul>
      <li>8.9% on commonsense reasoning (Stories Cloze Test)</li>
      <li>5.7% on question answering (RACE)</li>
      <li>1.5% on textual entailment (MultiNLI)</li>
    </ul>

    <p>This approach minimizes task-specific architecture modifications by using “task-aware input transformations,” which convert structured inputs into a sequence format compatible with the pre-trained model.</p>

    <p>This paper laid important groundwork for later transformer-based language models, demonstrating that generative pre-training on unlabeled data could significantly improve performance on downstream language understanding tasks.</p>

  </div>
</details>
<p><br /></p>

<p>GPT-1 was the moment when everything clicked. While ULMFiT showed that transfer learning could work in NLP and BERT demonstrated the power of bidirectional representations, GPT-1 proved that a simple, scalable approach just predicting the next word could create surprisingly capable language models. This paper didn’t just introduce a new model, it established the blueprint that lead to GPT-2, GPT-3, and the entire generative AI revolution.</p>

<p><strong>Problem</strong></p>

<blockquote>
  <p>The ability to learn effectively from raw text is crucial to alleviating the dependence on supervised
learning in natural language processing (NLP). Most deep learning methods require substantial
amounts of manually labeled data, which restricts their applicability in many domains that suffer
from a dearth of annotated resources</p>
</blockquote>

<p><strong>Solution</strong></p>

<blockquote>
  <p>In this paper, we explore a semi-supervised approach for language understanding tasks using a combination of unsupervised pre-training and supervised fine-tuning. Our goal is to learn a universal representation that transfers with little adaptation to a wide range of tasks. We assume access to a large corpus of unlabeled text and several datasets with manually annotated training examples (target tasks). Our setup does not require these target tasks to be in the same domain as the unlabeled corpus. We employ a two-stage training procedure. First, we use a Language Modeling objective on the unlabeled data to learn the initial parameters of a neural network model. Subsequently, we adapt these parameters to a target task using the corresponding supervised objective.</p>
</blockquote>

<p>This was the beginning of the era we live in now. As funny as it may sound, I do not have a lot to add in this section. Because we have already talked about the majority things, the architecture and important concepts like attention in our <a href="https://goyalpramod.github.io/blogs/Transformers_laid_out/">transformers blog</a> and the training method in our <a href="#ulmfit">ULMFit</a> section.</p>

<p>So let’s use this section, to talk about the terms we popularly associate with LLMs like
Top k, Top p, Temperature, Sampling etc.</p>

<p>Also, since I wrote the transformers blog I have gained a deeper and better understanding of self-attention and masked attention, so we will spend a bit of time on that too.</p>

<p>I will be skipping most of the thing we have already talked about, but if you still wish to get a quick recap of everything you have learned so far about Language models. Consider reading this fabulous <a href="https://jalammar.github.io/illustrated-gpt2/">blog</a> by <a href="https://x.com/JayAlammar">Jay Alammar</a>. (you may say that this blog is on gpt-2 and not gpt-1. Well truth be told there is not much difference in gpt, gpt-2 and gpt-3 beside their obvious scale.)</p>

<h5 id="gpt-asap">GPT ASAP</h5>

<p>Generative Pre-trained Transformer is a decoder only auto-regressive model which was first pre-trained on a humongous amount of data using a Language Modeling method then fine-tuned on a much smaller supervised dataset to help solve specific tasks</p>

<p>That is a lot of ML jargon way of saying that, the authors took the transformer, got rid of the encoder, put up a lot of layers of decoder, trained it on lots of data, then fine tuned it for specific use cases.</p>

<p>Langaguage modeling is pretty straight forward, given a context, predict the next word. The amazing part is that it is unsupervised. We can create a dataset using any sentence/document/text data. We break it down to it’s individual tokens, then we create a dataset by shifting the tokens by 1.</p>

<p><img src="/assets/blog_assets/evolution_of_llms/43.webp" alt="Image of SentencePiece abstract" /></p>

<p><strong>Model Specification</strong></p>

<p>I took this excerpt directly from the paper as I cannot add anything else to it.</p>

<blockquote>
  <p>Model specifications Our model largely follows the original transformer work [62]. We trained a
12-layer decoder-only transformer with masked self-attention heads (768 dimensional states and 12
attention heads). For the position-wise feed-forward networks, we used 3072 dimensional inner states.
We used the Adam optimization scheme [27] with a max learning rate of 2.5e-4. The learning rate
was increased linearly from zero over the first 2000 updates and annealed to 0 using a cosine schedule.
We train for 100 epochs on minibatches of 64 randomly sampled, contiguous sequences of 512 tokens.
Since layernorm [2] is used extensively throughout the model, a simple weight initialization of
N(0, 0.02) was sufficient. We used a bytepair encoding (BPE) vocabulary with 40,000 merges [53]
and residual, embedding, and attention dropouts with a rate of 0.1 for regularization. We also
employed a modified version of L2 regularization proposed in [37], with w = 0.01 on all non bias or
gain weights. For the activation function, we used the Gaussian Error Linear Unit (GELU) [18]. We
used learned position embeddings instead of the sinusoidal version proposed in the original work.
We use the ftfy library2
to clean the raw text in BooksCorpus, standardize some punctuation and
whitespace, and use the spaCy tokenizer.</p>
</blockquote>

<h5 id="why-does-attention-work">Why does attention work</h5>

<p>I am almost giddy writing this section because I find it astounding that this question never came to my mind. Why does attention even work, we all know it works. But why?</p>

<p>Even before I begin I NEED to mention all these amazing blogs that helped me build an intuition behind the idea</p>

<ul>
  <li><a href="https://www.jeremyjordan.me/attention/">Understanding the attention mechanism in sequence models</a> by <a href="https://x.com/jeremyjordan">Jeremy Jordan</a></li>
  <li><a href="https://arxiv.org/pdf/1409.0473v7">Additive attention</a> the paper that introduced the idea of attention</li>
  <li><a href="https://e2eml.school/transformers.html">Transformers from Scratch</a> by <a href="https://www.brandonrohrer.com/blog.html">Brandon Rohrer</a> this is attention explained each matrix multiplication at a time</li>
  <li><a href="https://peterbloem.nl/blog/transformers">Transformers from scratch</a> by <a href="https://peterbloem.nl/">Peter Bloem</a> a different approach but equally amazing</li>
  <li><a href="https://eugeneyan.com/writing/attention/">Some Intuition on Attention and the Transformer</a> by <a href="https://eugeneyan.com/">Eugene Yan</a> if you are short on time, just read this.</li>
</ul>

<p><img src="/assets/blog_assets/evolution_of_llms/48.webp" alt="Image of SentencePiece abstract" /></p>

<p>Let’s forget about Q,K,V for a moment and just focus on a single matrix X. Which contains different tokens (these tokens can be anything. Words, image patches, audio segments, anything that can be embedded and represented by numbers)</p>

<p>This matrix consists of vectors X1, X2, X2…. Xn That make up the matrix X</p>

<p>(X vectors are embedded so they occupy a specific place in space)</p>

<p><img src="/assets/blog_assets/evolution_of_llms/49.webp" alt="Image of SentencePiece abstract" /></p>

<p>Now assume you want to see how close X1 is relative to every other word. What do you do? Take dot product, which gives us similarity scores between vectors - the higher the dot product, the more similar (or ‘closer’) the vectors are in the semantic space.</p>

<p>Now let’s say you want to see, how different vectors relate to X1, I.e How far is X2 from X1 (notice how we went from how far X1 is from X2, X3 and so on. To how far X2 is from X1).</p>

<p>Both of the above dot products will give us the same result. I.E how far each vector is relative to each other. This is ATTENTION. Distance between different words. We can think of like this.</p>

<p>In a sentence, “Steve is an amazing person, who loves Machine Learning”. After applying attention, a lot of words will basically get filtered out because we are not paying attention to them anymore</p>

<p>So the sentence becomes</p>

<p>“Steve … amazing …. Machine learning”</p>

<p><img src="/assets/blog_assets/evolution_of_llms/50.webp" alt="Image of SentencePiece abstract" /></p>

<p>Now we need to apply these masks to something right, Hence we need to know the original sentence as well</p>

<p>So we multiply this mask with X. This gives us an attention mask over the original matrix.</p>

<p>We just calculated Self-attention. With 1 sentence. Without using the terms Q,K,V.</p>

<p><img src="/assets/blog_assets/evolution_of_llms/51.webp" alt="Image of SentencePiece abstract" /></p>

<p>Now the question arises of W_q, W_k, W_v.</p>

<p>These are linear transformations, they are present to change the position of our X in the vector space.</p>

<p>But dot product gives us the similarity between different vectors in space, by changing representations. We are getting the similarity score from different angles.</p>

<p>This proves that attention works, because dot product is just giving us the similarity in space. What about “but which layer gives us what”</p>

<p>Think of it this way, lets assume our W matrices are all identity matrix. (so no transformation), Now when we do our attention scoring. If any token is more close to any other token it will get a very high score. This essentially gives us the attention of that pair. This will be represented in the first layer, the second layer will then calculate the attention of pairs (much like how CNNs work and are visualized. You start with small lines, then move on to more complex geometric figures)</p>

<p>Now by changing our W matrices in all different ways, we get different representation. And make pairs of different tokens stronger.</p>

<p><strong>Masked Attention</strong></p>

<p>Masked Attention is much like attention itself, but here we get rid of the bi-directional nature.
The current token can only look at itself and the tokens before it.</p>

<p><img src="/assets/blog_assets/evolution_of_llms/38.webp" alt="Image of Word2Vec explanation" /></p>

<p>In a matrix representation is looks something like this, It is very vital. Because our inference is auto-regressive so if during training we can look at the future tokens we run into data leakage, the model would essentially be “cheating” by seeing the answer during training, making it unable to generate text properly during inference when it only has access to previous tokens.</p>

<p><img src="/assets/blog_assets/evolution_of_llms/masked_attention.webp" alt="Image of SentencePiece abstract" />
<em>Image taken from this beautiful <a href="https://poloclub.github.io/transformer-explainer/">visualizer</a></em></p>

<p>The image shows how masked attention works in practice. First we calculate the attention score by multiplying Q and K, then a mask is applied to it. Each token (represented by the colored circles) can only attend to itself and the tokens that came before it. The connections show which tokens each position can “see”, notice how the connections always flow backwards or stay at the same position, never forward. This creates the triangular pattern you see in attention matrices, ensuring the model learns to predict based only on past context.</p>

<h5 id="llm-glossary">LLM Glossary</h5>

<p>What this is -&gt; A mathematical foundation for terms commonly used during LLM inference <br />
What this is not -&gt; A guide to deciding the best values for your use case</p>

<p><strong>Temperature</strong></p>

\[P(token_i) = \frac{e^{logit_i/T}}{\sum_j e^{logit_j/T}}\]

<p>where T is temperature.</p>

<p>Temperature controls the randomness of token selection by scaling the logits before applying softmax. When T=1, we get the original distribution. As T approaches 0, the model becomes deterministic (always picks the highest probability token). Higher T values (&gt;1) flatten the distribution, making the model more creative but potentially less coherent.</p>

<p>For example if Temperatur is 0, we will always get the same response from the model no matter how many times we run it, for Temperature = 1 we will get varied results.</p>

<p><strong>Top K</strong></p>

\[P_{new}(token_i) = \frac{p_i}{\sum_{j=1}^k p_j}\]

<p>for $i \leq k$, and 0 otherwise</p>

<p>Top-k sampling restricts token selection to only the k most probable tokens, setting all other probabilities to zero before renormalization. If the original probabilities are $[p_1, p_2, …, p_n]$ sorted in descending order, top-k keeps only $[p_1, p_2, …, p_k]$ and renormalizes the tokens using the above formula. This prevents the model from selecting very unlikely tokens while maintaining some randomness.</p>

<p>For example,if the model earlier had 10 most likely options to choose from, by setting Top k=3 we are liminting those options to only 3 most likely options.</p>

<p><strong>Top P (Nucleus Sampling)</strong></p>

\[\sum_{i=1}^k p_i \geq p\]

<p>Top-p sampling dynamically selects the smallest set of tokens whose cumulative probability exceeds threshold p. Given sorted probabilities $[p_1, p_2, …, p_n]$, we find the smallest k such that helps us clear the above criteria, then renormalize only these k tokens. Unlike top-k which uses a fixed number of tokens, top-p adapts to the confidence of the model - using fewer tokens when the model is confident and more when uncertain.</p>

<p>for example, if the model is fairly certain we will have the top token probabilities like [70%, 20%, 4%, 1% …] and if we set top p threshold as 93% we will make the model choose from the top 3 tokens only
But if the model is uncertain and we get probabilities like [30%,29%,7%,5%,3%…] the model will have many tokens to choose from.</p>

<p><strong>Sampling Methods</strong></p>

<p>Sampling methods introduce controlled randomness by selecting tokens according to their probability distribution, often producing more diverse and human-like text at the cost of potential coherence.</p>

\[token = \arg\max_i P(token_i)\]

<p>Greedy sampling always selects the token with highest probability. While deterministic and fast, this can lead to repetitive text and suboptimal sequences due to the exposure bias problem.</p>

\[score(sequence) = \frac{1}{|sequence|^\alpha} \sum_{i=1}^{|sequence|} \log P(token_i|context_i)\]

<p>Where α is a length penalty to prevent bias toward shorter sequences.</p>

<p>Beam search maintains multiple candidate sequences simultaneously and explores the most promising ones. Instead of greedily picking the best token at each step, it keeps track of the top k sequences (beams) and expands each one.</p>

<p>Random sampling selects tokens according to their probability distribution without any constraints. While this can produce creative outputs, it often leads to incoherent text as low-probability tokens get selected too frequently.</p>

<p>Consider checking out the <a href="https://huggingface.co/docs/transformers/main/en/generation_strategies">transformers documentation</a> by HF.</p>

<p><strong>Repetition Penalty</strong></p>

<p>Repetition penalty reduces the probability of tokens that have already appeared in the generated sequence. The modified logit for a token that appeared n times is:</p>

\[logit_{new} = \frac{logit_{original}}{penalty^n}\]

<p>if penalty &gt; 1, which decreases the likelihood of selecting repeated tokens. This helps prevent the model from getting stuck in repetitive loops while maintaining natural language flow.</p>

<p><strong>Frequency Penalty</strong></p>

<p>Similar to repetition penalty but applies a linear reduction based on token frequency</p>

\[logit_{new} = logit_{original} - \alpha \times count(token)\]

<p>where α is the penalty coefficient and count(token) is how many times the token appeared. This provides more nuanced control over repetition compared to the exponential scaling of repetition penalty.</p>

<p><strong>Presence Penalty</strong></p>

<p>A binary version of frequency penalty that applies a fixed penalty regardless of how many times a token appeared</p>

\[logit_{new} = logit_{original} - \alpha\]

<p>if the token was seen before, unchanged otherwise. This encourages the model to use new vocabulary without heavily penalizing natural repetitions that occur in coherent text.</p>

<p>That wraps up our section on GPT, don’t worry as we move forward to the future years we will talk about how LLMs work with RL, How different architectures affect different things, talk about optimizers and so much more. For now I feel like these are the most we should keep for this year.</p>

<h3 id="sentencepiece">Sentencepiece</h3>

<p><img src="/assets/blog_assets/evolution_of_llms/sentencepiece_abstract.webp" alt="Image of SentencePiece abstract" /></p>

<blockquote>
  <p>Link to the paper: <a href="https://arxiv.org/abs/1808.06226">SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing</a></p>
</blockquote>

<details>
<summary>Quick Summary</summary>
<div>

    <p>This paper introduces SentencePiece, an open-source subword tokenizer and detokenizer designed specifically for neural text processing, including Neural Machine Translation (NMT). The key innovation of SentencePiece is that it can train subword models directly from raw sentences without requiring pre-tokenization, enabling truly end-to-end and language-independent text processing.</p>

    <p>The authors highlight several important features:</p>

    <ol>
      <li>It implements two subword segmentation algorithms: byte-pair encoding (BPE) and unigram language model</li>
      <li>It provides lossless tokenization that preserves all information needed to reconstruct the original text</li>
      <li>The model is fully self-contained, ensuring reproducibility across implementations</li>
      <li>It offers efficient training and segmentation algorithms</li>
      <li>It includes library APIs for on-the-fly processing</li>
    </ol>

    <p>They validate their approach through experiments on English-Japanese translation, showing comparable accuracy to systems that use pre-tokenization, while being significantly faster for non-segmented languages like Japanese.</p>

  </div>
</details>
<p><br /></p>

<p>Tokenization might seem like a boring preprocessing step, but it’s actually one of the most crucial components of any language model. Think of it as teaching a computer how to “read” - before a model can understand Shakespeare or debug your code, it needs to know how to break text into meaningful chunks. SentencePiece was a game-changer because it made this process truly language-agnostic.</p>

<p><strong>Problem</strong></p>

<blockquote>
  <p>Tough to make NMT language independent</p>
</blockquote>

<p>Before SentencePiece, building multilingual NLP systems was a nightmare. Each language needed its own custom preprocessing pipeline - Japanese needed special word segmentation, Chinese required different tokenization rules, and languages like Thai had their own complexities. This meant you couldn’t easily train one model across multiple languages, severely limiting the scope and efficiency of NLP systems.</p>

<p><strong>Solution</strong></p>

<blockquote>
  <p>SentencePiece comprises four main components:
Normalizer, Trainer, Encoder, and Decoder.
Normalizer is a module to normalize semantically equivalent Unicode characters into canonical
forms. Trainer trains the subword segmentation
model from the normalized corpus. We specify a
type of subword model as the parameter of Trainer.
Encoder internally executes Normalizer to normalize the input text and tokenizes it into a subword sequence with the subword model trained by
Trainer. Decoder converts the subword sequence
into the normalized tex</p>
</blockquote>

<p>The following articles helped me write this section</p>

<ul>
  <li><a href="https://huggingface.co/docs/transformers/en/tokenizer_summary">Transformers Documentation</a></li>
  <li><a href="https://towardsdatascience.com/sentencepiece-tokenizer-demystified-d0a3aac19b15/">Sentencepiece tokenizer demystified</a></li>
</ul>

<h5 id="what-are-tokenizers">What are tokenizers?</h5>

<p>As we have discussed earlier. Machines do not understand words, They understand numbers. Tokens are basically words represented as numbers that Language Models use to communicate. (You will see why this is an oversimplification in a minute?)</p>

<p><strong>Why call them tokens if they are just words?</strong></p>

<p>Taking back what I said, they aren’t exactly words. Let us understand why.</p>

<p><strong>Word Level Tokenization</strong></p>

<p>Let’s start with a simple sentence and tokenize (converting words in a document to token) it.</p>

<p><strong>Example:</strong> “I love machine learning” <br />
<strong>Tokens:</strong> [“I”, “love”, “machine”, “learning”] <br />
<strong>Token IDs:</strong> [1, 234, 5678, 9012]</p>

<p>Now imagine instead of a sentence, it was a page, or a whole book. It will contain thousands if not hundreds of thousands of unique words. There are languages which have more than a <a href="https://en.wikipedia.org/wiki/List_of_dictionaries_by_number_of_words">million unique words</a>. It will be unfeasible to tokenize each word.</p>

<p>Also what if we run into a word we have never seen during training. That will break our model, to overcome this, we can use something called Character Level Tokenization.</p>

<p><strong>Character Level Tokenization</strong></p>

<p>Let’s again start with the same sentence</p>

<p><strong>Example:</strong> “I love machine learning” <br />
<strong>Tokens:</strong> [“I”, “ “, “l”, “o”, “v”, “e”, “ “, “m”, “a”, “c”, “h”, “i”, “n”, “e”, “ “, “l”, “e”, “a”, “r”, “n”, “i”, “n”, “g”] <br />
<strong>Token IDs:</strong> [1, 2, 12, 15, 22, 5, 2, 13, 1, 3, 8, 9, 14, 5, 2, 12, 5, 1, 18, 14, 9, 14, 7]</p>

<p>This fixes our problem of a huge vocabulary, but we run into another issue. Characters by themselves do not hold any meaning. This removes any semantic relation.</p>

<p><img src="/assets/blog_assets/evolution_of_llms/e.jpeg" alt="Image of SentencePiece abstract" /><a href="https://knowyourmeme.com/memes/lord-marquaad-e">(Unless it’s E ofc)</a></p>

<p>There have been innovations made to fix these problems, let’s look into them one by one.</p>

<h5 id="subword-tokenization">Subword Tokenization</h5>

<p>The idea is quite simple, In our previous word level tokenization. <code class="language-plaintext highlighter-rouge">Fun</code>, <code class="language-plaintext highlighter-rouge">Funny</code>, <code class="language-plaintext highlighter-rouge">Funniest</code> ,and other variations of the word <code class="language-plaintext highlighter-rouge">Fun</code> will be treated as different tokens. So in sub-word tokenization. We break down the words, something like <code class="language-plaintext highlighter-rouge">Fun</code> + <code class="language-plaintext highlighter-rouge">niest</code>.</p>

<p>The reason being in english we add a lot of different pre-fixes (Like <code class="language-plaintext highlighter-rouge">un</code> to <code class="language-plaintext highlighter-rouge">funny</code> that makes it <code class="language-plaintext highlighter-rouge">unfunny</code>) and suffixes (Like <code class="language-plaintext highlighter-rouge">niest</code> to <code class="language-plaintext highlighter-rouge">Fun</code> to get <code class="language-plaintext highlighter-rouge">Funniest</code>). The techniques of forming these subwords varies from different techniques to different techniques. Let’s explore them</p>

<p><strong>Byte-pair encoding</strong></p>

<p>This <a href="https://huggingface.co/docs/transformers/en/tokenizer_summary">blog</a> helped me immensely with this section.</p>

<p>Byte-pair encoding (BPE) was introduced in <a href="https://arxiv.org/abs/1508.07909">Neural Machine Translation of Rare Words with Subword Units (Sennrich et al., 2015)</a>. This is the encoding method the famous tiktoken tokenizer of OpenAI uses.</p>

<p>It essentially follows two steps, the pretokenization then merging. Let’s understand each.</p>

<p>In the pretokenization step, we determine the set of unique words in the training data along with their frequency. Using this unique set, BPE breaks the words down into it’s individual characters, which are then merged to form the final vocabulary. Let’s understand it with an example.</p>

<p>Suppose after the pre-tokenization step, we have the following words with their frequencies that we got from the training data</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">(</span><span class="s">"hug"</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="p">(</span><span class="s">"pug"</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="p">(</span><span class="s">"pun"</span><span class="p">,</span> <span class="mi">12</span><span class="p">),</span> <span class="p">(</span><span class="s">"bun"</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="p">(</span><span class="s">"hugs"</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
</code></pre></div></div>

<p>Consequently, the base vocabulary is <code class="language-plaintext highlighter-rouge">["b", "g", "h", "n", "p", "s", "u"]</code>. Splitting all words into characters of the base vocabulary, we get:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">(</span><span class="s">"h"</span> <span class="s">"u"</span> <span class="s">"g"</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="p">(</span><span class="s">"p"</span> <span class="s">"u"</span> <span class="s">"g"</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="p">(</span><span class="s">"p"</span> <span class="s">"u"</span> <span class="s">"n"</span><span class="p">,</span> <span class="mi">12</span><span class="p">),</span> <span class="p">(</span><span class="s">"b"</span> <span class="s">"u"</span> <span class="s">"n"</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="p">(</span><span class="s">"h"</span> <span class="s">"u"</span> <span class="s">"g"</span> <span class="s">"s"</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
</code></pre></div></div>

<p>BPE then counts the character pair with the high frequencies and then pairs them up. Like here it will first pair up <code class="language-plaintext highlighter-rouge">u</code> and <code class="language-plaintext highlighter-rouge">g</code> as they occur the most together (10 times in hug, 5 times in pug and 5 more times in hugs. Totalling 20 times). This pair <code class="language-plaintext highlighter-rouge">ug</code> is then added to the vocabulary.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">(</span><span class="s">"h"</span> <span class="s">"ug"</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="p">(</span><span class="s">"p"</span> <span class="s">"ug"</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="p">(</span><span class="s">"p"</span> <span class="s">"u"</span> <span class="s">"n"</span><span class="p">,</span> <span class="mi">12</span><span class="p">),</span> <span class="p">(</span><span class="s">"b"</span> <span class="s">"u"</span> <span class="s">"n"</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="p">(</span><span class="s">"h"</span> <span class="s">"ug"</span> <span class="s">"s"</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
</code></pre></div></div>

<p>Then we identify the next most frequent pair, that will be <code class="language-plaintext highlighter-rouge">u</code> and <code class="language-plaintext highlighter-rouge">n</code> that occur 16 times together. Then the next most frequent pair interestingly is <code class="language-plaintext highlighter-rouge">h</code> with <code class="language-plaintext highlighter-rouge">ug</code> which occur together 15 times. (This teaches an important lesson, we do not only consider characters or symbols while pairing them up. But the frequency pairs that exist in the vocabulary).</p>

<p>Now our entire vocabulary looks something like this <code class="language-plaintext highlighter-rouge">["b", "g", "h", "n", "p", "s", "u", "ug", "un", "hug"]</code> and our set of unique words</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">(</span><span class="s">"hug"</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="p">(</span><span class="s">"p"</span> <span class="s">"ug"</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="p">(</span><span class="s">"p"</span> <span class="s">"un"</span><span class="p">,</span> <span class="mi">12</span><span class="p">),</span> <span class="p">(</span><span class="s">"b"</span> <span class="s">"un"</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="p">(</span><span class="s">"hug"</span> <span class="s">"s"</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
</code></pre></div></div>

<p>Assuming we stop our merging at this point. Our new vocabulary will be used to tokenize new words (Unless they were not present in our training data). For example, <code class="language-plaintext highlighter-rouge">"bug"</code> will be tokenized to <code class="language-plaintext highlighter-rouge">["b", "ug"]</code> but “mug” would be tokenized as <code class="language-plaintext highlighter-rouge">["&lt;unk&gt;", "ug"]</code> since the symbol <code class="language-plaintext highlighter-rouge">"m"</code> is not in the base vocabulary.</p>

<p>In general, single letters such as “m” are not replaced by the “<unk>" symbol because the training data usually includes at least one occurrence of each letter, but it is likely to happen for very special characters like emojis.</unk></p>

<p>Using BPE the vocab size is the base vocabulary size + the number of merges, which is a hyperparameter we can choose. For instance GPT has a vocabulary size of 40,478 since they have 478 base characters and chose to stop training after 40,000 merges.</p>

<p><strong>Wordpiece</strong></p>

<p>Wordpiece is very similar to our BPE algorithm. It starts with the same idea of pre-tokenization followed by merging. It differs in the way it merges - instead of following the highest frequency, it merges based on a scoring system that considers how valuable each merge is.</p>

<p>The key difference is in how pairs are selected for merging:</p>

<p>WordPiece doesn’t just pick the most frequent pair like BPE does. Instead, it calculates a score for each pair by dividing how often the pair appears together by how often each part appears separately. This means it prefers to merge pairs where the individual parts are rare on their own, but common when together. This approach ensures that merges actually improve the tokenization rather than just combining frequent but unrelated pieces.</p>

\[score = \frac{freq\_of\_pair}{freq\_of\_first\_element \times freq\_of\_second\_element}\]

<p>HF also has a <a href="https://huggingface.co/learn/llm-course/en/chapter6/6?fw=pt">short course</a> on LLMs where they talk about wordpiece pretty well.</p>

<p>Taking an excerpt from the said course explains it well</p>

<blockquote>
  <p>By dividing the frequency of the pair by the product of the frequencies of each of its parts, the algorithm prioritizes the merging of pairs where the individual parts are less frequent in the vocabulary. For instance, it won’t necessarily merge (“un”, “##able”) even if that pair occurs very frequently in the vocabulary, because the two pairs “un” and “##able” will likely each appear in a lot of other words and have a high frequency. In contrast, a pair like (“hu”, “##gging”) will probably be merged faster (assuming the word “hugging” appears often in the vocabulary) since “hu” and “##gging” are likely to be less frequent individually.</p>
</blockquote>

<p>If you are interested, I will recommend reading this <a href="https://research.google/blog/a-fast-wordpiece-tokenization-system/">blog</a> by google which talks about Wordpiece more in depth.</p>

<p><strong>Unigram</strong></p>

<p>The same <a href="https://huggingface.co/learn/llm-course/en/chapter6/7">course</a> talks about Unigram as well.</p>

<p>Unigram is quite different from our previously discussed tokenization methods like BPE and WordPiece. Instead of expanding the vocabulary, it starts with a big vocabulary and gradually trims down irrelevant tokens.</p>

<p>At each step of training, the Unigram model computes the loss over the entire corpus. Then, for each token it calculates how much the loss will increase if the token is removed, and removes the ones which increase the loss the least.</p>

<p>Note that we never remove the base characters, to make sure any word can be tokenized.</p>

<p>Using the same example as above we start with a sample corpus</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">(</span><span class="s">"hug"</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="p">(</span><span class="s">"pug"</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="p">(</span><span class="s">"pun"</span><span class="p">,</span> <span class="mi">12</span><span class="p">),</span> <span class="p">(</span><span class="s">"bun"</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="p">(</span><span class="s">"hugs"</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
</code></pre></div></div>

<p>We calculate the frequencies of all the subwords as below</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">(</span><span class="s">"h"</span><span class="p">,</span> <span class="mi">15</span><span class="p">)</span> <span class="p">(</span><span class="s">"u"</span><span class="p">,</span> <span class="mi">36</span><span class="p">)</span> <span class="p">(</span><span class="s">"g"</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span> <span class="p">(</span><span class="s">"hu"</span><span class="p">,</span> <span class="mi">15</span><span class="p">)</span> <span class="p">(</span><span class="s">"ug"</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span> <span class="p">(</span><span class="s">"p"</span><span class="p">,</span> <span class="mi">17</span><span class="p">)</span> <span class="p">(</span><span class="s">"pu"</span><span class="p">,</span> <span class="mi">17</span><span class="p">)</span> <span class="p">(</span><span class="s">"n"</span><span class="p">,</span> <span class="mi">16</span><span class="p">)</span>
<span class="p">(</span><span class="s">"un"</span><span class="p">,</span> <span class="mi">16</span><span class="p">)</span> <span class="p">(</span><span class="s">"b"</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span> <span class="p">(</span><span class="s">"bu"</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span> <span class="p">(</span><span class="s">"s"</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span> <span class="p">(</span><span class="s">"hug"</span><span class="p">,</span> <span class="mi">15</span><span class="p">)</span> <span class="p">(</span><span class="s">"gs"</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span> <span class="p">(</span><span class="s">"ugs"</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
</code></pre></div></div>

<p>To tokenize a given word, we look at all possible subwords and calculate their probabilities. All the tokens are considered independent of each other so we can just multiply all sub-tokens as below</p>

<p>For the word <code class="language-plaintext highlighter-rouge">hug</code>, by tokenizing it as <code class="language-plaintext highlighter-rouge">h</code>,<code class="language-plaintext highlighter-rouge">u</code>, and <code class="language-plaintext highlighter-rouge">g</code></p>

<p>\(P(h) \times P(u) \times P(g)\)
\(\frac{15}{210} \times \frac{36}{210} \times \frac{20}{210} = 0.000389\)</p>

<p>It is also tokenized as <code class="language-plaintext highlighter-rouge">hu</code>, <code class="language-plaintext highlighter-rouge">g</code> and <code class="language-plaintext highlighter-rouge">h</code>, <code class="language-plaintext highlighter-rouge">ug</code>, and <code class="language-plaintext highlighter-rouge">hug</code>. The scores are</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">[</span><span class="s">"h"</span><span class="p">,</span> <span class="s">"u"</span><span class="p">,</span> <span class="s">"g"</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.000389</span>
<span class="p">[</span><span class="s">"hu"</span><span class="p">,</span> <span class="s">"g"</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">15</span><span class="o">/</span><span class="mi">210</span><span class="p">)</span> <span class="err">×</span> <span class="p">(</span><span class="mi">20</span><span class="o">/</span><span class="mi">210</span><span class="p">)</span> <span class="o">=</span> <span class="mf">0.0095</span>
<span class="p">[</span><span class="s">"h"</span><span class="p">,</span> <span class="s">"ug"</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">15</span><span class="o">/</span><span class="mi">210</span><span class="p">)</span> <span class="err">×</span> <span class="p">(</span><span class="mi">20</span><span class="o">/</span><span class="mi">210</span><span class="p">)</span> <span class="o">=</span> <span class="mf">0.0095</span>
<span class="p">[</span><span class="s">"hug"</span><span class="p">]</span> <span class="o">=</span> <span class="mi">15</span><span class="o">/</span><span class="mi">210</span> <span class="o">=</span> <span class="mf">0.071</span>
</code></pre></div></div>

<p>Hence <code class="language-plaintext highlighter-rouge">hug</code> will be tokenized as <strong>[“hug”]</strong> as it has the highest probability.</p>

<p>In practice this iterative process is not used, but rather <a href="https://en.wikipedia.org/wiki/Viterbi_algorithm">Viterbi algorithm</a> is used to find the subword tokenization with the maximum probability.</p>

<p>Using this same method a score is calculated for all the words, as below</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="s">"hug"</span><span class="p">:</span> <span class="p">[</span><span class="s">"hug"</span><span class="p">]</span> <span class="p">(</span><span class="n">score</span> <span class="mf">0.071428</span><span class="p">)</span>
<span class="s">"pug"</span><span class="p">:</span> <span class="p">[</span><span class="s">"pu"</span><span class="p">,</span> <span class="s">"g"</span><span class="p">]</span> <span class="p">(</span><span class="n">score</span> <span class="mf">0.007710</span><span class="p">)</span>
<span class="s">"pun"</span><span class="p">:</span> <span class="p">[</span><span class="s">"pu"</span><span class="p">,</span> <span class="s">"n"</span><span class="p">]</span> <span class="p">(</span><span class="n">score</span> <span class="mf">0.006168</span><span class="p">)</span>
<span class="s">"bun"</span><span class="p">:</span> <span class="p">[</span><span class="s">"bu"</span><span class="p">,</span> <span class="s">"n"</span><span class="p">]</span> <span class="p">(</span><span class="n">score</span> <span class="mf">0.001451</span><span class="p">)</span>
<span class="s">"hugs"</span><span class="p">:</span> <span class="p">[</span><span class="s">"hug"</span><span class="p">,</span> <span class="s">"s"</span><span class="p">]</span> <span class="p">(</span><span class="n">score</span> <span class="mf">0.001701</span><span class="p">)</span>
</code></pre></div></div>

<p>We want to calculate the negative log likelihood of the whole corpus and that becomes</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">freq</span><span class="p">(</span><span class="s">"hug"</span><span class="p">)</span> <span class="err">×</span> <span class="p">(</span><span class="o">-</span><span class="n">log</span><span class="p">(</span><span class="n">P</span><span class="p">(</span><span class="s">"hug"</span><span class="p">)))</span> <span class="o">+</span> <span class="n">freq</span><span class="p">(</span><span class="s">"pug"</span><span class="p">)</span> <span class="err">×</span> <span class="p">(</span><span class="o">-</span><span class="n">log</span><span class="p">(</span><span class="n">P</span><span class="p">(</span><span class="s">"pug"</span><span class="p">)))</span> <span class="o">+</span> <span class="n">freq</span><span class="p">(</span><span class="s">"pun"</span><span class="p">)</span> <span class="err">×</span> <span class="p">(</span><span class="o">-</span><span class="n">log</span><span class="p">(</span><span class="n">P</span><span class="p">(</span><span class="s">"pun"</span><span class="p">)))</span> <span class="o">+</span> <span class="n">freq</span><span class="p">(</span><span class="s">"bun"</span><span class="p">)</span> <span class="err">×</span> <span class="p">(</span><span class="o">-</span><span class="n">log</span><span class="p">(</span><span class="n">P</span><span class="p">(</span><span class="s">"bun"</span><span class="p">)))</span> <span class="o">+</span> <span class="n">freq</span><span class="p">(</span><span class="s">"hugs"</span><span class="p">)</span> <span class="err">×</span> <span class="p">(</span><span class="o">-</span><span class="n">log</span><span class="p">(</span><span class="n">P</span><span class="p">(</span><span class="s">"hugs"</span><span class="p">)))</span>
<span class="mi">10</span> <span class="err">×</span> <span class="p">(</span><span class="o">-</span><span class="n">log</span><span class="p">(</span><span class="mf">0.071428</span><span class="p">))</span> <span class="o">+</span> <span class="mi">5</span> <span class="err">×</span> <span class="p">(</span><span class="o">-</span><span class="n">log</span><span class="p">(</span><span class="mf">0.007710</span><span class="p">))</span> <span class="o">+</span> <span class="mi">12</span> <span class="err">×</span> <span class="p">(</span><span class="o">-</span><span class="n">log</span><span class="p">(</span><span class="mf">0.006168</span><span class="p">))</span> <span class="o">+</span> <span class="mi">4</span> <span class="err">×</span> <span class="p">(</span><span class="o">-</span><span class="n">log</span><span class="p">(</span><span class="mf">0.001451</span><span class="p">))</span> <span class="o">+</span> <span class="mi">5</span> <span class="err">×</span> <span class="p">(</span><span class="o">-</span><span class="n">log</span><span class="p">(</span><span class="mf">0.001701</span><span class="p">))</span> <span class="o">=</span> <span class="mf">169.8</span>
</code></pre></div></div>

<p>Now each token is removed to see how that affects the overall loss as below</p>

<p>Removing token “pu”: Loss change = 0 (since “pug” can use [“p”, “ug”] with same score)
Removing token “hug”: Loss increases by 23.5 (forces less optimal tokenizations)</p>

<p>Tokens with the smallest loss increase are removed first until the desired vocabulary size is reached.</p>

<p><strong>SentencePiece</strong></p>

<p>This is a fabulous <a href="https://towardsdatascience.com/sentencepiece-tokenizer-demystified-d0a3aac19b15/">blog</a> on the topic, it explains everything along with the implementation code. Consider checking it out.</p>

<p>This sub-section will be the shortest even though this section is about sentencepiece, you know why? well because sentencepiece is not a tokenizer at all, it’s a pre-tokenization algorithm used in conjunction with unigram or BPE.</p>

<p>All tokenization methods talked about so far have the same problem, they assume that the input text uses spaces to separate words. However, not all languages use spaces to separate words. One possible solution is to use language specific pre-tokenizers, e.g. XLM uses a specific Chinese, Japanese, and Thai pre-tokenizer. To solve this problem more generally, SentencePiece treats the input as a raw input stream, thus including the space in the set of characters to use. It then uses the BPE or unigram algorithm to construct the appropriate vocabulary.</p>

<p>Decoding with SentencePiece is very easy since all tokens can just be concatenated and “▁” is replaced by a space.</p>

<p>All transformers models in the library that use SentencePiece use it in combination with unigram. Examples of models using SentencePiece are ALBERT, XLNet, LLama2, etc.</p>

<p>Well that is all there is to know about SentencePiece really, if you want to know how it is implemented you can check out the blog I mentioned above.</p>

<h3 id="bert">BERT</h3>

<p><img src="/assets/blog_assets/evolution_of_llms/BERT_abstract.webp" alt="Image of BERT" /></p>

<blockquote>
  <p>Link to paper: <a href="https://arxiv.org/abs/1810.04805">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a></p>
</blockquote>

<details>
<summary>Quick Summary</summary>
<div>

    <p>This paper introduces BERT (Bidirectional Encoder Representations from Transformers), a groundbreaking language representation model that significantly advanced the state of natural language processing in 2018. The key innovation of BERT is its ability to pre-train deep bidirectional representations from unlabeled text, unlike previous models that were limited to unidirectional contexts (either left-to-right or right-to-left).</p>

    <p>BERT employs two novel pre-training tasks:</p>

    <ol>
      <li><strong>Masked Language Model (MLM)</strong>: Randomly masks some percentage of input tokens and predicts those masked tokens</li>
      <li><strong>Next Sentence Prediction (NSP)</strong>: Predicts whether two sentences follow each other in original text</li>
    </ol>

    <p>These pre-training objectives allow BERT to create context-aware representations that capture information from both left and right contexts. After pre-training on large text corpora (BookCorpus and Wikipedia), BERT can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of NLP tasks without task-specific architecture modifications.</p>

    <p>The paper demonstrated significant improvements over previous methods on eleven NLP tasks, including the GLUE benchmark, SQuAD, and SWAG datasets.</p>

  </div>
</details>
<p><br /></p>

<p>These two blogs helped me immensely with this section:</p>

<ul>
  <li><a href="https://jalammar.github.io/illustrated-bert/">The illustrated BERT</a></li>
  <li><a href="https://huggingface.co/blog/bert-101">BERT 101</a></li>
</ul>

<p>This paper wasn’t trying to find a problem then solve it per se. It is more of an innovation, taking an excerpt from the paper.</p>

<blockquote>
  <p>BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference</p>
</blockquote>

<p><strong>Problem</strong></p>

<p>Previous language models like GPT-1 and ELMo had a fundamental limitation: they could only look in one direction. GPT-1 used masked attention (only seeing previous tokens), while ELMo concatenated separate left-to-right and right-to-left models. But human language understanding is inherently bidirectional - to understand “The animal didn’t cross the street because it was too [tired/wide],” you need context from both sides to know whether “it” refers to the animal or the street.</p>

<p><strong>Solution</strong></p>

<p>BERT solved this by using the encoder portion of the Transformer architecture, which allows true bidirectional attention. Instead of predicting the next word (like GPT), BERT learns by predicting masked words using context from both directions simultaneously.</p>

<p>All the papers I have mentioned in this blog are great, but the BERT paper is particularly awesome. It stands out even today, and the sheer amount of innovations from one paper is astounding. I implore you to check it out.</p>

<p>Let’s first answer the question <strong>what is BERT?</strong></p>

<p>BERT stands for Bi-directional Encoder Representation from Transformers. That’s a mounthful, the name aside. It is quite simple, Remember our Transformers, Well this is essentially that but only the encoder (Quite the opposite of GPT). Bi-directional means that it looks both ways, forward and backward. Remember in GPT we used masked self-attention which only got representation for current and previous token, well the encoder uses vanilla self-attention which gets representation from both sides. The transformer part is pretty self explanatory.</p>

<p><strong>What is it used for?</strong></p>

<p>This is the most amazing thing about BERT, it can be used for a plethora of NLP tasks like question answering, Parts of speech taging, Sentiment analysis, Classification etc. This is due to the way it is trained.</p>

<p><strong>How is it trained?</strong></p>

<p>Much the way we have discussed before, it has a pre-training phase in which we feed it lots and lots of data for it to learn a representation. Then fine-tune it for the particular task we would like it for.</p>

<p><img src="/assets/blog_assets/evolution_of_llms/39.webp" alt="Image of BERT" />
<em>Image taken from the paper</em></p>

<p>There are a lot of new terms from the above image like CLS, Masked LM, NSP etc that may not make sense at first. But they are quite simple once we understand them. Let us begin</p>

<p><strong>Masked Language Modeling</strong></p>

<p>This is one of the methods of pre-training BERT. We take the input sentence and randomly mask out 15% of the tokens, then using the output of the position we try to predict what could have been the possible text.</p>

<p>In practice, 80% of the times the token is masked, 10% of the times it is replaced with a random token and other 10% of the time it’s left unchanged.</p>

<p>Let’s see this in action:</p>

<ul>
  <li>Original: “The cat sat on the mat”</li>
  <li>Masked: “The cat [MASK] on the mat”</li>
  <li>BERT sees both “The cat” and “on the mat” to predict “sat”</li>
</ul>

<p>This bidirectional context is why BERT became so powerful for understanding tasks.</p>

<p><img src="/assets/blog_assets/evolution_of_llms/41.webp" alt="Image of BERT" /></p>

<p><strong>Next Sentence Prediction</strong></p>

<p>Next Sentence Prediction is exactly what it sounds like, but instead of predicting the next sentence. We predict if the given next sentence will come after the first sentence.</p>

<p>The special token CLS learns representation from the both the sentences and the output from it’s position is used to predict whether the sentence comes next or not. In practice, the CLS token is fine tuned for classification, sentiment analysis etc.
SEP is a special token that acts as a seperator between both the sentences.</p>

<p>In reality, both NSP and MLM are done simultaneously.</p>

<p><img src="/assets/blog_assets/evolution_of_llms/42.webp" alt="Image of BERT" /></p>

<p><strong>Fine-Tuning</strong></p>

<p>The BERT paper is quite accessible, and has great visuals too. The below image is directly taken from it and shows that to fine tune we just use a small set of training data with labels to fine-tune bert for a specific task.</p>

<p><img src="/assets/blog_assets/evolution_of_llms/40.webp" alt="Image of BERT" /></p>

<p>We can use bert to generate embeddings too!! Much like the way we did with ELMo. How? well I leave that upto you to explore.</p>

<p><strong>Why BERT was Revolutionary</strong></p>

<p>Before BERT, each NLP task needed its own specialized architecture. Question answering systems, sentiment classifiers, and named entity recognizers all looked completely different. BERT changed this by providing a universal representation that could be fine-tuned for any task with just a small additional layer.</p>

<p>And that concludes BERT too, now we have talked about the two big architectures of LLMs, moving forward we will mostly be talking about the innovations done in the architecture and the solutions found to increase the scale at which to train them.</p>

<!-- [Add info on Token type embedding https://stackoverflow.com/questions/57960995/how-are-the-tokenembeddings-in-bert-created] -->

<h2 id="2019-scaling-and-efficiency">2019: Scaling and Efficiency</h2>

<h3 id="gpt-2">GPT-2</h3>

<p><img src="/assets/blog_assets/evolution_of_llms/gpt2_abstract.webp" alt="Image of GPT-2" /></p>

<blockquote>
  <p>Link to paper: <a href="https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf">Language Models are Unsupervised Multitask Learners</a>,<a href="https://openai.com/index/better-language-models/">blog</a></p>
</blockquote>

<details>
<summary>Quick Summary</summary>
<div>

    <p>This 2019 paper by Radford et al. (OpenAI) introduces GPT-2, a large-scale language model that demonstrates impressive zero-shot learning capabilities across multiple NLP tasks. The key insight of this paper is that language models trained on sufficiently large and diverse datasets naturally acquire the ability to perform various language tasks without explicit supervision.</p>

    <p>Key contributions:</p>

    <ol>
      <li>Introduction of WebText - a high-quality web dataset created by scraping outbound links from Reddit with at least 3 karma</li>
      <li>Development of GPT-2, a Transformer-based language model with 1.5 billion parameters</li>
      <li>Demonstration that a single unsupervised language model can perform multiple NLP tasks without task-specific training</li>
      <li>Evidence that model performance scales in a log-linear fashion with model size</li>
    </ol>

    <p>The paper shows that GPT-2 achieves state-of-the-art results on 7 out of 8 tested Language Modeling datasets in a zero-shot setting. It also demonstrates promising zero-shot performance on tasks like reading comprehension, summarization, translation, and question answering without any task-specific fine-tuning.</p>

    <p>This work represents a significant step toward building more general NLP systems that can learn to perform tasks from naturally occurring demonstrations in text, rather than requiring task-specific datasets and architectures for each application.</p>

  </div>
</details>
<p><br /></p>

<p>This was paper was an improvement over the original GPT, and a huge scaling of it</p>

<p><img src="/assets/blog_assets/evolution_of_llms/54.webp" alt="Image of scale difference between gpt-1 and gpt 2" /></p>

<p>My first question was how did one even get so much data? The authours have underlined that well. So quoting the paper</p>

<blockquote>
  <p>Common Crawl. Trinh &amp; Le (2018)’s best results were achieved using a small subsample of Common Crawl which included only documents most similar to their target dataset, the Winograd Schema Challenge. While this is a pragmatic approach to improve performance on a specific task, we want to avoid making assumptions about the tasks to be performed ahead of time. Instead, we created a new web scrape which emphasizes document quality. To do this we only scraped web pages which have been curated/filtered by humans. Manually filtering a full web scrape would be exceptionally expensive so as a starting point, we scraped all outbound links from Reddit, a social media platform, which received at least 3 karma. This can be thought of as a heuristic indicator for whether other users found the link interesting, educational, or just funny. The resulting dataset, WebText, contains the text subset of these 45 million links. To extract the text from HTML responses we use a combination of the Dragnet (Peters &amp; Lecocq, 2013) and Newspaper1 content extractors. All results presented in this paper use a preliminary version of WebText which does not include links created after Dec 2017 and which after de-duplication and some heuristic based cleaning contains slightly over 8 million documents for a total of 40 GB of text. We removed all Wikipedia documents from WebText since it is a common data source for other datasets and could complicate analysis due to over…</p>
</blockquote>

<p>My second question was how do you run such large models economically?</p>

<p>(Obviously I have skipped how do you “train” such large models, as we will be talking about that in the next section)</p>

<p>And the answer came in the form of …</p>

<h5 id="kv-cache">KV Cache!!!</h5>

<p>These two blogs helped me immensely while writing this section, check’em out!!! (they have very nice animations)</p>

<ul>
  <li><a href="https://huggingface.co/blog/not-lain/kv-caching">HF Blog</a></li>
  <li><a href="https://medium.com/@joaolages/kv-caching-explained-276520203249">Medium Blog</a></li>
</ul>

<p><img src="/assets/blog_assets/evolution_of_llms/57.webp" alt="Image of quick inference in LLMs" /></p>

<p>Before we start understanding KV Cache, let us revisit our inference to see what happens.</p>

<p>Let’s say we ask the LLM a question. “Captial of India?” (we already know what happens inside an LLM so I have skipped a lot of the blocks except the essential ones)</p>

<p>Using that, a matrix is formed, for each token id we have embeddings, using which Q,K, and V matrices are made. That gives us our attention values, That is fed to a feed forward network which outputs a vector of logits vocab size, after passing it through softmax let’s say we take the token id with the maximum probability. This gives us a vector output, of token id. (Here I am showing the embeddings)</p>

<p>The embedding of this token is then appended to our original input X, and this operation is repeated till we run into the <EOS> token (End of sentence token)</EOS></p>

<p>Now if we do not cache anything, these values need to be calculated again and again whenever a new token is added as shown above</p>

<p>But we only need to get the attention value for the latest query, we can save the previously calculated queries. (If you are having a hard time thinking why? I will explain it in a while. Hint: Masked Causal attention)</p>

<p><img src="/assets/blog_assets/evolution_of_llms/55.webp" alt="Image of inference without KV cache" /></p>

<p>Let’s look closer at the attention calculation itself, If we go vector by vector. We can see that we need to compute $QK^t$ again and again, each time a new token is calculated.</p>

<p>But we do not need to do this for a decoder, as previous tokens never attend to the future tokens. We can just cache the attention of previous tokens and compute the attention score for the latest token!</p>

<p><img src="/assets/blog_assets/evolution_of_llms/56.webp" alt="Image of inference with KV cache" /></p>

<p>I had two interesting discoveries during this:</p>

<ul>
  <li>We only cache $K$ and $V$ not $Q$. Because there is no point storing it. To calculate the latest attention score we need the entire $K$ and $V$, but not $Q$. For nth token $Q$, we need the entire of K to calculate $QK^t$. Then we need to multiple the entire $V$ to get the latest attention score. (This is definitely tough to think about, pause take a moment and image it)</li>
  <li>When we append another vector, the shape of $K$ and $V$ also change. But we do not calculate the values of old $Q$ with these because of our masking (We do not need to see the future values), $Q_1$ has no point being multiplied with $K_4$ as that will be masked as infiti anyhoo (We have already talked about this).</li>
</ul>

<p>The second point also gives us another amazing point, KV caching is not possible in models like BERT (Think why?)</p>

<blockquote>
  <p>P.S. KV-Cache was not introduced in gpt-2 but became wildly popular after it. I am not able to find the source of the idea. Comment down below if you know about it!!!</p>
</blockquote>

<p>That pretty much covers the amazing things that the GPT-2 paper talked about,
Let’s move on to see how we can train such big models.</p>

<h3 id="roberta">RoBERTa</h3>

<p><img src="/assets/blog_assets/evolution_of_llms/roberta_abstract.webp" alt="Image of RoBERTa" /></p>

<blockquote>
  <p>Link to paper: <a href="https://arxiv.org/abs/1907.11692">RoBERTa: A Robustly Optimized BERT Pretraining Approach</a></p>
</blockquote>

<details>
<summary>Quick Summary</summary>
<div>

    <p>This 2019 paper by Liu et al. from Facebook AI presents RoBERTa (Robustly Optimized BERT Pretraining Approach), which demonstrates that BERT was significantly undertrained and can achieve state-of-the-art performance with careful optimization choices.</p>

    <p>Key contributions:</p>

    <ol>
      <li>
        <p>The paper identifies several critical design decisions that significantly improve BERT’s performance:</p>

        <ul>
          <li>Training the model longer with larger batches over more data</li>
          <li>Removing the Next Sentence Prediction (NSP) objective</li>
          <li>Training on longer sequences</li>
          <li>Dynamically changing the masking pattern applied to training data</li>
        </ul>
      </li>
      <li>
        <p>The researchers collect a larger dataset (including a new CC-NEWS corpus) to better control for training set size effects.</p>
      </li>
      <li>
        <p>Through extensive experimentation, they show that when properly optimized, BERT’s masked Language Modeling objective is competitive with newer approaches like XLNet.</p>
      </li>
      <li>
        <p>RoBERTa achieves state-of-the-art results on GLUE, RACE, and SQuAD benchmarks without multi-task fine-tuning for GLUE or additional data for SQuAD.</p>
      </li>
    </ol>

    <p>The authors emphasize that seemingly mundane training decisions (like batch size, training time, and dataset size) can have as much impact on final performance as architectural innovations. This raises important questions about the source of improvements in recent NLP models and highlights the need for careful replication studies.</p>

    <p>The paper is particularly notable for its thorough empirical analysis of training hyperparameters and careful ablation studies showing the contribution of each modification to overall performance.</p>

  </div>
</details>
<p><br /></p>

<p><strong>Problem</strong></p>

<p>The authors found that most of the models released post BERT were challenging to train, hard to compare and they were mostly working as a black box and it was difficult to tell which hyper-parameter were significant. RoBERTa was a replication study on BERT to better understand it. And they unearthed that BERT was significantly under trained.</p>

<p><strong>Solution</strong></p>

<ol>
  <li>Present a set of important BERT design and training strategies</li>
  <li>Introduction of a novel dataset (CCNEWS)</li>
</ol>

<p><strong>Dynamic Masking</strong></p>

<p>While training BERT a static masking strategy is employed. I.e the same words are masked throughout training, This can lead to the model memorizing these positions instead of generalizing and learning the relationship between words.</p>

<p>Dynamic masking was a method made to mitigate this issue by varying the masked positions in each epoch. This lead to greater generalization and better performance.</p>

<p><strong>Gradient Accumulation</strong></p>

<p>These two blogs helped me out with this part:</p>

<ul>
  <li><a href="https://blog.dailydoseofds.com/p/gradient-accumulation-increase-batch">Blog 1</a></li>
  <li><a href="https://aman.ai/primers/ai/grad-accum-checkpoint/">Blog 2</a></li>
</ul>

<p>It is well known that mini-batch descent produces good results, but sometimes even the mini batch is large in some cases. Like for CV on a 4k image etc. In those scenarios when we lack memory we use Gradient Accumulation to overcome that issue.</p>

<p>Instead of updating the model in every mini batch, we accumulate the gradient over the course of a few batches then update it.</p>

<p><img src="/assets/blog_assets/evolution_of_llms/70.webp" alt="Image of grad accumulation" /></p>

<p>This essentially acts like a bigger batch</p>

<p>A typical training loop looks like this</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">labels</span> <span class="ow">in</span> <span class="n">data_loader</span><span class="p">:</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>

    <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">output</span><span class="p">,</span><span class="n">labels</span><span class="p">)</span>

    <span class="n">loss</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">()</span>
</code></pre></div></div>

<p>But when we do gradient accumulation it looks something like this</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">acc_steps</span> <span class="o">=</span> <span class="mi">3</span>

<span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">data_loader</span><span class="p">):</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>

    <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">output</span><span class="p">,</span><span class="n">labels</span><span class="p">)</span>

    <span class="n">loss</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>

    <span class="c1"># Only do update the weights if it the last mini batch
</span>    <span class="k">if</span> <span class="p">((</span><span class="n">idx</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="n">acc_steps</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span><span class="n">idx</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">data_loader</span><span class="p">)):</span>
        <span class="n">optimizer</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">()</span>
</code></pre></div></div>

<p><strong>Large Batch Training</strong></p>

<p>The authors also noted that large batch training can improve training without the need to do complex techniques.</p>

<p>A quote from the paper</p>

<blockquote>
  <p>Large batch training can improve training efficiency even
without large scale parallel hardware through gradient accumulation, whereby gradients from multiple mini-batches
are accumulated locally before each optimization step.</p>
</blockquote>

<p><strong>Gradient Checkpointing</strong></p>

<p>There really is no reason to mention this here, But this is a popular method to overcome memory limitations too. So I though I will briefly talk about it.</p>

<p>It is rather straightforward, We know when we train a NN, we need to store the gradients for backpropagation. For large models this can get quite big.S
So for some layers instead of storing it, we re-compute it during backprop.</p>

<p>(Obviously much more goes into training the large scale LLMs we use nowdays, we will talk about them as we move forward)</p>

<h3 id="distilbert-and-model-compression">DistilBERT and Model Compression</h3>

<p><img src="/assets/blog_assets/evolution_of_llms/distillbert_abstract.webp" alt="Image of BERT" /></p>

<blockquote>
  <p>Link to paper: <a href="https://arxiv.org/abs/1910.01108">DistilBERT, a distilled version of BERT: smaller,faster, cheaper and lighter</a></p>
</blockquote>

<details>
<summary>Quick Summary</summary>
<div>
    <p>This 2020 paper by Sanh et al. from Hugging Face introduces DistilBERT, a smaller, faster version of BERT created through knowledge distillation. The authors address the growing concern that state-of-the-art NLP models are becoming increasingly large and computationally expensive, limiting their practical deployment, especially on edge devices.</p>

    <p>Key contributions:</p>

    <ol>
      <li>
        <p>They create a distilled version of BERT that retains 97% of its language understanding capabilities while being 40% smaller and 60% faster at inference time.</p>
      </li>
      <li>
        <p>DistilBERT is built using knowledge distillation during the pre-training phase (rather than task-specific distillation), using a triple loss function that combines:</p>

        <ul>
          <li>The standard masked Language Modeling loss</li>
          <li>A distillation loss using the teacher’s soft target probabilities</li>
          <li>A cosine embedding loss to align the directions of the student and teacher hidden states</li>
        </ul>
      </li>
      <li>
        <p>The student model (DistilBERT) uses the same architecture as BERT but with half the number of layers, and is initialized by taking every other layer from the teacher model.</p>
      </li>
      <li>
        <p>The authors demonstrate that DistilBERT performs well across various NLP tasks:</p>

        <ul>
          <li>On GLUE benchmark tasks, it retains 97% of BERT-base’s performance</li>
          <li>On IMDb sentiment classification, it achieves 92.82% accuracy (vs. 93.46% for BERT-base)</li>
          <li>On SQuAD question answering, it reaches 85.8 F1 (vs. 88.5 for BERT-base)</li>
        </ul>
      </li>
      <li>
        <p>They also show that DistilBERT can run effectively on mobile devices, with a model size of 207 MB and 71% faster inference time than BERT on an iPhone 7 Plus.</p>
      </li>
    </ol>

    <p>This work demonstrates that through careful distillation, smaller and more efficient models can be created without significant loss in performance, making state-of-the-art NLP more accessible for resource-constrained applications.</p>

  </div>
</details>
<p><br /></p>

<p><img src="/assets/blog_assets/evolution_of_llms/84.webp" alt="Image of BERT" /></p>

<p>The following sources helped me immensely while writing this section.</p>

<ul>
  <li><a href="https://arxiv.org/pdf/2006.05525">Survey paper</a> on Knowledge Distillation</li>
  <li><a href="https://huggingface.co/blog/Kseniase/kd">HuggingFace Blog</a> on Knowledge distillation</li>
  <li><a href="https://medium.com/huggingface/distilbert-8cf3380435b5">Blog by the creators of distillbert</a></li>
</ul>

<h5 id="what-is-knowledge-distillation">What is Knowledge Distillation</h5>

<p><img src="/assets/blog_assets/evolution_of_llms/52.webp" alt="Image of Knowledge Distillation" /></p>

<p>The idea is quite simple we have a big heavy model trained for long period of time on a lot of data, and we want a smaller model to learn from the bigger model.</p>

<p>There are many reasons we may wish to do this, they are cheaper and faster to inference. They can on edge devices like mobiles, watches, drones etc. And most of the times they retain 90+ performance of the original model while being significally smaller.</p>

<h5 id="types-of-knowledge-distillation">Types of Knowledge Distillation</h5>

<p><img src="/assets/blog_assets/evolution_of_llms/60.webp" alt="Image of different types of knowledge distillation" />
<em>Image taken from <a href="https://arxiv.org/pdf/2006.05525">paper</a></em></p>

<p>There are definitely many kinds of distillation methods available to us (the above image makes it quite obvious). We will not be able to talk about all of them, So I will touch on the most popular one’s and ask you to explore the one’s you find interesting.</p>

<p><strong>Response Based</strong></p>

<p><img src="/assets/blog_assets/evolution_of_llms/63.webp" alt="Image of BERT" /></p>

<p>In this method, we take a sample of training data. Run it by both our models. And then make the smaller model learn the soft labels of the bigger model.</p>

<p>We are essentially trying to make the smaller model predict <code class="language-plaintext highlighter-rouge">like</code> the bigger model.</p>

<p>Now one may question what are soft labels, Lets see it with an example</p>

<p><code class="language-plaintext highlighter-rouge">[0,1,0,0] -&gt; hard labels</code></p>

<p><code class="language-plaintext highlighter-rouge">[0.1,0.3,0.5,0.1] -&gt; soft labels</code></p>

<p>A model outputs an array of probabilities, out of which many are near zero probabilities. These near zero probabilities still hold a lot of knowledge</p>

<p>For instance an apple can be confused for a red ball, but it should not be confused for the moon.</p>

<p>This is known as <a href="https://www.ttic.edu/dl/dark14.pdf">dark knowledge</a>. We are essentially trying to make our smaller model learn this dark knowledge.</p>

<p><a href="https://arxiv.org/pdf/1503.02531">Hinton et al</a> introduced an idea of temperature to control the importance of these soft labels. As T tends to inifity all latels have the same value</p>

\[p_i = \frac{\exp(z_i/T)}{\sum_j \exp(z_j/T)}\]

<p>Where:</p>

<ul>
  <li>$p_i$ is the probability of class $i$</li>
  <li>$z_i$ is the logit for class $i$</li>
  <li>$T$ is the temperature parameter</li>
</ul>

<p>Temperature $T$ controls how “soft” the probability distribution becomes. When $T = 1$, we get the normal softmax. As $T$ increases, the distribution becomes more uniform (softer), revealing the relative similarities between classes that the teacher model has learned. When $T → ∞$, all probabilities approach equal values. During distillation, we use $T &gt; 1$ to extract this “dark knowledge” from near-zero probabilities.</p>

<p><strong>Feature Based</strong></p>

<p><img src="/assets/blog_assets/evolution_of_llms/64.webp" alt="Image of BERT" /></p>

<p>In this, we hope to replicate the weights of the bigger model in our smaller model. Instead of matching final outputs, we match intermediate representations (feature maps) from corresponding layers of teacher and student models. The student learns to produce similar internal representations as the teacher, capturing how the teacher processes information at different depths. Transformation functions $\Phi_t$ and $\Phi_s$ may be needed when teacher and student have different dimensions.</p>

\[L_{FeaD}(f_t(x), f_s(x)) = L_F(\Phi_t(f_t(x)), \Phi_s(f_s(x)))\]

<p>where:</p>

<ul>
  <li>$L_{FeaD}$ is the feature distillation loss</li>
  <li>$f_t(x)$ and $f_s(x)$ are the teacher and student feature representations</li>
  <li>$\Phi_t$ and $\Phi_s$ are transformation functions for the teacher and student features</li>
  <li>$L_F$ is the loss function applied to the transformed features</li>
</ul>

<p><strong>Relation Based</strong></p>

<p><img src="/assets/blog_assets/evolution_of_llms/65.webp" alt="Image of BERT" /></p>

<p>Take a pair of feature maps from both the models, and minimize the loss between their relationship.</p>

<blockquote>
  <p>NOTE: Even though feature maps is used both in featured based and realtion based KD, they are quite different. For example in Feature based we are trying to map the exact features to be in the same distribution. But in realtion based, the indiividual values do not matter, the realtion between them does. For example in feature based if Ft(Xt) = 5 then we want Fs(X) = 5 as well. But in Relation Ft(x1,x2) = 5 then Fs(x3,x4) = 5 where the feature maps x by themselves can be quite different</p>
</blockquote>

<p>As mentioned earlier, there are a lot of methods available for KD. If you are further intersted, checkout the survey!</p>

<h5 id="how-was-diltillbert-trained">How was diltillbert trained</h5>

<p>Distillbert was trained using the response based distillation method. KLD is used as the distillation Loss.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="n">F</span>
<span class="kn">from</span> <span class="nn">torch.optim</span> <span class="kn">import</span> <span class="n">Optimizer</span>

<span class="n">KD_loss</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">KLDivLoss</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="s">'batchmean'</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">kd_step</span><span class="p">(</span><span class="n">teacher</span><span class="p">:</span> <span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">,</span>
            <span class="n">student</span><span class="p">:</span> <span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">,</span>
            <span class="n">temperature</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
            <span class="n">inputs</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">,</span>
            <span class="n">optimizer</span><span class="p">:</span> <span class="n">Optimizer</span><span class="p">):</span>
    <span class="n">teacher</span><span class="p">.</span><span class="nb">eval</span><span class="p">()</span> <span class="c1"># Notice teacher model is in eval mode
</span>    <span class="n">student</span><span class="p">.</span><span class="n">train</span><span class="p">()</span>

    <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="n">logits_t</span> <span class="o">=</span> <span class="n">teacher</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="n">inputs</span><span class="p">)</span> <span class="c1"># Do not store the gradients of teacher
</span>    <span class="n">logits_s</span> <span class="o">=</span> <span class="n">student</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="n">inputs</span><span class="p">)</span>

    <span class="n">loss</span> <span class="o">=</span> <span class="n">KD_loss</span><span class="p">(</span><span class="nb">input</span><span class="o">=</span><span class="n">F</span><span class="p">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="n">logits_s</span><span class="o">/</span><span class="n">temperature</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">),</span>
                   <span class="n">target</span><span class="o">=</span><span class="n">F</span><span class="p">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">logits_t</span><span class="o">/</span><span class="n">temperature</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">))</span> <span class="c1"># KLD loss on teacher output with student output.
</span>                   <span class="c1"># Notice how the student output is a log softmax, because we are training it
</span>
    <span class="n">loss</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">()</span>
</code></pre></div></div>

<p><em>Code modified from <a href="https://gist.github.com/VictorSanh/db90644aae5094654db87f9769c2e5ae">gist</a></em></p>

<p><img src="/assets/blog_assets/evolution_of_llms/62.webp" alt="Architecture difference" />
<em><a href="https://www.researchgate.net/publication/382939584_A_novel_iteration_scheme_with_conjugate_gradient_for_faster_pruning_on_transformer_models">Source</a></em></p>

<p>The architecture of distillbert is very similar to BERT, they have reduced the number of layers. Significantly decreasing the amount of parameters (From 110M to 66M), While maintaing 95% performance of BERT. The authors additionally removed the token-type embeddings and the pooler (as there is no Next sentence predition here)</p>

<p>It is interesting to talk about the initialization and training as well. As noted by the author they initialized distillbert by taking the weights of every other layer (as they had common hidden size namely 768). They also trained the model on very large batches, using gradient accumulation, with dynamic masking and NSP removed.</p>

<p>Now let’s talk about the loss used to train DistillBert. They used a triple loss which is a combination of MLM Loss, Distillation Loss &amp; Similarity Loss.</p>

<blockquote>
  <p>These Beautiful images were taken from this <a href="https://towardsdatascience.com/distilbert-11c8810d29fc/">blog</a></p>
</blockquote>

<p><img src="/assets/blog_assets/evolution_of_llms/66.webp" alt="Image of MLM Loss" />
<em><a href="https://towardsdatascience.com/distilbert-11c8810d29fc/">Source</a></em></p>

<p>Masked Language Modeling Loss is pretty straight forward and we have already talked about it in our <a href="#bert">BERT</a> section. In this we do cross entropy over predicted distribution and true distribution.</p>

<p><img src="/assets/blog_assets/evolution_of_llms/67.webp" alt="Image of BERT" />
<em><a href="https://towardsdatascience.com/distilbert-11c8810d29fc/">Source</a></em></p>

<p>Distillation loss is an idea that is new here, it is a response based loss, In which we compare the KLD between the Teacher’s output distribution and the Student’s output distribution.</p>

<p>Here KLD and Cross-Entropy has been used interchangably, but that is usually not the case.</p>

<p>KLD and cross-entropy can be used interchangeably only when the one distribution is fixed (Here the teacher distribution). Here’s why:</p>

\[\text{KLD}(p_{teacher} || p_{student}) = \sum_i p_{teacher}(i) \log \frac{p_{teacher}(i)}{p_{student}(i)}\]

\[= \sum_i p_{teacher}(i) \log p_{teacher}(i) - \sum_i p_{teacher}(i) \log p_{student}(i)\]

<p>Since the teacher’s distribution is fixed during training, the first term $\sum_i p_{teacher}(i) \log p_{teacher}(i)$ is constant and can be ignored for optimization purposes. The remaining term $-\sum_i p_{teacher}(i) \log p_{student}(i)$ is exactly the cross-entropy loss. Therefore, minimizing KLD is equivalent to minimizing cross-entropy in this context.</p>

<p><img src="/assets/blog_assets/evolution_of_llms/68.webp" alt="Image of BERT" />
<em><a href="https://towardsdatascience.com/distilbert-11c8810d29fc/">Source</a></em></p>

<p>Cosine embedding loss is straight forward as well, here we just take the cosine distance between the embedding vector.</p>

<p>Remember the embedding matrix is also a learned parameter!</p>

<p><img src="/assets/blog_assets/evolution_of_llms/69.webp" alt="Image of BERT" />
<em><a href="https://towardsdatascience.com/distilbert-11c8810d29fc/">Source</a></em></p>

<p>In the end, we put all of it together and train DistillBERT!!!</p>

<p>If you would like to distill a model, this is a <a href="https://docs.pytorch.org/tutorials/beginner/knowledge_distillation_tutorial.html">good tutorial</a> by PyTorch.
Furthermore this was a nice <a href="https://arxiv.org/pdf/2502.08606">paper</a> on distillation scalling laws, consider checking it out!</p>

<h3 id="bart">BART</h3>

<p><img src="/assets/blog_assets/evolution_of_llms/BART_abstract.webp" alt="Image of BART" /></p>

<blockquote>
  <p>Link to paper: <a href="https://arxiv.org/abs/1910.13461">BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension</a></p>
</blockquote>

<details>

<summary>Quick Summary</summary>
<div>
    <p>The paper introduces BART (Bidirectional and Auto-Regressive Transformers), a denoising autoencoder for pretraining sequence-to-sequence models. BART works in two stages:</p>

    <ol>
      <li>It first corrupts text with various noising functions (like token masking, deletion, text infilling, sentence shuffling)</li>
      <li>Then it learns to reconstruct the original text</li>
    </ol>

    <p>BART combines the bidirectional encoding approach of BERT with the autoregressive generation capabilities of GPT. This architecture makes it particularly effective for both text generation and comprehension tasks. The authors evaluate various noising approaches and find that randomly shuffling sentences combined with a novel text infilling scheme (replacing spans with mask tokens) works best.</p>

    <p>In experiments, BART achieves strong performance across multiple NLP tasks:</p>

    <ul>
      <li>Matching RoBERTa on classification tasks like GLUE and SQuAD</li>
      <li>Achieving new state-of-the-art results on summarization tasks (with up to 6 ROUGE point improvements)</li>
      <li>Showing effectiveness for dialogue, question answering, and even machine translation</li>
    </ul>

    <p>The paper presents a thorough ablation study comparing BART to other pretraining approaches and demonstrates its versatility as a general-purpose language model.</p>

  </div>
</details>
<p><br /></p>

<p>BART is not as complex as the ideas we have discussed so far, reading the summary above should be enough.</p>

<p><img src="/assets/blog_assets/evolution_of_llms/58.webp" alt="Image of BART" /></p>

<p>In simple terms BART is just BERT + GPT. The novel idea it introduced was in it’s training. Where corrupted data was given as the input and using reconstruction loss (Cross entropy over predicted and original data distribution), the outputs were predicted.</p>

<p><img src="/assets/blog_assets/evolution_of_llms/59.webp" alt="Image of BART" /></p>

<p>Most of the noise and corruption is self explanatory from the above image, except text infilling. So let us talk about that for a moment.</p>

<p>In it we take a text (Pizza is the most delicious dish in the world) and we sample a span, whose length is drawn from a Poisson distribution ($\lambda=3$). These spans (Pizza is/delicious/in the world) are then masked ([MASK] the most [MASK] dish [MASK]).</p>

<p>In the current age, The BART architecture never gained popularity. I believe partly due to it’s complexity. There is nothing else left to discuss so I’ll be skipping the fine-tuning method. If you wish to know more about it, consider going through the paper.</p>

<h3 id="transformer-xl">Transformer-XL</h3>

<p><img src="/assets/blog_assets/evolution_of_llms/transformer_xl.webp" alt="Image of Transformer-XL" /></p>

<blockquote>
  <p>Link to paper: <a href="https://arxiv.org/abs/1901.02860">Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context</a></p>
</blockquote>

<details>
<summary>Quick Summary</summary>
<div>
    <p>This paper introduces <strong>Transformer-XL</strong>, a novel neural architecture that addresses a key limitation of standard Transformers in language modeling: their inability to capture dependencies beyond a fixed context length.</p>

    <p>Key Problems Solved</p>

    <p><strong>Context Fragmentation</strong>: Traditional Transformers process text in fixed-length segments without information flow between segments, leading to inefficient optimization and poor prediction of tokens at segment boundaries.</p>

    <p><strong>Limited Dependency Length</strong>: Vanilla Transformers can only model dependencies within their fixed context window, typically a few hundred tokens.</p>

    <p><strong>Main Innovations</strong></p>

    <ol>
      <li>Segment-Level Recurrence Mechanism</li>
    </ol>

    <ul>
      <li>Reuses hidden states from previous segments as extended context for the current segment</li>
      <li>Enables modeling of much longer dependencies (80% longer than RNNs, 450% longer than vanilla Transformers)</li>
      <li>Creates a recurrent connection between segments while maintaining the parallelization benefits of self-attention</li>
    </ul>

    <ol>
      <li>Relative Positional Encoding</li>
    </ol>

    <ul>
      <li>Replaces absolute positional encodings with relative ones to enable state reuse</li>
      <li>Decomposes attention into four intuitive terms: content-based addressing, content-dependent positional bias, global content bias, and global positional bias</li>
      <li>Allows models trained on shorter sequences to generalize to longer ones during evaluation</li>
    </ul>

    <p><strong>Results</strong></p>

    <p>Transformer-XL achieved state-of-the-art results across multiple datasets:</p>

    <ul>
      <li><strong>WikiText-103</strong>: 18.3 perplexity (previous best: 20.5)</li>
      <li><strong>enwiki8</strong>: 0.99 bits per character (first to break 1.0)</li>
      <li><strong>One Billion Word</strong>: 21.8 perplexity</li>
      <li>Up to <strong>1,874x faster</strong> evaluation speed compared to vanilla Transformers</li>
    </ul>

    <p><strong>Technical Contributions</strong></p>

    <p>The architecture maintains gradient flow within segments while allowing information to propagate across segments through cached hidden states. The relative positional encoding scheme ensures temporal coherence when reusing states and generalizes well to longer contexts than seen during training.</p>

    <p><strong>Impact</strong></p>

    <p>Transformer-XL demonstrated the ability to generate coherent text spanning thousands of tokens and established new benchmarks for long-range dependency modeling. The techniques introduced became influential for subsequent developments in large language models, particularly for handling longer contexts efficiently.</p>

    <p>The paper represents a significant step forward in extending the effective context length of Transformer-based models while maintaining computational efficiency.</p>

  </div>
</details>
<p><br /></p>

<p><strong>Problem</strong>
The big problem with vanilla transformers was their fixed context length. During training and inference, they could only pay attention to a limited chunk of text at a time, like reading a book through a keyhole. Any information outside that small window was completely invisible.</p>

<p>This method of splitting text into rigid segments also caused context fragmentation. Imagine a sentence being cut in half between two segments. The model would struggle to understand the beginning of the second segment because it couldn’t see the end of the first one, leading to poor performance and inefficient training.</p>

<p><strong>Solution</strong></p>

<ol>
  <li>A method to pass information through the segments</li>
  <li>Novel Positional Encoding Scheme</li>
</ol>

<p><img src="/assets/blog_assets/evolution_of_llms/71.webp" alt="Image of Fixed context window" />
<em><a href="https://arxiv.org/pdf/1901.02860">Source</a></em></p>

<p>The above image is of a traditional model during it’s train and evaluation phase. Notice the segments, you will see that Segment 2 [$X_5$ to $X_8$] have no information passed from Segment 1 [$X_1$ to $X_4$]. This leads to two problems, first being we can only use our model withing a limited context window, Outside which we won’t be able to run it. The other being of data fragmentation, I.E text data is highly connected and breaking it into segments can break the flow of information (Something present in the first segment which is relevant to the third segment won’t be present in it!)</p>

<p>To solve this problem, the authors introduced two ideas</p>

<p><strong>Segment-level Recurrence</strong></p>

<p><img src="/assets/blog_assets/evolution_of_llms/72.webp" alt="Image of Fixed context window" />
<em><a href="https://arxiv.org/pdf/1901.02860">Source</a></em></p>

<p>In this during the train phase, after a segment is processed, its calculated hidden states are cached and reused as an extended context for the next segment. When the model processes the new segment, it can attend to both its own tokens and the tokens from the previous segment’s cache.</p>

<p>Crucially, the gradients only flow through the current segment. The cached states are “frozen,” acting as a read-only memory. This allows the model to learn from past information without the massive computational cost of backpropagating through the entire sequence, similar in spirit to truncated backpropagation in RNNs.</p>

<p><strong>Relative Positional Encodings</strong></p>

<p>There is still one minor problem that needs to be addressed. Back then, fixed positional encoding used to be used in models (<a href="https://goyalpramod.github.io/blogs/Transformers_laid_out/#sinusoidal-encoding">Read this</a> if you are new to this idea).</p>

<p>It assigns a unique embedding to each position (e.g., position 1, position 2, etc.). This breaks when we introduce recurrence.</p>

<ul>
  <li>Segment 1 has tokens at positions <code class="language-plaintext highlighter-rouge">1, 2, 3, 4</code></li>
  <li>Segment 2 also has tokens at positions <code class="language-plaintext highlighter-rouge">1, 2, 3, 4</code></li>
</ul>

<p>When we process Segment 2, the model sees the token at absolute position 5 (the first token of Segment 2) and the token at absolute position 1 (the first token of Segment 1). But both are labeled with the same positional encoding for “position 1”. The model has no way to tell them apart, leading to what the paper calls temporal confusion.</p>

<p>And the authors solve it using Relative Positional Encoding. Ok, this part get’s kind of tough, so bear with me as I walk you through step by step</p>

<p>Step 1: The Math of a Standard Transformer</p>

<p>In a standard Transformer, the input for a token at a specific position <code class="language-plaintext highlighter-rouge">i</code> is the sum of its word embedding and its positional embedding:</p>

\[X_i = E_i + U_i\]

<p>Here, $E_i$ is the word embedding (the token’s meaning), and $U_i$ is the <strong>absolute positional embedding</strong> (the token’s fixed address in the sequence).</p>

<p>The Query and Key vectors are linear transformations of this input:</p>

<p>\(Q_i = (E_i + U_i)W_q\)
\(K_j = (E_j + U_j)W_k\)</p>

<p>The attention score is the dot product of a specific Query vector $Q_i$ with a specific Key vector $K_j$. If we expand this dot product, we reveal four distinct components that the model uses to calculate attention:</p>

\[A_{i,j}^{\text{abs}} = ( (E_i + U_i)W_q )^\top ( (E_j + U_j)W_k )\]

\[A_{i,j}^{\text{abs}} = \underbrace{E_i^\top W_q^\top W_k E_j}_{\text{(a) content-content}} + \underbrace{E_i^\top W_q^\top W_k U_j}_{\text{(b) content-position}} + \underbrace{U_i^\top W_q^\top W_k E_j}_{\text{(c) position-content}} + \underbrace{U_i^\top W_q^\top W_k U_j}_{\text{(d) position-position}}\]

<p>The problem lies in terms (b), (c), and (d), which all rely on the absolute position vectors $U_i$ and $U_j$. This rigid system breaks when we use recurrence, as the model can’t distinguish between position <code class="language-plaintext highlighter-rouge">5</code> in segment one and position <code class="language-plaintext highlighter-rouge">5</code> in segment two.</p>

<p>Step 2: Rebuilding the Attention Score for Transformer-XL</p>

<p>To solve this, Transformer-XL re-engineers the attention score to be based only on relative distances, removing all absolute positional information. This is done by carefully modifying each of the four terms.</p>

<p>The new formula is:</p>

\[A_{i,j}^{\text{rel}} = \underbrace{E_i^\top W_q^\top W_{k,E} E_j}_{\text{(a) content-based addressing}} + \underbrace{E_i^\top W_q^\top W_{k,R} R_{i-j}}_{\text{(b) content-dependent positional bias}} + \underbrace{\mathbf{u}^\top W_{k,E} E_j}_{\text{(c) global content bias}} + \underbrace{\mathbf{v}^\top W_{k,R} R_{i-j}}_{\text{(d) global positional bias}}\]

<p>Let’s break down the changes:</p>

<ol>
  <li>
    <p><strong>Replacing Absolute Positions</strong>: The key’s absolute position vector $U_j$ is replaced with a <strong>relative position vector</strong> $R_{i-j}$. This vector encodes the distance between the query <code class="language-plaintext highlighter-rouge">i</code> and the key <code class="language-plaintext highlighter-rouge">j</code> (e.g., “3 steps behind”).</p>
  </li>
  <li>
    <p><strong>Splitting the Key Matrix</strong>: The key weight matrix $W_k$ is split into $W_{k,E}$ for producing content-based keys and $W_{k,R}$ for producing location-based keys.</p>
  </li>
  <li>
    <p><strong>Removing the Query’s Position</strong>: The query’s absolute position term ($U_i^\top W_q^\top$) is entirely replaced by two trainable parameter vectors, $\mathbf{u}$ and $\mathbf{v}$. These act as global biases.</p>
    <ul>
      <li>Term (c) now represents a global bias for the <em>content</em> of the key word.</li>
      <li>Term (d) now represents a global bias for the <em>relative distance</em> to the key word.</li>
    </ul>
  </li>
</ol>

<p>With these changes, the attention score no longer depends on where a token is, but only on what it is and how far away it is from other tokens. This elegant solution makes the attention mechanism compatible with the segment-level recurrence that allows Transformer-XL to see beyond a fixed context.</p>

<p>The above is definitely as straight forward as some of the other concepts we have talked about so far, so take your time trying to understand it.</p>

<h3 id="xlnet">XLNet</h3>

<p><img src="/assets/blog_assets/evolution_of_llms/XLNet_abstract.webp" alt="Image of XLNet" /></p>

<blockquote>
  <p>Link to paper: <a href="https://arxiv.org/abs/1906.08237">XLNet: Generalized Autoregressive Pretraining for Language Understanding</a></p>
</blockquote>

<details>
<summary>Quick Summary</summary>
<div>
    <p>XLNet is a novel approach to pretraining language models that combines the advantages of both autoregressive (AR) language models like GPT and autoencoding (AE) models like BERT, while avoiding their limitations.</p>

    <p>The key innovation of XLNet is its <strong>permutation Language Modeling objective</strong>. Rather than using a fixed left-to-right order like traditional autoregressive models, XLNet maximizes the expected log likelihood over all possible permutations of the factorization order for a sequence. This allows each token to effectively see context from both directions while maintaining the autoregressive property.</p>

    <p>XLNet addresses two key limitations of BERT:</p>

    <ol>
      <li>It eliminates the independence assumption BERT makes during training (where masked tokens are predicted independently)</li>
      <li>It avoids the pretrain-finetune discrepancy caused by the artificial [MASK] tokens used in BERT</li>
    </ol>

    <p>Key architectural components include:</p>

    <ul>
      <li>A <strong>two-stream attention mechanism</strong> that creates separate content and query streams to enable target-aware predictions</li>
      <li>Integration of <strong>Transformer-XL</strong> for better handling of long sequences</li>
      <li><strong>Relative positional encodings</strong> and <strong>relative segment encodings</strong> for improved generalization</li>
    </ul>

    <p>In empirical evaluations, XLNet outperforms BERT on 20 tasks including question answering, natural language inference, sentiment analysis, and document ranking, often by substantial margins.</p>

  </div>
</details>
<p><br /></p>

<p>It’s hard to break this paper down into a problem and solution segment, As the broader idea was “Both Auto-regressive (Decoder only) &amp; Auto-Encoding (Encoder Only) have their pros and cons, but how do we bring the best of both worlds together”. This is not an easy paper by a long shot. I will try my best to explain it as well as I can.</p>

<p>First we need to understand the pros and cons of AR &amp; AE models as identified by the authors</p>

<p><strong>AE Pros &amp; Cons</strong></p>

<p>Pros:</p>

<ul>
  <li>It is bidirectional by nature, so it can understand information from both direction</li>
</ul>

<p>Cons:</p>

<ul>
  <li>
    <p><strong>Independence Assumption</strong>: BERT assumes all masked tokens are predicted independently of each other. For the phrase “New York,” if both words are masked, BERT doesn’t use its prediction for “New” to help it predict “York”.</p>
  </li>
  <li>
    <p><strong>Pretrain-Finetune Discrepancy</strong>: The artificial <code class="language-plaintext highlighter-rouge">[MASK]</code> symbol used during pre-training is absent during fine-tuning, creating a mismatch that can affect performance.</p>
  </li>
</ul>

<p><strong>AR Pros &amp; Cons</strong></p>

<p>Pros:</p>

<ul>
  <li>Captures the sequential nature of natural language, i.e one word comes after the other.</li>
</ul>

<p>Cons:</p>

<ul>
  <li>It is unidirectional, meaning it can only process context from one direction (either left-to-right or right-to-left), which is a disadvantage for many downstream tasks. For example classification</li>
</ul>

<p>To address the cons of BERT it uses an AR model, and to make it bidirectional. The authors introduce a new idea called <strong>Permutation Language Modeling</strong> objective.</p>

<p>In this instead of training our models in sequential form <code class="language-plaintext highlighter-rouge">1,2,3,4</code> and so on. It is instead trained on all possible permutations of the sequence. For example <code class="language-plaintext highlighter-rouge">4,2,1,3</code>/<code class="language-plaintext highlighter-rouge">2,4,3,1</code>/<code class="language-plaintext highlighter-rouge">1,4,2,3</code> and many more. Now it is a common source of confusion that the data it self is scrambled. But there is no augmentation done in the data or position encoding side. The embeddings themselves and their positions remain the same.</p>

<p>Instead the attention values are masked. Let us understand how.</p>

<p><img src="/assets/blog_assets/evolution_of_llms/73.webp" alt="Image of Megatron" />
<em>Modified from <a href="https://arxiv.org/pdf/1906.08237">source</a></em></p>

<p>This looks extremely convoluted, I know!! But let’s go through it one by one.</p>

<p>To understand how masking achieves this, let’s look at the image below, which shows how the model predicts the token at position 3 (x₃) under different factorization (prediction) orders.</p>

<p>This looks complex, but the logic is simple: a token can only see other tokens that come earlier in the random permutation order.</p>

<ul>
  <li>
    <p>In panel (a), the order is 3 → 2 → 4 → 1. Since x₃ is the first token to be predicted, its context is empty.</p>
  </li>
  <li>
    <p>In panel (b), the order is 2 → 4 → 3 → 1. Here, x₃ is the third token to be predicted, so it can see the context from the first two predicted tokens, x₂ and x₄.</p>
  </li>
</ul>

<p>By doing this over and over with random orders, the model learns that any token can be surrounded by any other token, forcing it to learn a rich, bidirectional understanding of the language.</p>

<p>P.S. mem here is the cached hidden state from the previous segment, an idea inspired by Transformer-XL</p>

<p><img src="/assets/blog_assets/evolution_of_llms/74.webp" alt="Image of Megatron" />
<em><a href="https://arxiv.org/pdf/1906.08237">Source</a></em></p>

<p>The permutation objective creates a challenge: To predict a token at a target position, the model needs to know the position, but it cannot know the content of the token at that position (otherwise, it would just copy it). A standard Transformer can’t do this, as its hidden state at any position always contains both content and position information.</p>

<p>To solve this, XLNet introduces a Two-Stream Self-Attention mechanism.</p>

<p>Content Stream <code class="language-plaintext highlighter-rouge">h</code> (The Memory)
This is the standard Transformer representation. It encodes both the content and position of a token. Its job is to serve as the rich context, or “memory,” that other tokens can attend to. In the image, this is what the nodes labeled <code class="language-plaintext highlighter-rouge">h</code> represent.</p>

<p>Query Stream <code class="language-plaintext highlighter-rouge">g</code> (The Predictor)
This is a special representation that only has access to the target position, not its content. Its entire purpose is to gather information from the Content Stream of its context tokens to make a prediction for the target position. In the image, this is what the nodes labeled <code class="language-plaintext highlighter-rouge">g</code> represent.</p>

<p>In simple terms, for each token being predicted, the Query Stream <code class="language-plaintext highlighter-rouge">g</code> asks the question, and the Content Stream <code class="language-plaintext highlighter-rouge">h</code> of the other tokens provides the answer. At the final layer, the output of the Query Stream is used to predict the word. During fine-tuning, the query stream is discarded, and we use the powerful, bidirectionally-trained Content Stream for downstream tasks.</p>

<h3 id="megatron">Megatron</h3>

<p><img src="/assets/blog_assets/evolution_of_llms/megatron_abstract.webp" alt="Image of Megatron" /></p>

<blockquote>
  <p>Link to paper: <a href="https://arxiv.org/abs/1909.08053">Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism</a></p>
</blockquote>

<details>
<summary>Quick Summary</summary>
<div>
    <p>The Megatron-LM paper presents an approach for training extremely large language models using model parallelism that enables training transformer models with billions of parameters. Let me explain the key aspects of this work:</p>

    <p><strong>Core Innovation</strong>: Simple and Efficient Model Parallelism</p>

    <p>The authors implement a simple but effective model parallel approach where they split transformer layers across multiple GPUs in a way that minimizes communication overhead. They do this through:</p>

    <ol>
      <li>
        <p><strong>Intra-layer model parallelism</strong>: Rather than splitting entire layers across GPUs (pipeline parallelism), they split individual operations within transformer layers.</p>
      </li>
      <li>
        <p><strong>Strategic tensor partitioning</strong>: Matrices in transformer layers are partitioned along specific dimensions to minimize communication:</p>

        <ul>
          <li>In the MLP blocks, the first GEMM is split column-wise and the second GEMM is split row-wise</li>
          <li>In self-attention, they partition across attention heads, allowing each GPU to process different attention heads</li>
        </ul>
      </li>
      <li>
        <p><strong>Communication optimization</strong>: They carefully place all-reduce operations to minimize the number of synchronization points needed between GPUs.</p>
      </li>
      <li>
        <p><strong>Duplicate computation</strong>: Instead of communicating for small operations like dropout or layer normalization, they duplicate these computations across GPUs.</p>
      </li>
    </ol>

    <p><strong>Scaling Achievements</strong></p>

    <ul>
      <li>They established a strong baseline by training a 1.2 billion parameter model on a single GPU that achieves 39 TeraFLOPs (30% of theoretical peak)</li>
      <li>They scaled to an 8.3 billion parameter model using 512 GPUs with 8-way model parallelism and 64-way data parallelism</li>
      <li>They achieved 15.1 PetaFLOPs sustained performance with 76% scaling efficiency compared to the single GPU case</li>
    </ul>

    <p><strong>Architecture Innovation for BERT Models</strong></p>

    <p>The authors discovered that the standard BERT architecture suffers from degradation when scaled beyond the BERT-Large size. They fixed this by rearranging the layer normalization and residual connections in the architecture, enabling larger BERT models to achieve consistently better results.</p>

    <p><strong>Results</strong></p>

    <p>Their models achieved state-of-the-art results on:</p>

    <ul>
      <li>WikiText103 (10.8 perplexity vs previous SOTA of 15.8)</li>
      <li>LAMBADA (66.5% accuracy vs previous SOTA of 63.2%)</li>
      <li>RACE dataset (90.9% accuracy vs previous SOTA of 89.4%)</li>
    </ul>

    <p>The paper demonstrates that with the right implementation approach, training multi-billion parameter language models is feasible, and these larger models lead to superior performance on a wide range of NLP tasks.</p>

  </div>
</details>
<p><br /></p>

<p>This is a great time to talk about data, model and pipeline paralism and how massively large LLMs are trained across GPUs, Because that is what essential the NVIDIA team did with megatron and shared how they did it!</p>

<p>Obviously Parallel training of huge models on large clusters is a MEGA topic, and we can write books on it. (There are companies and books made on it).
So my objective here will be to introduce you to the various concepts of model parallalism, going a bit overboard on what was done in megatron too (for brevity’s sake)</p>

<p>Let us start with the idea of megatron training and we can build on top of that</p>

<blockquote>
  <p>note: the paper talks about two main ideas, training &amp; how they improved performance on BERT. We will only focus on training in this blog. If you are curious about BERT modifications. Check out the paper!</p>
</blockquote>

<p>Now to be complete, We also need to talk about how we can calculate the amount of VRAM required before we start with distributed training. Lest you get too many or too less GPU (Compute is extremely expensive, so we gotta be mindful of what we have)</p>

<p>These are the rough GPU memory requirements:</p>

<ul>
  <li>FP16 parameter ~ 2 bytes</li>
  <li>FP16 gradient ~ 2 bytes</li>
  <li>FP32 optimizer state ~ 8 bytes based on the Adam optimizers</li>
  <li>FP32 copy of parameter ~ 4 bytes (needed for the optimizer apply (OA) operation)</li>
  <li>FP32 copy of gradient ~ 4 bytes (needed for the OA operation)</li>
</ul>

<blockquote>
  <p>Even for a relatively small DL model with 10 billion parameters, it can require at least 200GB of memory, which is much larger than the typical GPU memory (for example, NVIDIA A100 with 40GB/80GB memory and V100 with 16/32 GB) available on a single GPU. Note that on top of the memory requirements for model and optimizer states, there are other memory consumers such as activations generated in the forward pass. The memory required can be a lot greater than 200GB.
(Taken from sagemaker AWS documentation)</p>
</blockquote>

<p>If you want to understand the memory better I will recommend the following blogs:</p>

<ul>
  <li><a href="https://huggingface.co/blog/hf-bitsandbytes-integration">A Gentle Introduction to 8-bit Matrix Multiplication for transformers</a></li>
  <li><a href="https://cloud.google.com/blog/products/ai-machine-learning/bfloat16-the-secret-to-high-performance-on-cloud-tpus">BFloat16: The secret to high performance on Cloud TPUs</a></li>
</ul>

<p>Some Misc blogs I found while researching the GPU memory problem:</p>

<ul>
  <li>HF has a <a href="https://huggingface.co/docs/accelerate/en/usage_guides/model_size_estimator">good estimator</a>.</li>
  <li><a href="https://swsmith.cc/posts/gpu-memory.html">Calculate it yourself</a>!</li>
  <li><a href="https://huggingface.co/docs/transformers/en/perf_hardware">How to build you own machine</a>.</li>
  <li><a href="https://timdettmers.com/2023/01/30/which-gpu-for-deep-learning/#RTX_4090s_and_Melting_Power_Connectors_How_to_Prevent_Problems">What hardware you should be getting for Deep Learning</a>.</li>
</ul>

<p>The following blogs and articles were insanely helpful, While writing the below section:</p>

<ul>
  <li>This <a href="https://alessiodevoto.github.io/parallelism/">blog</a> gave a great TL;DR with pseudocode (The code in this section has been taken/inspired from here)</li>
  <li>Great overview <a href="https://huggingface.co/docs/transformers/v4.15.0/parallelism">documentation</a> by HF.</li>
  <li><a href="https://docs.aws.amazon.com/sagemaker/latest/dg/model-parallel-intro.html">AWS Documentation</a>had some nice animations with practical code that helped me understand stuff.</li>
  <li>This <a href="https://distributedlexicon.com/">blog</a> acted as an awesome glossary and gave good summaries of the ideas.</li>
</ul>

<h5 id="naive-parallalism">Naive Parallalism</h5>

<p>Naive Model Prallelism, is well… naive and straightforward. Here you just cut the layers of a large model. And put it in different GPUs sequentially.</p>

<p><img src="/assets/blog_assets/evolution_of_llms/79.webp" alt="Image of Megatron" />
<em>Inspired from <a href="https://huggingface.co/docs/transformers/v4.15.0/parallelism">here</a></em></p>

<p>The implementation is simple as well, you just keep changing the device using <code class="language-plaintext highlighter-rouge">.to()</code>, But there are a few problems. The first one being, when the data travels from layer 0 upto layer 3. It is pretty fast as they are in the same device, but when it needs to go to layer 4, this introduced a computational overhead. If both the GPUs are not in the same machine, this will make it very slow.</p>

<p>Another problem is, even though we have 2 devices, while the data is being transfered from layer1 to layer3. The other GPUs remain idle. To fix these problems, the idea of pipeline parallelism was introduced.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Define model portions
</span><span class="n">model_part1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">layer1</span><span class="p">,</span> <span class="n">layer2</span><span class="p">).</span><span class="n">to</span><span class="p">(</span><span class="s">'cuda:0'</span><span class="p">)</span>
<span class="n">model_part2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">layer3</span><span class="p">,</span> <span class="n">layer4</span><span class="p">).</span><span class="n">to</span><span class="p">(</span><span class="s">'cuda:1'</span><span class="p">)</span>

<span class="c1"># Forward pass
</span><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">model_part1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="s">'cuda:1'</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">model_part2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</code></pre></div></div>

<p><em>Code taken from <a href="https://alessiodevoto.github.io/parallelism/">here</a></em></p>

<h5 id="pipeline-parallelism">Pipeline Parallelism</h5>

<p><img src="/assets/blog_assets/evolution_of_llms/76.webp" alt="Image of Megatron" />
<em><a href="https://research.google/blog/introducing-gpipe-an-open-source-library-for-efficiently-training-large-scale-neural-network-models/">Source</a></em></p>

<p>Piepline Parallelism is very similar to Naive MP, We the models split in this too. But to solve the idling problem. The data is sent in mini batches. So each GPU has access to some part of the data and they work somewhat concurrently.</p>

<p>There is an obvious bubble present though, that happens when one GPU is waiting for the gradients to do backprop. In practice, an ideal size of batch along with numbers of GPU is calculated so when one forward pass is completed, mini batch of calculated results keep coming back.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Define stages
</span><span class="n">stage1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">layer1</span><span class="p">,</span> <span class="n">layer2</span><span class="p">).</span><span class="n">to</span><span class="p">(</span><span class="s">'cuda:0'</span><span class="p">)</span>
<span class="n">stage2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">layer3</span><span class="p">,</span> <span class="n">layer4</span><span class="p">).</span><span class="n">to</span><span class="p">(</span><span class="s">'cuda:1'</span><span class="p">)</span>

<span class="c1"># Pipeline forward
</span><span class="k">def</span> <span class="nf">pipeline_forward</span><span class="p">(</span><span class="n">batches</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">batch</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">batches</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">stage1</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="s">'cuda:1'</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">i</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">yield</span> <span class="n">stage2</span><span class="p">(</span><span class="n">prev_x</span><span class="p">)</span>
        <span class="n">prev_x</span> <span class="o">=</span> <span class="n">x</span>
    <span class="k">yield</span> <span class="n">stage2</span><span class="p">(</span><span class="n">prev_x</span><span class="p">)</span>
</code></pre></div></div>

<p><em>Code taken from <a href="https://alessiodevoto.github.io/parallelism/">here</a></em></p>

<h5 id="data-parallelism">Data Parallelism</h5>

<p><img src="/assets/blog_assets/evolution_of_llms/80.webp" alt="Image of Megatron" />
<em><a href="https://distributedlexicon.com/">Source</a></em></p>

<p>This is another simple method of parallelism. Each device has a full copy of the model, and the data is split between them. And at the end the gradients are synchronized together.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># On each device
</span><span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">dataloader</span><span class="p">:</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>
    <span class="n">loss</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>

    <span class="c1"># Synchronize gradients across devices
</span>    <span class="n">all_reduce</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">parameters</span><span class="p">.</span><span class="n">grad</span><span class="p">)</span>

    <span class="n">optimizer</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>
</code></pre></div></div>

<p><em>Code taken from <a href="https://alessiodevoto.github.io/parallelism/">here</a></em></p>

<h5 id="tensor-parallelism">Tensor Parallelism</h5>

<p><img src="/assets/blog_assets/evolution_of_llms/75.webp" alt="Image of Megatron" />
<a href="https://huggingface.co/docs/transformers/v4.19.4/en/parallelism">source</a></p>

<p>This is the kind of parallelism first introuduce by the megatron paper.</p>

<p>This is a great read on the topic as well <a href="https://github.com/huggingface/transformers/issues/10321#issuecomment-783543530">github comment</a>.</p>

<p>In this you partition the individual tensors (weights, activations) across devices. And each device computes a portion of the tensor.</p>

<p>This was beautifully explained by the paper, Imagine you have to do an operation (as shown in the above image). We can simply break the matrix and compute it in different devices and put it back together.</p>

<p><strong>Parallelizing the MLP Block</strong></p>

<p>An MLP block contains two linear layers. The first is a GEMM (GEneral Matrix Multiply) followed by a GeLU nonlinearity:</p>

\[Y = \text{GeLU}(XA)\]

<p>To parallelize this across two GPUs, the authors split the weight matrix $A$ column-wise: $A = [A_1, A_2]$. The input $X$ is fed to both GPUs, and each computes its part of the operation:</p>

\[[Y_1, Y_2] = [\text{GeLU}(XA_1), \text{GeLU}(XA_2)]\]

<p>This is efficient because the GeLU activation can be applied independently on each GPU without any communication.</p>

<p><img src="/assets/blog_assets/evolution_of_llms/78.webp" alt="Image of Megatron" /></p>

<p>The second linear layer in the MLP block involves another GEMM, $Z = YB$. To make this work, the second weight matrix $B$ is split row-wise:</p>

\[B = \begin{bmatrix} B_1 \\ B_2 \end{bmatrix}\]

<p>Each GPU then computes its part, and the results are summed up using an <code class="language-plaintext highlighter-rouge">all-reduce</code> operation across the GPUs. This <code class="language-plaintext highlighter-rouge">all-reduce</code> is the only communication needed in the forward pass for the MLP block.</p>

<p><strong>Parallelizing the Self-Attention Block</strong></p>

<p>The same principle is applied to the self-attention mechanism. The large weight matrices for Query, Key, and Value ($W_Q, W_K, W_V$) are split column-wise across the GPUs, partitioned by the number of attention heads. Each GPU can compute its share of attention heads independently.</p>

<p><img src="/assets/blog_assets/evolution_of_llms/77.webp" alt="Image of Megatron" /></p>

<p>Just like in the MLP block, the output linear layer is split row-wise, and a single <code class="language-plaintext highlighter-rouge">all-reduce</code> operation synchronizes the results at the end.</p>

<p>The key takeaway is that this method cleverly arranges the matrix splits so that a full Transformer layer only requires <strong>two <code class="language-plaintext highlighter-rouge">all-reduce</code> operations</strong> in the forward pass (one for the MLP, one for attention) and two in the backward pass. This minimizes communication overhead and leads to excellent scaling efficiency.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Simplified tensor parallel linear layer
</span><span class="k">class</span> <span class="nc">TPLinear</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span><span class="p">,</span> <span class="n">n_devices</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="n">out_features</span> <span class="o">//</span> <span class="n">n_devices</span><span class="p">,</span> <span class="n">in_features</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">local_out</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="n">linear</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">weight</span><span class="p">)</span><span class="n">Z</span>
        <span class="k">return</span> <span class="n">all_gather</span><span class="p">(</span><span class="n">local_out</span><span class="p">)</span>
</code></pre></div></div>

<p><em>Code taken from <a href="https://alessiodevoto.github.io/parallelism/">here</a></em></p>

<h5 id="2d-parallalism--3d-parallalism">2d parallalism &amp; 3d parallalism</h5>

<p>I mentioned it here to keep you aware of these ideas, we will discuss in depth about them later on. But for now a simple deifition is enough.</p>

<ul>
  <li>2d parallism -&gt; When you use two of the techniques described above together.</li>
  <li>3d parallism -&gt; following the above definition when you use 3 of the above described techniques together it’s called 3d parallism.</li>
</ul>

<p>There is also ZeRO but we will talk about it when we get to that section.</p>

<p>For a deeper dive into how to implement each and use them in your application, check out the documentation by the <a href="https://docs.pytorch.org/tutorials/distributed.html">PyTorch Team</a>.</p>

<p>Additionally I will recommend checking these two phenomenol blogs by an engineer at Anthropic.</p>

<ul>
  <li><a href="https://siboehm.com/articles/22/pipeline-parallel-training">Blog 1: Pipeline Parallel Training</a></li>
  <li><a href="https://siboehm.com/articles/22/data-parallel-training">Blog 2: Data Parallel Training</a></li>
</ul>

<p>The code behind <a href="https://github.com/NVIDIA/Megatron-LM">Megatron-LM</a></p>

<h3 id="sparse-attention-patterns">Sparse Attention Patterns</h3>

<p><img src="/assets/blog_assets/evolution_of_llms/sparse_abstract.webp" alt="Image of Megatron" /></p>

<blockquote>
  <p>Link to paper: <a href="https://arxiv.org/abs/1904.10509">Generating Long Sequences with Sparse Transformers</a></p>
</blockquote>

<ul>
  <li>Reduced computational complexity for long sequences</li>
</ul>

<details>
<summary>Quick Summary</summary>
<div>

    <p>This 2019 paper by Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever from OpenAI introduces Sparse Transformers, an architectural innovation that makes Transformers more efficient for modeling very long sequences.</p>

    <p>Key innovations:</p>

    <ul>
      <li>Introduces sparse factorizations of the attention matrix that reduce computational complexity from O(n²) to O(n√n)</li>
      <li>Proposes architectural modifications to train deeper networks</li>
      <li>Implements memory-efficient recomputation of attention matrices</li>
      <li>Develops fast attention kernels for training</li>
    </ul>

    <p>The authors demonstrate that Sparse Transformers can effectively model sequences of tens of thousands of timesteps using hundreds of layers. They apply the same architecture to model images, audio, and text from raw bytes, achieving state-of-the-art results on density modeling tasks for Enwik8, CIFAR-10, and ImageNet-64. Notably, they show it’s possible to use self-attention to model sequences of length one million or more.</p>

  </div>
</details>
<p><br /></p>

<p><strong>Problem</strong></p>

<p>Transformers are awesome, and we all love them (otherwise, you wouldn’t be reading such a huge blog on the topic). But they face one big issue: the self-attention calculation has a quadratic time and memory complexity, which becomes impractical for very long sequences.</p>

<p><strong>Solution</strong></p>

<p>The authors introduce sparse factorization methods to the attention mechanism, reducing its complexity to $O(N\sqrt{N})$. They additionally introduce fast custom kernels and a recomputation method (gradient checkpointing) to save memory.</p>

<p>The following blogs helped me immensely while writing this section:</p>

<ul>
  <li><a href="https://reinforcedknowledge.com/sparse-transformers/">Questioning the authors</a></li>
  <li><a href="https://lilianweng.github.io/posts/2018-06-24-attention/">Attention? Attention!</a></li>
  <li>The original <a href="https://openai.com/index/sparse-transformer/">blog post</a> by OpenAI</li>
</ul>

<h5 id="the-cost-of-full-attention">The Cost of Full Attention</h5>

<p>If you’ve read this far, I am going to assume you have a fair bit of knowledge about computer science. One of the earliest ideas talked about in CS101 is Big O notation. So let’s quickly calculate the complexity of self-attention.</p>

<p>The self-attention mechanism can be expressed as:</p>

\[\text{Attention}(Q, K, V) = \text{Softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V\]

<p>Let’s break down the computational complexity step-by-step for a sequence of length $N$:</p>

<ol>
  <li><strong>Computing $QK^T$</strong>: The multiplication of a $[N \times d_k]$ matrix ($Q$) with a $[d_k \times N]$ matrix ($K^T$) results in an $[N \times N]$ attention matrix. The complexity is $O(N^2 \cdot d_k)$.</li>
  <li><strong>Scaling and Softmax</strong>: These are element-wise operations on the $[N \times N]$ matrix, so their complexity is $O(N^2)$.</li>
  <li><strong>Multiplying with V</strong>: Multiplying the $[N \times N]$ attention matrix with the $[N \times d_v]$ value matrix ($V$) has a complexity of $O(N^2 \cdot d_v)$.</li>
</ol>

<p>Since the hidden dimensions $d_k$ and $d_v$ are considered constants with respect to the sequence length, the dominant factor is $N^2$. The overall complexity of self-attention is:</p>

\[O(N^2)\]

<p>This quadratic cost is computationally expensive for large $N$. This is where Sparse Attention comes in, reducing the complexity to a more manageable $O(N\sqrt{N})$.</p>

<h5 id="building-the-sparse-patterns">Building the Sparse Patterns</h5>

<p>The objective of this blog is to help you believe you could have come up with these ideas on your own. So let’s begin by understanding the rationale of the researchers. By visualizing the attention patterns of a deep, fully-trained Transformer, they noticed some recurring themes.</p>

<p><img src="/assets/blog_assets/evolution_of_llms/53.webp" alt="Attention mask img" />
<em>Image taken from the <a href="https://arxiv.org/pdf/1904.10509">paper</a>, Figure 2</em></p>

<p>a) Many early layers learned <strong>local patterns</strong>, resembling convolutions.<br />
b) Some layers learned to attend to entire <strong>rows and columns</strong>, effectively factorizing the attention.<br />
c) Deeper layers showed <strong>global, data-dependent patterns</strong>.<br />
d) Surprisingly, the deepest layers exhibited <strong>high sparsity</strong>, with positions activating rarely.<br /></p>

<p>From these observations, we get a sense of the useful patterns: we need something local, something that can access information globally, and perhaps something in between, like a stride.</p>

<p><img src="/assets/blog_assets/evolution_of_llms/82.webp" alt="Attention mask img" />
<em>Image taken from the <a href="https://arxiv.org/pdf/1904.10509">paper</a>, Figure 3</em></p>

<p>This image might look daunting, but it’s quite simple if we break it down. Let’s quickly clarify the two rows of images. The <strong>top row</strong> shows the perspective of a single output element (the dark blue square) and which input elements (the light blue squares) it can attend to. The <strong>bottom row</strong> provides a holistic view of the entire sequence’s connectivity matrix.</p>

<p>Now, let’s look at the patterns:</p>

<p><strong>(b) Strided Attention</strong>
This pattern combines two kinds of attention. One head focuses on a <strong>local window</strong> of nearby positions. For example, the i-th position might only attend to keys from <code class="language-plaintext highlighter-rouge">i-w</code> to <code class="language-plaintext highlighter-rouge">i</code>, where <code class="language-plaintext highlighter-rouge">w</code> is the window size. The other head focuses on more global interactions by having the i-th query attend to every <code class="language-plaintext highlighter-rouge">c</code>-th key, where <code class="language-plaintext highlighter-rouge">c</code> is the stride. This captures both local context and a coarser, long-range context.</p>

<p>This method works very well for data with a natural periodic structure, like images (where a pixel is related to its neighbors and also to the pixel directly above it). However, it can fail when it comes to text. The reason is that relationships in text are not based on fixed intervals; a word’srelevance to another isn’t determined by a consistent spatial coordinate or stride.</p>

<p><strong>(c) Fixed Attention</strong>
To solve the problem with text, the authors introduced “fixed attention.” This also uses a local window, but the second head is different. Instead of a stride, it has all future positions attend to a few fixed “summary” locations from previous block.</p>

<p><img src="/assets/blog_assets/evolution_of_llms/81.webp" alt="Attention mask img" />
<em>Inspired from <a href="https://newsletter.theaiedge.io/p/understanding-the-sparse-transformers">here</a></em></p>

<p>The local attention creates the block-like structure. This works well but has one big issue: each block only has information from within its own block.</p>

<p><img src="/assets/blog_assets/evolution_of_llms/83.webp" alt="Attention mask img" />
<em>Inspired from <a href="https://newsletter.theaiedge.io/p/understanding-the-sparse-transformers">here</a></em></p>

<p>The fixed summary positions act as “global connectors.” Information captured by the end of a block can now flow globally to all subsequent blocks, allowing the model to connect ideas across the entire sequence.</p>

<h5 id="calculating-the-complexity-of-sparse-attention">Calculating the Complexity of Sparse Attention</h5>

<p>These sparse patterns significantly reduce the computational cost. Let’s see how, using the formulas from the paper.</p>

<p>In the <strong>strided case</strong>, each query attends to a local window of size $w$ and roughly $N/c$ strided positions, where $c$ is the stride. The total cost for $N$ queries is:</p>

\[N \left( w + \frac{N}{c} \right) \sim O(N\sqrt{N}), \quad \text{if } c = \sqrt{N}\]

<p>In the <strong>fixed case</strong>, the sequence is split into blocks of length $l$. Each query attends to at most $l$ local positions and $c$ summary positions. The total cost for all queries is:</p>

\[N(l+c) \sim O(N\sqrt{N}), \quad \text{if } l = \sqrt{N}\]

<p>In both cases, by carefully choosing the hyperparameters, we can achieve the desired sub-quadratic complexity of $O(N\sqrt{N})$.</p>

<p>Some other ideas introduced in the paper as stated by the authors include:</p>

<blockquote>
  <p>• A restructured residual block and weight initialization to improve training of very deep networks
• A set of sparse attention kernels which efficiently compute subsets of the attenion matrix
• Recomputation of attention weights during the backwards pass to reduce memory usage</p>
</blockquote>

<p>If you wish to check out the work done by OpenAI, you can do so here: <a href="https://github.com/openai/sparse_attention">Sparse Attention Repo</a>. They also open-sourced their work on fast kernels: <a href="https://github.com/openai/blocksparse/">Blocksparse Repo</a>.</p>

<h2 id="notice-build-first-write-later">NOTICE: Build first, Write later</h2>

<blockquote>
  <p>This blog is now on a short hiatus, I give my reasons <a href="https://goyalpramod.github.io/thoughts/build_first_write_later/">here</a>. I will still continue updating this blog, but not as often. Thank you for reading everything I write.
P.S. I am putting out all of the unstructured thoughts and content below for anyone who is curious to check out the resources I had collected for each section</p>
</blockquote>

<h2 id="2020-the-scale-revolution">2020: The Scale Revolution</h2>

<h3 id="reformer">Reformer</h3>

<p><img src="/assets/blog_assets/evolution_of_llms/reformer_abstract.webp" alt="Image of Reformer" /></p>

<blockquote>
  <p>Link to paper: <a href="https://arxiv.org/abs/2001.04451">Reformer: The Efficient Transformer</a></p>
</blockquote>

<details>
<summary>Quick Summary</summary>
<div>

    <p>This 2020 ICLR paper introduces the Reformer, a more memory-efficient and computationally efficient variant of the Transformer architecture. The authors (Kitaev, Kaiser, and Levskaya) address key bottlenecks in standard Transformers:</p>

    <ol>
      <li>The quadratic memory and computation requirements of self-attention (O(L²) where L is sequence length)</li>
      <li>The memory needed to store activations for all layers during backpropagation</li>
      <li>The large memory footprint of feed-forward layers</li>
    </ol>

    <p>Their solution combines two main innovations:</p>

    <ul>
      <li>Replacing standard dot-product attention with a locality-sensitive hashing (LSH) based attention mechanism, reducing complexity from O(L²) to O(L log L)</li>
      <li>Using reversible residual layers that allow recovering activations during backpropagation without storing them, significantly reducing memory requirements</li>
    </ul>

    <p>The authors show that Reformer achieves comparable performance to standard Transformers while enabling training on much longer sequences (up to 64K tokens) and with substantially lower memory usage. They demonstrate results on text (enwik8) and image generation (ImageNet-64) tasks.</p>

  </div>
</details>
<p><br />
https://www.youtube.com/watch?app=desktop&amp;v=i4H0kjxrias&amp;t=0s&amp;ab_channel=YannicKilcher</p>

<p>https://www.pinecone.io/learn/series/faiss/locality-sensitive-hashing/
https://jaketae.github.io/study/lsh/</p>

<p>https://research.google/blog/reformer-the-efficient-transformer/</p>

<p>https://raviteja-ganta.github.io/Reformer-The-efficient-Transformer</p>

<h3 id="longformer">Longformer</h3>

<p><img src="/assets/blog_assets/evolution_of_llms/longformer_abstract.webp" alt="Image of Longformer" /></p>

<blockquote>
  <p>Link to paper: <a href="https://arxiv.org/abs/2004.05150">Longformer: The Long-Document Transformer</a></p>
</blockquote>

<details>

<summary>Quick Summary</summary>
<div>

    <p>The Longformer paper addresses a key limitation of traditional Transformer models: their quadratic computational complexity with respect to sequence length, which makes processing long documents prohibitively expensive. The authors introduce a novel attention mechanism that scales linearly with sequence length, enabling the processing of documents with thousands of tokens.</p>

    <p><strong>Key innovations:</strong></p>

    <ol>
      <li>
        <p><strong>Attention mechanism</strong>: Longformer uses a combination of:</p>

        <ul>
          <li><strong>Sliding window attention</strong>: Each token attends to a fixed window of surrounding tokens</li>
          <li><strong>Dilated sliding window</strong>: Increases receptive field without increasing computation by adding gaps between attended tokens</li>
          <li><strong>Global attention</strong>: Task-specific tokens (like [CLS] or question tokens in QA) can attend to the entire sequence</li>
        </ul>
      </li>
      <li>
        <p><strong>Efficient implementation</strong>: Custom CUDA kernels enable processing sequences of up to 32K characters</p>
      </li>
      <li>
        <p><strong>Performance</strong>: Longformer achieves:</p>

        <ul>
          <li>State-of-the-art results on character-level Language Modeling (text8 and enwik8)</li>
          <li>Outperforms RoBERTa on long document tasks</li>
          <li>Sets new state-of-the-art results on WikiHop and TriviaQA</li>
        </ul>
      </li>
      <li>
        <p><strong>Longformer-Encoder-Decoder (LED)</strong>: A variant for sequence-to-sequence tasks like summarization</p>
      </li>
    </ol>

    <p>The paper demonstrates both the theoretical and practical advantages of this approach across multiple tasks including classification, question answering, and coreference resolution.</p>

  </div>
</details>
<p><br /></p>

<h3 id="gshard">GShard</h3>

<p><img src="/assets/blog_assets/evolution_of_llms/gshard_abstract.webp" alt="Image of Gshard" /></p>

<blockquote>
  <p>Link to paper: <a href="https://arxiv.org/abs/2006.16668">GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding</a></p>
</blockquote>

<p>https://www.youtube.com/watch?v=1VdEw_mGjFk&amp;ab_channel=YannicKilcher</p>

<details>

<summary>Quick Summary</summary>
<div>

    <p>GShard addresses critical scaling challenges in training extremely large neural network models. The paper introduces a module that enables efficient training of models with hundreds of billions to trillions of parameters through:</p>

    <ol>
      <li>
        <p><strong>Conditional computation</strong> - Using Sparsely-Gated Mixture-of-Experts (MoE) layers where only a subset of the model is activated for each input, allowing computation to scale sublinearly with model size</p>
      </li>
      <li>
        <p><strong>Automatic sharding</strong> - A separation between model description and parallelization implementation through simple annotation APIs that allow the XLA compiler to automatically partition computation across thousands of accelerators</p>
      </li>
      <li>
        <p><strong>Single Program Multiple Data (SPMD)</strong> - A compiler technique that generates a single program to run on all devices, keeping compilation time constant regardless of the number of devices</p>
      </li>
    </ol>

    <p>The effectiveness of GShard is demonstrated through multilingual machine translation experiments, where they trained a 600 billion parameter Transformer model with MoE layers on 2048 TPU v3 accelerators in just 4 days. This model achieved superior translation quality across 100 languages compared to both bilingual baselines and dense Transformer models, while using less computational resources.</p>

    <p>Key benefits of the approach include:</p>

    <ul>
      <li>Sublinear scaling of computation relative to model size</li>
      <li>Constant memory usage per device as model size increases</li>
      <li>Efficient training with little communication overhead</li>
      <li>Easy-to-use APIs that separate model description from parallelization implementation</li>
    </ul>

  </div>
</details>
<p><br /></p>

<h3 id="rag-retrieval-augmented-generation">RAG (Retrieval-Augmented Generation)</h3>

<p><img src="/assets/blog_assets/evolution_of_llms/RAG_abstract.webp" alt="Image of RAG" /></p>

<blockquote>
  <p>Link to paper: <a href="https://arxiv.org/abs/2005.11401">Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks</a></p>
</blockquote>

<details>

<summary>Quick Summary</summary>
<div>

    <p>This paper introduces RAG (Retrieval-Augmented Generation), a hybrid model architecture that combines the strengths of parametric memory (knowledge stored in neural network parameters) and non-parametric memory (knowledge stored in an external database that can be retrieved).</p>

    <p>The key innovation is a framework where:</p>

    <ol>
      <li>A retriever component fetches relevant passages from a large corpus (Wikipedia)</li>
      <li>A generator component (BART) uses both the input query and retrieved passages to produce outputs</li>
      <li>The entire pipeline is trained end-to-end, treating the retrieved documents as latent variables</li>
    </ol>

    <p>The authors explore two model variants:</p>

    <ul>
      <li>RAG-Sequence: uses the same retrieved document for generating the entire output sequence</li>
      <li>RAG-Token: can use different documents for generating different tokens in the output</li>
    </ul>

    <p>They evaluate RAG on knowledge-intensive tasks including open-domain QA, fact verification, and knowledge-grounded generation, achieving state-of-the-art results on several benchmarks. One particularly interesting aspect is that RAG’s non-parametric memory can be easily updated (by changing the retrieval corpus) without retraining the model.</p>

  </div>
</details>
<p><br /></p>

<h3 id="big-bird">Big Bird</h3>

<p><img src="/assets/blog_assets/evolution_of_llms/big_bird_abstract.webp" alt="Image of Big Bird" /></p>

<blockquote>
  <p>Link to paper: <a href="https://arxiv.org/abs/2007.14062">Big Bird: Transformers for Longer Sequences</a></p>
</blockquote>

<details>

<summary>Quick Summary</summary>
<div>

    <p>The paper introduces BigBird, a sparse attention mechanism for transformer models that reduces the quadratic dependency on sequence length to linear, enabling the processing of sequences up to 8x longer than previously possible with similar hardware.</p>

    <p><strong>Key Innovations:</strong></p>

    <p>BigBird’s sparse attention mechanism consists of three main components:</p>

    <ol>
      <li><strong>Global tokens</strong> - A set of g tokens that attend to all parts of the sequence</li>
      <li><strong>Window attention</strong> - All tokens attend to a set of w local neighboring tokens</li>
      <li><strong>Random attention</strong> - All tokens attend to a set of r random tokens</li>
    </ol>

    <p><strong>Theoretical Contributions:</strong></p>

    <p>The authors provide theoretical guarantees for BigBird, showing that:</p>

    <ol>
      <li>It’s a universal approximator of sequence functions</li>
      <li>It’s Turing complete, preserving the expressive properties of full attention models</li>
      <li>Their theoretical analysis reveals the benefits of global tokens for maintaining expressivity</li>
    </ol>

    <p><strong>Experimental Results:</strong></p>

    <p>BigBird shows significant improvements in tasks requiring longer contexts:</p>

    <ol>
      <li><strong>Question Answering</strong> - Achieves state-of-the-art results on various datasets (HotpotQA, Natural Questions, TriviaQA, WikiHop)</li>
      <li><strong>Document Summarization</strong> - Outperforms previous methods on long document summarization tasks (Arxiv, PubMed, BigPatent)</li>
      <li><strong>Genomics Applications</strong> - Novel application to DNA sequences, improving performance on promoter region prediction (99.9% F1) and chromatin profile prediction</li>
    </ol>

    <p><strong>Technical Details:</strong></p>

    <ul>
      <li>The paper addresses implementation details for efficiently computing the sparse attention on GPUs/TPUs through “blockification” of the attention pattern</li>
      <li>The authors prove there’s “no free lunch” - showing a natural task where sparse attention mechanisms require polynomially more layers compared to full attention</li>
      <li>Their approach balances theoretical guarantees with practical efficiency</li>
    </ul>

  </div>
</details>
<p><br /></p>

<p><strong>Problem</strong></p>

<p>”””
Unfortunately, one of their core limitations is the
quadratic dependency (mainly in terms of memory) on the sequence length due to
their full attention mechanism.
“””</p>

<p><strong>Solution</strong></p>

<p>”””
, BIGBIRD, a sparse
attention mechanism that reduces this quadratic dependency to linear
“””</p>

<p>https://huggingface.co/blog/big-bird</p>

<h3 id="gpt-3">GPT-3</h3>

<p><img src="/assets/blog_assets/evolution_of_llms/gpt3_abstract.webp" alt="Image of gpt-3" /></p>

<blockquote>
  <p>Link to paper: <a href="https://arxiv.org/abs/2005.14165">Language Models are Few-Shot Learners</a></p>
</blockquote>

<ul>
  <li>In-context learning</li>
  <li>Few-shot capabilities</li>
  <li>Scaling laws discovery</li>
  <li>Batch size scaling</li>
</ul>

<details>

<summary>Quick Summary</summary>
<div>

    <p><strong>Key Contributions</strong></p>

    <p>This paper demonstrates how scaling up language models to unprecedented sizes (175B parameters, 10x larger than previous models) enables significant improvements in few-shot learning capabilities. The authors show that large language models can perform tasks with few or no examples through “in-context learning,” where the model adapts to new tasks simply by being conditioned on examples in its prompt, without parameter updates.</p>

    <p><strong>Main Findings</strong></p>

    <ol>
      <li>
        <p><strong>Scaling Laws</strong>: Performance on various tasks improves smoothly with model size, following predictable power-law scaling trends.</p>
      </li>
      <li>
        <p><strong>Few-Shot Learning</strong>: GPT-3 can perform impressively on numerous tasks with just a few examples in the context, sometimes matching or approaching state-of-the-art fine-tuned models.</p>
      </li>
      <li>
        <p><strong>Zero-Shot and One-Shot</strong>: Even with no examples (zero-shot) or just one example (one-shot), GPT-3 shows remarkable capabilities.</p>
      </li>
      <li>
        <p><strong>Versatility</strong>: GPT-3 demonstrates strong performance across a wide range of NLP tasks including question answering, translation, common sense reasoning, reading comprehension, and more.</p>
      </li>
      <li>
        <p><strong>Emergent Abilities</strong>: Certain capabilities like arithmetic, novel word usage, and unscrambling words emerge more strongly at the largest model sizes, suggesting qualitative improvements beyond simple scaling.</p>
      </li>
    </ol>

    <p><strong>Key Results Across Task Categories</strong></p>

    <ul>
      <li><strong>Language Modeling</strong>: Sets new SOTA on Penn Tree Bank perplexity (20.5)</li>
      <li><strong>Cloze and Completion Tasks</strong>: Substantial improvements on LAMBADA (86.4% accuracy)</li>
      <li><strong>Question Answering</strong>: Competitive with fine-tuned systems on TriviaQA (71.2%)</li>
      <li><strong>Translation</strong>: Approaches SOTA unsupervised NMT results</li>
      <li><strong>Winograd-Style Tasks</strong>: Strong performance (88.6% on Winograd, 77.7% on Winogrande)</li>
      <li><strong>Common Sense Reasoning</strong>: State-of-the-art on PIQA (82.8%)</li>
      <li><strong>Reading Comprehension</strong>: Strong results on CoQA (85.0 F1)</li>
      <li><strong>SuperGLUE</strong>: Competitive with fine-tuned BERT-Large</li>
    </ul>

    <p><strong>Limitations</strong></p>

    <p>The authors transparently address several limitations:</p>

    <ol>
      <li>
        <p>GPT-3 still struggles with some tasks requiring complex reasoning, bidirectional context, or specialized knowledge.</p>
      </li>
      <li>
        <p>The model shows some biases in gender, race, and religion reflective of its training data.</p>
      </li>
      <li>
        <p>Even at this scale, sample efficiency during pre-training is much less than human learning.</p>
      </li>
      <li>
        <p>Some tasks still show a large gap between few-shot performance and fine-tuned models.</p>
      </li>
    </ol>

    <p><strong>Broader Impacts</strong></p>

    <p>The paper discusses potential misuse concerns and ethics issues, including biases in the model and potential for generating misleading content. The authors conducted experiments showing that humans can distinguish GPT-3-generated news articles from human-written ones only at chance levels.</p>

    <p><strong>Significance</strong></p>

    <p>This work represents a paradigm shift in how we think about language models - rather than fine-tuning smaller models for specific tasks, it suggests that scaling up models enables general in-context learning abilities that can be applied to many tasks without task-specific training.</p>

  </div>
</details>
<p><br /></p>

<h3 id="rethinking-attention-with-performers">Rethinking Attention with Performers</h3>

<p><img src="/assets/blog_assets/evolution_of_llms/longformer_abstract.webp" alt="Image of Performer" /></p>

<blockquote>
  <p>Link to paper: <a href="https://arxiv.org/abs/2009.14794v4">Rethinking Attention with Performers</a></p>
</blockquote>

<p>https://medium.com/analytics-vidhya/paper-explained-rethinking-attention-with-performers-b207f4bf4bc5</p>

<p>https://www.youtube.com/watch?v=xJrKIPwVwGM&amp;ab_channel=YannicKilcher</p>

<details>

<summary>Quick Summary</summary>
<div>

    <p>This 2021 ICLR paper introduces the Performer, a Transformer architecture that can estimate regular (softmax) attention with provable accuracy while achieving linear (rather than quadratic) space and time complexity. The key innovation is the FAVOR+ (Fast Attention Via positive Orthogonal Random features) mechanism, which enables efficient approximation of softmax attention kernels without assumptions about sparsity or low-rankness.</p>

    <p>The paper makes several key contributions:</p>

    <ol>
      <li>A new method for approximating softmax attention using positive orthogonal random features</li>
      <li>Linear-time complexity attention mechanism that’s fully compatible with regular Transformers</li>
      <li>Strong theoretical guarantees on the quality of the approximation</li>
      <li>The ability to efficiently model kernelizable attention mechanisms beyond softmax</li>
    </ol>

    <p>The authors demonstrate the Performer’s effectiveness on diverse tasks from pixel prediction to protein sequence modeling, showing competitive results with other efficient attention methods while enabling much longer sequence lengths.</p>

  </div>
</details>
<p><br /></p>

<h3 id="t5">T5</h3>

<p><img src="/assets/blog_assets/evolution_of_llms/t5_abstract.webp" alt="Image of Longformer" /></p>

<blockquote>
  <p>Link to paper: <a href="https://arxiv.org/abs/1910.10683">Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</a></p>
</blockquote>

<ul>
  <li>Encoder-decoder architecture</li>
  <li>Unified text-to-text framework</li>
  <li>Span corruption</li>
  <li>Multi-task pre-training</li>
</ul>

<p>”””
Link: https://huggingface.co/docs/transformers/model_doc/t5
Family: Transformer
Pretraining Architecture: Encoder/Decoder
Pretraining Task: DAE
Extension: Same as original Transformer with some additions such as relative positional embeddings like Transformer XL
Application: General language tasks including machine translation, question answering, abstractive summarization, and text classification
Date (of first known publication): 10/2019
Num. Params: 11 B (up to)
Corpus: Colossal Clean Crawled Corpus (C4) — Cleaned up version of the Common Crawl dataset — 750 GB
License: Open, Apache-2.0
Lab: Google
“””</p>

<details>

<summary>Quick Summary</summary>
<div>

    <p>This 2020 paper by Raffel et al. introduces the “Text-to-Text Transfer Transformer” (T5), a unified approach to transfer learning for NLP tasks. The authors convert all text-based language problems into a consistent text-to-text format, where both inputs and outputs are always text strings. This allows them to use the same model, loss function, and training procedure across diverse tasks.</p>

    <p>The paper presents a comprehensive empirical study examining various aspects of transfer learning for NLP, including:</p>

    <ol>
      <li>Model architectures</li>
      <li>Pre-training objectives</li>
      <li>Pre-training datasets</li>
      <li>Transfer approaches</li>
      <li>Scaling effects</li>
    </ol>

    <p>They introduce the “Colossal Clean Crawled Corpus” (C4), a massive dataset of cleaned web text for pre-training. By combining insights from their systematic study with scale (training models up to 11 billion parameters), they achieve state-of-the-art results on many NLP benchmarks including GLUE, SuperGLUE, SQuAD, and CNN/DailyMail summarization.</p>

    <p>The T5 approach demonstrates the effectiveness of a unified text-to-text framework for transfer learning across diverse NLP tasks, showing that with the right architecture and sufficient scale, a single approach can excel across the NLP landscape.</p>

  </div>
</details>
<p><br />
https://cameronrwolfe.substack.com/p/t5-text-to-text-transformers-part</p>

<h3 id="measuring-massive-multitask-language-understanding">Measuring Massive Multitask Language Understanding</h3>

<p><img src="/assets/blog_assets/evolution_of_llms/mmlu_abstract.webp" alt="Image of Longformer" /></p>

<blockquote>
  <p>Link to paper: <a href="https://arxiv.org/abs/2009.03300">Measuring Massive Multitask Language Understanding</a></p>
</blockquote>

<p>(benchmark)</p>

<details>

<summary>Quick Summary</summary>
<div>

    <p>This 2021 paper introduces a comprehensive benchmark for evaluating language models’ multitask capabilities across 57 diverse subjects. The authors (Hendrycks et al.) created a test covering fields like STEM, humanities, social sciences, and professional domains at varying levels of difficulty, from elementary to advanced professional knowledge.</p>

    <p>The key findings show that while smaller models performed near random chance (25% on multiple-choice questions), the largest GPT-3 model (175B parameters) achieved 43.9% accuracy - significantly better than random but still far below expert-level performance (estimated at ~90%). Performance was notably lopsided across subjects, with calculation-heavy topics like physics and mathematics showing poor results, as did socially important subjects like law and morality.</p>

    <p>The research highlights several important insights about large language models circa 2021:</p>

    <ol>
      <li>Models struggled with procedural knowledge vs. declarative knowledge</li>
      <li>Models were often miscalibrated (not knowing when they don’t know)</li>
      <li>Even the largest models failed to exhibit expert-level performance in any subject</li>
      <li>The benchmark required diverse world knowledge beyond commonsense reasoning</li>
    </ol>

    <p>This work provided an important evaluation framework showing that while large language models were beginning to demonstrate impressive capabilities, they still had fundamental limitations in their ability to learn and apply specialized knowledge.</p>

  </div>
</details>
<p><br /></p>

<h3 id="zero-zero-redundancy-optimizer">ZeRO (Zero Redundancy Optimizer)</h3>

<p>https://dev.to/lewis_won/data-parallelism-4g3m</p>

<p><img src="/assets/blog_assets/evolution_of_llms/zero_abstract.webp" alt="Image of ZeRO" /></p>

<blockquote>
  <p>Link to paper: <a href="https://arxiv.org/abs/1910.02054">ZeRO: Memory Optimizations Toward Training Trillion Parameter Models</a></p>
</blockquote>

<ul>
  <li>Memory optimization for distributed training</li>
</ul>

<details>

<summary>Quick Summary</summary>
<div>

    <p>This paper introduces ZeRO (Zero Redundancy Optimizer), a memory optimization system designed to overcome memory limitations in training extremely large deep learning models.</p>

    <p><strong>Key Contributions:</strong></p>

    <ul>
      <li>ZeRO enables training models with billions to trillions of parameters by eliminating memory redundancies in data-parallel and model-parallel training</li>
      <li>The approach maintains high computational efficiency while drastically reducing memory requirements</li>
      <li>ZeRO includes different optimization stages that can provide up to linear memory reduction with the number of devices</li>
      <li>The authors demonstrate training models with over 100B parameters with super-linear speedup on 400 GPUs</li>
      <li>ZeRO enables training large models (up to 13B parameters) without model parallelism, making it more accessible</li>
      <li>ZeRO powered the creation of Turing-NLG (17B parameters), which at the time was the world’s largest language model</li>
    </ul>

    <p>The paper presents an elegant solution to a fundamental bottleneck in large model training, showing how clever memory management can effectively scale model size proportional to the number of available devices.</p>

  </div>
</details>
<p><br />
https://oracle-oci-ocas.medium.com/zero-redundancy-optimizers-a-method-for-training-machine-learning-models-with-billion-parameter-472e8f4e7a5b</p>

<p>https://www.youtube.com/watch?v=KgoHyMGpxBU&amp;ab_channel=nPlan</p>

<p>https://alessiodevoto.github.io/parallelism/ -&gt; Great blog on ZeRO</p>

<p>https://www.microsoft.com/en-us/research/blog/zero-deepspeed-new-system-optimizations-enable-training-models-with-over-100-billion-parameters/</p>

<p>https://huggingface.co/docs/transformers/v4.15.0/parallelism</p>

<h3 id="electra">ELECTRA</h3>

<p><img src="/assets/blog_assets/evolution_of_llms/" alt="Image of ELECTRA" /></p>

<blockquote>
  <p>Link to paper: <a href="https://arxiv.org/abs/2003.10555">ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators</a></p>
</blockquote>

<p>Google’s model that used a discriminative approach instead of masked Language Modeling, providing more efficient training As noted, “Electra deploys a ‘Masked Language Modeling’ approach that masks certain words and trains the model to predict them. Additionally, Electra incorporates a ‘Discriminator’ network that aids in comprehending language without the need to memorize the training data.”</p>

<details>

<summary>Quick Summary</summary>
<div>

    <p>ELECTRA presents a more efficient alternative to masked Language Modeling (MLM) pre-training methods like BERT. Instead of masking tokens and training a model to predict the original ones, ELECTRA proposes “replaced token detection” - a discriminative task where:</p>

    <ol>
      <li>A small generator model replaces some tokens with plausible alternatives</li>
      <li>A discriminator model (ELECTRA) learns to distinguish between original and replaced tokens</li>
    </ol>

    <p>The key advantages of this approach are:</p>

    <ul>
      <li>It’s more computationally efficient since the model learns from all input tokens rather than just the 15% that are masked</li>
      <li>It achieves better downstream performance given the same compute budget</li>
      <li>It works particularly well for smaller models, enabling high-quality language models to be trained on a single GPU</li>
    </ul>

    <p>The authors demonstrate ELECTRA’s efficiency by showing it outperforms BERT, GPT, and other models when controlling for compute. For example, ELECTRA-Small trained on one GPU for 4 days outperforms GPT (trained with 30x more compute) on the GLUE benchmark.</p>

  </div>
</details>
<p><br /></p>

<h3 id="switch-transformer">Switch Transformer</h3>

<p><a href="https://arxiv.org/abs/2101.03961">paper</a></p>

<p>Google’s early mixture-of-experts approach that demonstrated trillion-parameter scale was possible</p>

<details>

<summary>Quick Summary</summary>
<div>

    <p>This paper introduces the Switch Transformer, an architecture that simplifies the Mixture of Experts (MoE) approach to create more efficient and scalable language models. The key innovation is routing tokens to exactly one expert (rather than multiple experts) at each layer, which the authors call “switching.” This approach:</p>

    <ol>
      <li>Significantly increases parameter count while keeping computational costs fixed</li>
      <li>Achieves better performance per FLOP than dense models</li>
      <li>Offers training speedups of up to 7x compared to T5 models with the same computational budget</li>
      <li>Scales effectively to trillion-parameter models</li>
    </ol>

    <p>The authors demonstrate that even with as few as two experts, their approach shows improvements over standard Transformers. They also introduce techniques to improve training stability, including selective precision for routing operations and expert dropout for fine-tuning.</p>

  </div>
</details>
<p><br /></p>

<h3 id="scaling-laws">Scaling Laws</h3>

<p><a href="https://arxiv.org/abs/2001.08361">paper</a></p>

<p>OpenAI’s publication on the mathematical relationships between model size, dataset size, and computational budget demonstrated predictable patterns for improving performance This was part of the GPT-3 research which showed “that scaling up language models greatly improves task-agnostic, few-shot performance.”</p>

<details>

<summary>Quick Summary</summary>
<div>

    <p>This 2020 paper by Kaplan, McCandlish, et al. from OpenAI demonstrates that language model performance follows remarkably consistent power-law relationships across multiple dimensions of scale. The authors show that model loss decreases as a power-law function of three key factors: model size (number of parameters), dataset size, and amount of compute used for training.</p>

    <p>Their key findings include:</p>

    <ol>
      <li>Performance improves smoothly and predictably as model size, dataset size, or compute increases</li>
      <li>Model architecture details matter far less than scale factors</li>
      <li>Larger models are more sample-efficient than smaller models</li>
      <li>When optimizing for compute efficiency, it’s better to train very large models and stop before convergence</li>
      <li>The relationship between these factors allows optimal allocation of resources for a given compute budget</li>
    </ol>

    <p>The paper’s most striking insight is that these relationships hold across several orders of magnitude, suggesting fundamental scaling properties of neural language models that could inform how we approach building larger and more capable models.</p>

  </div>
</details>
<p><br /></p>

<h2 id="2021-instruction-tuning-and-alignment">2021: Instruction Tuning and Alignment</h2>

<h3 id="roformer-enhanced-transformer-with-rotary-position-embedding">RoFormer: Enhanced Transformer with Rotary Position Embedding</h3>

<p><a href="https://arxiv.org/abs/2104.09864">paper</a></p>

<details>

<summary>Quick Summary</summary>
<div>

    <p>This paper introduces RoFormer (Rotary Position Embedding), a novel technique for encoding positional information in transformer models. The key innovation is representing token positions using rotation matrices, which elegantly captures both absolute position information and relative position relationships between tokens. Unlike previous approaches that often add position embeddings to token representations, RoFormer multiplies token representations by rotation matrices, preserving their norms while encoding position.</p>

    <p>The authors demonstrate that RoFormer has several compelling properties:</p>

    <ul>
      <li>It naturally handles variable sequence lengths</li>
      <li>It models decreasing attention between tokens as their distance increases</li>
      <li>It can be integrated with linear self-attention variants, unlike many other position embedding schemes</li>
      <li>It yields improved performance on long text classification and machine translation tasks</li>
    </ul>

    <p>This approach appears to be a mathematically elegant reformulation of positional encoding in transformers that addresses limitations of previous methods while maintaining or improving performance.</p>

  </div>
</details>
<p><br />
https://huggingface.co/blog/designing-positional-encoding</p>

<h3 id="efficient-large-scale-language-model-training-on-gpu-clusters-using-megatron-lm">Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM</h3>

<p><a href="https://arxiv.org/abs/2104.04473">paper</a></p>

<details>

<summary>Quick Summary</summary>
<div>

    <p>This 2021 paper from NVIDIA, Stanford, and Microsoft researchers addresses the critical challenge of efficiently training extremely large language models (LLMs) with billions to trillions of parameters. The authors present a combined parallelization approach they call PTD-P that integrates:</p>

    <ol>
      <li><strong>Pipeline Parallelism</strong> (P): Distributing layers across GPUs</li>
      <li><strong>Tensor Parallelism</strong> (T): Splitting individual operations within layers</li>
      <li><strong>Data Parallelism</strong> (D): Processing different batches on different GPUs</li>
    </ol>

    <p>The paper demonstrates impressive scaling to 3072 NVIDIA A100 GPUs, achieving 502 petaFLOP/s performance (52% of theoretical peak) when training a 1 trillion parameter model. They introduce an interleaved pipeline schedule that improves throughput by over 10% and carefully analyze the tradeoffs between different parallelization strategies.</p>

    <p>Their approach makes training trillion-parameter models practical (estimated 3 months for full training), which was a significant advancement at publication time. The work includes both theoretical analysis and empirical validation of their proposed methods.</p>

  </div>
</details>
<p><br /></p>

<h3 id="transcending-scaling-laws-with-01-extra-compute">Transcending Scaling Laws with 0.1% Extra Compute</h3>

<p><a href="https://arxiv.org/abs/2210.11399">paper</a></p>

<details>

<summary>Quick Summary</summary>
<div>

    <p>This 2022 paper from Google introduces UL2R (UL2 Restore), a method that significantly improves large language models with minimal additional computation. The key idea is remarkably simple yet effective: taking a pre-trained language model (like PaLM) and continuing its training for a small number of steps using a mixture of different training objectives called “mixture-of-denoisers.”</p>

    <p>The authors demonstrate that applying UL2R to PaLM (creating “U-PaLM”) yields impressive results:</p>

    <ul>
      <li>With just 0.1% additional compute, they achieve significant performance improvements across various NLP tasks</li>
      <li>At the 540B parameter scale, U-PaLM achieves performance equivalent to the final PaLM model with approximately half the computational budget (saving ~4.4 million TPUv4 hours)</li>
      <li>U-PaLM demonstrates “emergent abilities” on challenging tasks, sometimes achieving strong performance at smaller model scales (62B) compared to the original model at larger scales (540B)</li>
      <li>The technique enables additional capabilities like bidirectional infilling, which allows the model to fill in blanks in the middle of text (not just generate continuations)</li>
    </ul>

    <p>This approach is particularly interesting because it challenges conventional wisdom about scaling laws by showing that strategic changes to training objectives can significantly improve efficiency beyond what simply scaling up with more compute would achieve.</p>

  </div>
</details>
<p><br /></p>

<h3 id="improving-language-models-by-retrieving-from-trillions-of-tokens">Improving language models by retrieving from trillions of tokens</h3>

<p><a href="https://arxiv.org/abs/2112.04426">paper</a></p>

<details>

<summary>Quick Summary</summary>
<div>

    <p>This 2021 paper from DeepMind introduces Retrieval-Enhanced Transformer (RETRO), a novel approach to Language Modeling that enhances traditional transformer architectures with retrieval capabilities from massive text databases.</p>

    <p><strong>Key Innovations:</strong></p>

    <ul>
      <li>RETRO models can retrieve from databases with trillions of tokens, effectively scaling the data available to the model by an order of magnitude beyond what can be consumed during training</li>
      <li>The architecture uses a “chunked cross-attention” mechanism to efficiently incorporate retrieved passages into the language model</li>
      <li>RETRO achieves performance comparable to models with 25× more parameters (e.g., similar to GPT-3 and Jurassic-1 despite using far fewer parameters)</li>
      <li>The approach effectively creates a semi-parametric model, combining the strengths of parametric models with explicit retrieval</li>
    </ul>

    <p><strong>Significance:</strong></p>

    <p>The paper demonstrates that retrieval offers an orthogonal scaling dimension to simply increasing model parameters, potentially providing a more efficient path to improving language model capabilities. It shows strong performance on downstream tasks like question answering while maintaining the flexibility to be used with or without retrieval at evaluation time.</p>

  </div>
</details>
<p><br /></p>

<h3 id="clip">CLIP</h3>

<p>https://openai.com/index/clip/
Briefly talk about</p>

<p>I have talked more extensively about it in this blog, so I will be skipping it here.</p>

<blockquote>
  <p>I mentioned it because it was still a very significant work and you should be aware that it came out in this period of time</p>
</blockquote>

<h3 id="dall-e">Dall-e</h3>

<p>Briefly talk about</p>

<p>I have an entire blog dedicated to diffusion models, consdier checking that out for more information on the topic.</p>

<blockquote>
  <p>From now on this blog will solely talk about developments in LLMs, for more general GenAI evolution. I will be writing another blog.</p>
</blockquote>

<h3 id="fsdp">FSDP</h3>

<p><a href="https://arxiv.org/abs/2304.11277">paper</a></p>

<details>

<summary>Quick Summary</summary>
<div>

    <p>This paper introduces PyTorch’s Fully Sharded Data Parallel (FSDP), an industry-grade solution for training large-scale deep learning models. The technique addresses a critical challenge in the field: enabling the training of models that are too large to fit on a single GPU device.</p>

    <p>The key innovation of FSDP is that it decomposes models into smaller units and shards parameters across multiple devices, materializing the full parameters only when needed during computation. The paper details how FSDP has been carefully co-designed with PyTorch’s core components (tensor implementation, dispatcher system, and CUDA memory caching allocator) to provide efficient training while maintaining user-friendly experiences.</p>

    <p>The authors explain various optimizations in FSDP including deferred initialization, configurable sharding strategies, communication-computation overlapping, and memory management techniques. Their evaluations show that FSDP achieves comparable performance to Distributed Data Parallel (DDP) for smaller models while enabling training of significantly larger models with near-linear TFLOPS scaling.</p>

  </div>
</details>
<p><br />
https://engineering.fb.com/2021/07/15/open-source/fsdp/</p>

<h3 id="humaneval">HumanEval</h3>

<p><a href="Evaluating Large Language Models Trained on Code">paper</a></p>

<details>

<summary>Quick Summary</summary>
<div>

    <p>This paper introduces Codex, a GPT language model fine-tuned on publicly available code from GitHub, and evaluates its capabilities in generating functional Python code. Here’s a high-level summary:</p>

    <p>The authors present Codex, a model derived from GPT and fine-tuned on GitHub code repositories. They evaluate Codex’s ability to generate working code by creating HumanEval, a benchmark consisting of 164 hand-written programming problems with unit tests. Unlike previous evaluations based on similarity metrics like BLEU score, they focus on functional correctness - whether the generated code passes the test cases.</p>

    <p>Key findings:</p>

    <ul>
      <li>Codex-12B (12 billion parameters) solves 28.8% of the problems with a single generation attempt</li>
      <li>When allowed to sample 100 solutions per problem, Codex solves 70.2% of problems</li>
      <li>They also created a variant (Codex-S) further fine-tuned on correctly implemented standalone functions, which improves performance to 37.7% on single attempts</li>
      <li>The paper discusses limitations including difficulty with complex docstrings and binding operations to variables</li>
      <li>The authors conduct a thorough analysis of potential broader impacts including safety, security, and economic implications</li>
    </ul>

    <p>This represents a significant step in code generation capabilities, moving beyond simple pattern matching to more sophisticated problem-solving, though still with substantial limitations.</p>

  </div>
</details>
<p><br /></p>

<h3 id="lora">LoRA</h3>

<p><a href="https://arxiv.org/abs/2106.09685">paper</a></p>

<details>

<summary>Quick Summary</summary>
<div>

    <p>This 2021 paper by Hu et al. from Microsoft introduces Low-Rank Adaptation (LoRA), an efficient fine-tuning method for large language models. The key innovation is freezing the pre-trained model weights while adding trainable low-rank decomposition matrices to each layer of the Transformer architecture.</p>

    <p>The main benefits of LoRA include:</p>

    <ol>
      <li>Drastically reducing the number of trainable parameters (by up to 10,000x compared to full fine-tuning)</li>
      <li>Reducing GPU memory requirements (by up to 3x)</li>
      <li>Allowing quick task-switching by only swapping the small LoRA modules</li>
      <li>No additional inference latency compared to fully fine-tuned models</li>
      <li>Competitive or better performance than full fine-tuning across various models (RoBERTa, DeBERTa, GPT-2, and GPT-3)</li>
    </ol>

    <p>The core insight is that while language models are heavily over-parameterized, the changes during adaptation have a low “intrinsic rank.” LoRA exploits this by representing weight updates as low-rank decompositions (BA, where B and A are small matrices). The authors show that surprisingly small rank values (r=1 to r=4) often suffice for strong performance, even for models as large as GPT-3 175B.</p>

    <p>The paper includes extensive empirical validation across multiple models and tasks, an analysis of why low-rank updates work well, and discussion of the relationship between the original weights and LoRA updates.</p>

  </div>
</details>
<p><br /></p>

<h3 id="self-instruct-aligning-language-models-with-self-generated-instructions">Self-Instruct: Aligning Language Models with Self-Generated Instructions</h3>

<p><a href="https://arxiv.org/abs/2212.10560">paper</a></p>

<details>

<summary>Quick Summary</summary>
<div>

    <p>This 2023 ACL paper by Wang et al. introduces SELF-INSTRUCT, a framework that improves instruction-following capabilities of pretrained language models by bootstrapping off their own generations. The key innovation is creating a semi-automated process that generates high-quality instruction data without extensive human annotation.</p>

    <p><strong>Key Points:</strong></p>

    <ol>
      <li>
        <p><strong>The Problem</strong>: Instruction-tuned language models depend heavily on human-written instruction data, which is limited in quantity, diversity, and creativity.</p>
      </li>
      <li>
        <p><strong>The Solution</strong>: SELF-INSTRUCT bootstraps a model’s own capabilities to generate diverse instruction data, including:</p>

        <ul>
          <li>Task instructions</li>
          <li>Input-output examples</li>
          <li>Classification task handling</li>
        </ul>
      </li>
      <li>
        <p><strong>The Process</strong>:</p>

        <ul>
          <li>Starts with just 175 seed tasks</li>
          <li>Iteratively prompts the model to generate new instructions</li>
          <li>Generates corresponding input-output instances</li>
          <li>Filters invalid or similar instructions</li>
          <li>Uses the generated data to finetune the original model</li>
        </ul>
      </li>
      <li>
        <p><strong>Results</strong>:</p>

        <ul>
          <li>Applied to vanilla GPT3, resulting in 52K instructions with 82K instances</li>
          <li>Demonstrated 33% absolute improvement over original model on SUPER-NATURALINSTRUCTIONS</li>
          <li>Performance comparable to InstructGPT001, which used private user data and human annotations</li>
          <li>Only a 5% performance gap behind InstructGPT001 on expert-written novel instructions</li>
        </ul>
      </li>
      <li>
        <p><strong>Significance</strong>: Provides an almost annotation-free method for aligning pretrained language models with instructions, enabling better instruction-following capabilities without expensive human annotation.</p>
      </li>
    </ol>

    <p>This work is particularly important because it addresses a key limitation in scaling instruction-tuned models - the dependency on human-written instruction data. By enabling models to generate their own diverse instruction data, SELF-INSTRUCT offers a path to more general and capable instruction-following AI systems.</p>

  </div>
</details>
<p><br /></p>

<h3 id="palm">PaLM</h3>

<p><a href="PaLM: Scaling Language Modeling with Pathways">paper</a></p>

<ul>
  <li>Pathways system</li>
  <li>Scaled dot product attention</li>
  <li>Multi-query attention</li>
  <li>Parallel training techniques</li>
</ul>

<p>”””
Link: https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html
Family: Transformer
Pretraining Architecture: Decoder
Pretraining Task: LM
Extension: Palm uses a typical decoder-only Transformer architecture, but adds quite a few extensions: SwiGLU activations, parallel layers, multi-query attention, RoPE embeddings, Shared Input-Output Embeddings, no biases, and a 256k SentencePiece vocabulary generated from the training data.
Application: PalM is designed as a general purpose language model with applicability to hundreds of different language tasks
Date (of first known publication): 04/2022
Num. Params: 540B
Corpus: 780B tokens from filtered webpages, books, Wikipedia, news articles, source code, and social media conversations. Code includes 24 programming languages.
License: Closed source, Accessible through API
Lab: Google
“””</p>

<details>

<summary>Quick Summary</summary>
<div>

    <p>This 2022 paper from Google Research introduces PaLM (Pathways Language Model), a 540-billion parameter autoregressive language model trained on 780 billion tokens of text. The key contributions include:</p>

    <ol>
      <li>
        <p><strong>Efficient scaling</strong>: PaLM demonstrates the first large-scale use of Google’s Pathways system, training across 6,144 TPU v4 chips with high efficiency (46.2% model FLOPS utilization).</p>
      </li>
      <li>
        <p><strong>State-of-the-art performance</strong>: PaLM achieves breakthrough performance across a wide range of natural language, reasoning, coding, and multilingual tasks, surpassing prior language models on 28 out of 29 widely-evaluated English NLP benchmarks.</p>
      </li>
      <li>
        <p><strong>Reasoning capabilities</strong>: When combined with chain-of-thought prompting, PaLM shows remarkable capabilities in multi-step reasoning tasks, matching or exceeding the fine-tuned state-of-the-art on various arithmetic and commonsense reasoning benchmarks.</p>
      </li>
      <li>
        <p><strong>Discontinuous improvements</strong>: For certain tasks, scaling from 62B to 540B parameters produced much larger improvements than scaling from 8B to 62B, suggesting emergent capabilities at larger scales.</p>
      </li>
      <li>
        <p><strong>Thorough analysis</strong>: The authors conduct extensive evaluations of memorization, dataset contamination, representational bias, and toxicity, providing a comprehensive understanding of the model’s strengths and limitations.</p>
      </li>
    </ol>

    <p>The paper contributes significantly to understanding how model scaling affects performance and demonstrates that performance improvements from scale had not plateaued as of 2022. The research also establishes a foundation for Pathways as an efficient ML scaling infrastructure at Google.</p>

  </div>
</details>
<p><br /></p>

<h3 id="gopher-deepmind">Gopher (DeepMind)</h3>

<p><a href="https://arxiv.org/abs/2112.11446">paper</a></p>

<ul>
  <li>280B parameter model released in December 2021 DeepMind introduced this model as a “280 billion parameter model” that was “evaluated on 152 diverse tasks, achieving state-of-the-art performance across the majority.”</li>
  <li>Demonstrated significant scaling benefits in reading comprehension and fact-checking</li>
  <li>Represented a major advancement in model scale from DeepMind</li>
</ul>

<details>

<summary>Quick Summary</summary>
<div>

    <p>This 2021 paper from DeepMind introduces Gopher, a 280 billion parameter autoregressive Transformer language model. The research team trained a family of models ranging from 44 million to 280 billion parameters on a custom dataset called MassiveText (a diverse collection of web pages, books, news articles, and code).</p>

    <p>The paper makes several key contributions:</p>

    <ol>
      <li>Detailed analysis of how performance scales with model size across 152 diverse tasks, showing that Gopher outperforms previous SOTA on 81% of tasks</li>
      <li>Discussion of where scaling works well (knowledge-intensive tasks like fact checking) and where it doesn’t (mathematical and logical reasoning)</li>
      <li>Extensive analysis of toxicity and bias in these models, including how these properties change with scale</li>
      <li>Exploration of using LLMs in dialogue settings</li>
      <li>Analysis of engineering considerations for training at scale, including infrastructure and optimization techniques</li>
    </ol>

    <p>The paper provides valuable insights into the capabilities and limitations of large language models circa 2021, predating many subsequent developments in the field but establishing important scaling trends and evaluation methodologies.</p>

  </div>
</details>
<p><br /></p>

<h3 id="megatron-turing-nlg">Megatron-Turing NLG</h3>

<p><a href="https://arxiv.org/abs/2201.11990">paper</a></p>

<ul>
  <li>530B parameter model announced in October 2021</li>
  <li>Combined Microsoft’s Turing and NVIDIA’s Megatron technologies</li>
  <li>Demonstrated advanced distributed training techniques</li>
  <li>Applied significant hardware optimization for large-scale training</li>
</ul>

<details>

<summary>Quick Summary</summary>
<div>

    <p>This paper presents Megatron-Turing NLG (MT-NLG), a 530 billion parameter autoregressive language model developed jointly by Microsoft and NVIDIA. At the time of publication (early 2022), this was the largest monolithic transformer-based language model ever trained. The paper focuses on three main aspects:</p>

    <ol>
      <li>
        <p><strong>Training Infrastructure</strong>: The authors detail their 3D parallelism approach, combining data, pipeline, and tensor-slicing parallelism to efficiently train at scale using DeepSpeed and Megatron frameworks.</p>
      </li>
      <li>
        <p><strong>Training Data and Process</strong>: The paper discusses their curated dataset comprising hundreds of billions of tokens, preprocessing techniques, and training recipes that improved optimization efficiency and stability.</p>
      </li>
      <li>
        <p><strong>Model Evaluation</strong>: The authors present extensive evaluation results showing MT-NLG’s superior performance on various NLP benchmarks in zero-shot, one-shot, and few-shot learning settings.</p>
      </li>
    </ol>

    <p>The model demonstrates impressive improvements in natural language understanding and generation capabilities, establishing new state-of-the-art results across several benchmarks. The authors also explore the model’s social biases and in-context learning abilities.</p>

  </div>
</details>
<p><br /></p>

<h2 id="2022-democratization">2022: Democratization</h2>

<h3 id="efficiently-scaling-transformer-inference">EFFICIENTLY SCALING TRANSFORMER INFERENCE</h3>

<p><a href="https://arxiv.org/pdf/2211.05102">paper</a></p>

<details>

<summary>Quick Summary</summary>
<div>

    <p>This 2022 paper from Google researchers addresses the crucial challenge of deploying large language models (LLMs) efficiently for inference. In particular, they focus on:</p>

    <ol>
      <li>
        <p><strong>Partitioning strategies</strong> for distributing large models (500B+ parameters) across multiple accelerator chips (TPU v4) that minimize communication costs while maximizing computational efficiency</p>
      </li>
      <li>
        <p><strong>Memory optimizations</strong>, especially utilizing multiquery attention to reduce KV cache memory requirements, enabling 32× longer context lengths</p>
      </li>
      <li>
        <p><strong>Low-level engineering optimizations</strong> including int8 quantization and communication/computation overlap techniques</p>
      </li>
    </ol>

    <p>The authors present an analytical model for selecting optimal partitioning strategies based on application requirements (latency vs. throughput), then empirically validate their approach using the PaLM family of models (8B, 62B, and 540B parameters). Their results demonstrate impressive achievements: 29ms per token latency for generation and 76% model FLOPS utilization (MFU) for processing input with 2048-token context on the PaLM 540B model.</p>

    <p>The research provides a clear framework for making partitioning decisions based on model characteristics and deployment requirements, advancing the practical deployment of massive language models.</p>

  </div>
</details>
<p><br />
“””
The primary goal of this paper is to provide a set of engineering principles for how best to partition a model in
order to scale Transformer inference. In other words, how is
the performance of different partitioning strategies affected
by changes in model size, sequence length, and number of
hardware chips? How does the optimal partitioning strategy
change when trading off between latency and throughput?
What is the intuitive and mathematical reasoning behind
these effects?</p>

<p>”””
https://rasa.com/blog/compressing-bert-for-faster-prediction-2/</p>

<h3 id="fast-inference-from-transformers-via-speculative-decoding">Fast Inference from Transformers via Speculative Decoding</h3>

<p><a href="https://arxiv.org/abs/2211.17192">paper</a></p>

<details>

<summary>Quick Summary</summary>
<div>

    <p>This paper introduces “speculative decoding,” a technique to accelerate inference from large autoregressive Transformer models without changing their architecture, training procedure, or output distribution.</p>

    <p>The key insight is that Language Modeling often contains easier subtasks that can be approximated by smaller, more efficient models. The authors use these smaller models to “speculate” on the next few tokens that the larger model would generate, and then run the larger model in parallel to verify these speculations.</p>

    <p>When the smaller model’s predictions match what the larger model would have produced, they accept multiple tokens at once, significantly reducing the number of sequential calls to the large model. The authors introduce a novel sampling method called “speculative sampling” that preserves the exact output distribution of the original model.</p>

    <p>Their experiments show 2-3x speedups for T5-XXL (11B parameters) without any changes to model outputs, and they analyze various smaller models as approximators, finding that models about two orders of magnitude smaller than the target model provide good trade-offs between accuracy and speed.</p>

  </div>
</details>
<p><br /></p>

<h3 id="chinchilla">Chinchilla</h3>

<p><a href="https://arxiv.org/abs/2203.15556">paper</a>
While the Chinchilla model itself wasn’t released until 2022, the research behind it began in 2021, establishing important scaling principles that:</p>

<ul>
  <li>Showed optimal token-to-parameter ratios should be approximately 20:1 This research found that “we need around 20 text tokens per parameter” for optimal training.</li>
  <li>Demonstrated many existing models were significantly undertrained</li>
  <li>Influenced the training methodology of subsequent models</li>
</ul>

<p>”””
Link: https://arxiv.org/abs/2203.15556
Family: GPT
Pretraining Architecture: Decoder
Pretraining Task: LM
Extension: Same as Gopher but with optimizations to reduce model size and therefore training/inference time with equal or superior performance
Application: Same as Gopher/GPT3
Date (of first known publication): 03/2022
Num. Params:70B
Corpus: Massive Text
License: Closed source.
Lab: Deepmind
“””</p>

<details>

<summary>Quick Summary</summary>
<div>

    <p>This 2022 paper by DeepMind (Hoffmann et al.) presents a significant finding that challenges previous assumptions about scaling large language models (LLMs).</p>

    <p>The authors discover that most large language models at the time (like GPT-3, Gopher, Jurassic-1) were significantly undertrained relative to their size. Through extensive experimentation with over 400 language models of various sizes trained on different amounts of data, they establish a key principle: <strong>for compute-optimal training, model size and training tokens should be scaled in equal proportions</strong>. This contradicts previous scaling laws from Kaplan et al. (2020), which suggested scaling model size more aggressively than training data.</p>

    <p>To validate their findings, they trained “Chinchilla,” a 70B parameter model on 1.4 trillion tokens, using the same compute budget as Gopher (280B parameters on 300B tokens). Chinchilla consistently outperformed much larger models like Gopher, GPT-3, and Megatron-Turing NLG across various benchmarks, achieving a state-of-the-art 67.5% accuracy on the MMLU benchmark.</p>

    <p>This work highlights the importance of balanced scaling between model size and training data, and explains why focusing solely on model size isn’t optimal. The paper has been highly influential in shaping how subsequent LLMs were developed.</p>

  </div>
</details>
<p><br /></p>

<h3 id="chain-of-thought-prompting">Chain-of-thought prompting</h3>

<p><a href="https://arxiv.org/abs/2201.11903">paper</a></p>

<details>

<summary>Quick Summary</summary>
<div>

    <p>This 2022 paper by Jason Wei et al. from Google Research introduces “chain-of-thought prompting,” a simple but powerful technique that enables large language models to perform complex reasoning tasks. The key insight is that by providing examples where the model sees step-by-step reasoning before giving an answer, the model learns to generate its own reasoning chains for new problems.</p>

    <p>The authors demonstrate that this ability emerges naturally in sufficiently large language models (like PaLM 540B) without any fine-tuning. The technique significantly improves performance on arithmetic, commonsense, and symbolic reasoning tasks. On some benchmarks like GSM8K (math word problems), the approach achieves state-of-the-art results, outperforming even fine-tuned models.</p>

    <p>What’s particularly interesting is that this reasoning ability is “emergent” - it only appears in models above a certain size threshold, and smaller models actually perform worse with chain-of-thought prompting than with standard prompting.</p>

  </div>
</details>
<p><br /></p>

<h3 id="instructgpt">InstructGPT</h3>

<p><a href="https://arxiv.org/abs/2203.02155">paper</a></p>

<p>https://openai.com/index/instruction-following/</p>

<ul>
  <li>RLHF pipeline <a href="https://huggingface.co/blog/rlhf">blog on the topic</a> &amp; <a href="https://wandb.ai/ayush-thakur/RLHF/reports/Understanding-Reinforcement-Learning-from-Human-Feedback-RLHF-Part-1--VmlldzoyODk5MTIx">blog 2</a></li>
  <li>PPO implementation</li>
  <li>Human feedback collection</li>
  <li>Alignment techniques</li>
</ul>

<p>”””
Link: https://github.com/openai/following-instructions-human-feedback
Family: GPT
Pretraining Architecture: Decoder
Pretraining Task: LM
Extension: GPTInstruct starts off with a pretrained GPT3 model and adds reward modeling through reinforcement learning after a supervised finetuning
Application: Knowledge-intensive dialog or language tasks
Date (of first known publication): 01/2022
Num. Params: Same as GPT3
Corpus: Same as GPT3 for pretraining, but finetuned and optimized using labeler data and prompts
License: Closed source, Accessible through API
Lab: OpenAI
“””</p>

<details>

<summary>Quick Summary</summary>
<div>

    <p>This paper introduces InstructGPT, a model trained to follow human instructions by fine-tuning GPT-3 using reinforcement learning from human feedback (RLHF). The authors show that alignment with human preferences can be achieved through a three-step process:</p>

    <ol>
      <li>Collecting human demonstrations of desired behavior for supervised fine-tuning (SFT)</li>
      <li>Gathering human comparisons between model outputs to train a reward model</li>
      <li>Using reinforcement learning to optimize the model against this reward function</li>
    </ol>

    <p>Their key findings show that even smaller InstructGPT models (1.3B parameters) can outperform the much larger GPT-3 (175B parameters) in terms of following user instructions, truthfulness, and harmlessness - demonstrating that alignment doesn’t necessarily require larger models. The approach also reduces harmful outputs while maintaining good performance on standard NLP benchmarks with minimal regressions.</p>

    <p>This work is significant as it provides a practical approach to aligning language models with human intent, though the authors note limitations including the model still making simple mistakes and the alignment being specifically to their team of human labelers rather than broader human values.</p>

  </div>
</details>
<p><br /></p>

<h3 id="bloom">BLOOM</h3>

<p><a href="https://arxiv.org/abs/2211.05100">paper</a></p>

<ul>
  <li>Multilingual pre-training</li>
  <li>Carbon footprint considerations</li>
  <li>Distributed training</li>
  <li>Community governance</li>
</ul>

<p>”””
Link: https://huggingface.co/docs/transformers/model_doc/bloom
Family: GPT
Pretraining Architecture: Decoder
Pretraining Task: LM
Extension: Main difference to GPT-3 is that it uses full attention instead of sparse attention
Application: Same as GPT-3
Date (of first known publication): 07/2022
Num. Params:176B
Corpus: 366B tokens (1.5 TB of text data) multilingual dataset
Lab: Big Science/Huggingface
License: Open, but need to follow restrictions in Attachment A, BigScience RAIL License v1.0
“””</p>

<details>

<summary>Quick Summary</summary>
<div>

    <p>This paper introduces BLOOM (BigScience Large Open-science Open-access Multilingual Language Model), a 176 billion parameter language model created through a large-scale collaborative effort called BigScience. Here’s a high-level summary:</p>

    <p><strong>Key Points:</strong></p>

    <ol>
      <li>
        <p><strong>Open and Collaborative</strong>: Unlike many large language models developed by well-resourced organizations, BLOOM was created through a collaboration of hundreds of researchers and is publicly released.</p>
      </li>
      <li>
        <p><strong>Multilingual Focus</strong>: BLOOM was trained on 46 natural languages and 13 programming languages, addressing the English-centric bias of many previous large language models.</p>
      </li>
      <li>
        <p><strong>Training Data</strong>: The model was trained on the ROOTS corpus, a carefully curated 1.61TB dataset spanning multiple languages, with attention to data governance and ethical considerations.</p>
      </li>
      <li>
        <p><strong>Architecture</strong>: BLOOM uses a causal decoder-only Transformer architecture with 176B parameters, incorporating ALiBi positional embeddings and embedding layer normalization.</p>
      </li>
      <li>
        <p><strong>Evaluation</strong>: The model shows competitive performance on various benchmarks, including SuperGLUE, machine translation, summarization, and code generation. Performance improves significantly after multitask prompted finetuning (resulting in BLOOMZ).</p>
      </li>
      <li>
        <p><strong>Environmental Impact</strong>: The authors estimate BLOOM’s carbon footprint at 25 tons of CO2eq, significantly less than models like GPT-3 (502 tons), partly due to using a low-carbon energy grid.</p>
      </li>
      <li>
        <p><strong>Ethical Considerations</strong>: The paper discusses social limitations of LLM development and how the BigScience effort tried to address these through an Ethical Charter, more diverse representation, and a Responsible AI License.</p>
      </li>
    </ol>

    <p>This paper represents a significant milestone in democratizing access to large language model technology while also attempting to address some of the ethical, environmental, and linguistic diversity concerns associated with these powerful systems.</p>

    <p>Is there a specific aspect of the paper you’d like to explore further?</p>

  </div>
</details>
<p><br /></p>

<h3 id="emergent-abilities-of-large-language-models">Emergent Abilities of Large Language Models</h3>

<p><a href="https://arxiv.org/abs/2206.07682">paper</a></p>

<details>

<summary>Quick Summary</summary>
<div>

    <p>This paper by researchers from Google, Stanford, UNC Chapel Hill, and DeepMind explores a fascinating phenomenon they call “emergent abilities” in large language models (LLMs).</p>

    <p>The key idea is that some capabilities in LLMs do not appear gradually as models scale up, but rather emerge suddenly when models reach a certain size threshold. Before this threshold, models perform at random chance on certain tasks, but after crossing this threshold, performance jumps significantly. This pattern differs from the smooth, predictable scaling laws typically observed in language model pretraining.</p>

    <p>The paper defines emergent abilities as “abilities that are not present in smaller models but are present in larger models,” meaning they cannot be predicted by simply extrapolating performance improvements from smaller models.</p>

    <p>Some examples they document include:</p>

    <ul>
      <li>Arithmetic reasoning with 3-digit numbers</li>
      <li>Translation from phonetic alphabets</li>
      <li>Word unscrambling</li>
      <li>Various types of reasoning tasks</li>
    </ul>

    <p>The authors also explore how certain capabilities like chain-of-thought reasoning, instruction following, and self-consistency only emerge at certain model scales and may be harmful for smaller models.</p>

    <p>The paper raises important questions about what other abilities might emerge with further scaling, whether emergence thresholds could be lowered with better architectures or training data, and why emergence happens at all.</p>

  </div>
</details>
<p><br /></p>

<h3 id="flash-attention">Flash Attention</h3>

<p><a href="https://arxiv.org/abs/2205.14135">paper</a></p>

<details>

<summary>Quick Summary</summary>
<div>

    <p>This paper introduces FlashAttention, an algorithm that makes the standard Transformer attention mechanism significantly faster and more memory-efficient by being “IO-aware” - that is, by carefully managing how data moves between different levels of GPU memory (high-bandwidth memory and on-chip SRAM).</p>

    <p>The key innovations are:</p>

    <ol>
      <li>Using tiling techniques to avoid materializing the large N×N attention matrix in GPU high-bandwidth memory</li>
      <li>Recomputing certain values during the backward pass rather than storing them</li>
      <li>Fusing multiple operations into a single GPU kernel to minimize memory traffic</li>
    </ol>

    <p>These techniques reduce memory requirements from quadratic to linear in sequence length and achieve substantial speedups (3-7.6x on attention computation). This enables training Transformers with much longer context lengths, leading to better model quality and new capabilities like solving the Path-X sequence modeling challenge (16K tokens).</p>

    <p>The authors also extend FlashAttention to block-sparse attention, creating an even faster approximate attention algorithm.</p>

  </div>
</details>
<p><br /></p>

<h3 id="grouped-query-attention">Grouped-query attention</h3>

<p><a href="https://arxiv.org/abs/2305.13245">paper</a></p>

<details>

<summary>Quick Summary</summary>
<div>

    <p>This paper from Google Research tackles an important problem in transformer model inference: the memory bandwidth bottleneck caused by loading keys and values during autoregressive decoding.</p>

    <p>The authors make two main contributions:</p>

    <ol>
      <li>
        <p><strong>Uptraining Existing Models</strong>: They show that existing multi-head attention (MHA) models can be efficiently converted to multi-query attention (MQA) models using just 5% of the original pre-training compute. Rather than training new models from scratch for faster inference, this approach allows reusing existing checkpoints.</p>
      </li>
      <li>
        <p><strong>Introducing Grouped-Query Attention (GQA)</strong>: They propose a new attention mechanism that sits between MHA (where every query has its own key and value head) and MQA (where all queries share a single key-value head). GQA organizes query heads into groups, with each group sharing a key-value head.</p>
      </li>
    </ol>

    <p>The results demonstrate that GQA achieves quality close to multi-head attention while being nearly as fast as multi-query attention - essentially getting the best of both worlds. This approach is particularly beneficial for larger models where the memory bandwidth from loading the KV cache becomes a major bottleneck.</p>

    <p>I’d be happy to dive deeper into any specific aspect of the paper that interests you most.</p>

  </div>
</details>
<p><br /></p>

<h3 id="alibi-position-encoding">ALiBi position encoding</h3>

<p><a href="https://arxiv.org/abs/2108.12409">paper</a></p>

<details>

<summary>Quick Summary</summary>
<div>

    <p>This paper introduces ALiBi (Attention with Linear Biases), a position encoding method for transformer models that enables training on shorter sequences while extrapolating to longer sequences at inference time.</p>

    <p>The key contributions are:</p>

    <ol>
      <li>
        <p>The authors identify a limitation in transformers: models trained on sequences of length L struggle to handle longer sequences at inference time.</p>
      </li>
      <li>
        <p>They show that existing position encoding methods (sinusoidal, rotary, T5 bias) have limited extrapolation capabilities.</p>
      </li>
      <li>
        <p>They introduce ALiBi, which doesn’t add positional embeddings but instead modifies attention by applying a distance-based linear bias to attention scores.</p>
      </li>
      <li>
        <p>ALiBi enables models to be trained on shorter sequences and extrapolate effectively to much longer ones - even extrapolating to sequences that are 2-10x longer than those seen during training.</p>
      </li>
      <li>
        <p>The method is computationally efficient and requires minimal changes to transformer code, with no additional parameters.</p>
      </li>
      <li>
        <p>Experiments show ALiBi outperforms other position methods on WikiText-103 and other datasets, even when extrapolating.</p>
      </li>
    </ol>

    <p>This work has significant practical implications for transformer efficiency, as training on shorter sequences requires substantially less computational resources while still enabling effective processing of longer sequences during inference.</p>

  </div>
</details>
<p><br /></p>

<h3 id="deepspeed-inference-enabling-efficient-inference-of-transformer-models-at-unprecedented-scale">DeepSpeed Inference: Enabling Efficient Inference of Transformer Models at Unprecedented Scale</h3>

<p><a href="https://arxiv.org/abs/2207.00032">paper</a></p>

<details>

<summary>Quick Summary</summary>
<div>

    <p>This paper introduces DeepSpeed Inference, a comprehensive system for efficient inference of transformer models at unprecedented scales. The authors address challenges in deploying extremely large transformer models (from billions to trillions of parameters) for inference applications.</p>

    <p>The main components of DeepSpeed Inference include:</p>

    <ol>
      <li>
        <p><strong>DeepSpeed Transformer</strong>: A GPU-only solution designed to minimize latency while maximizing throughput for both dense and sparse transformer models. It includes optimized single-GPU transformer kernels, many-GPU dense transformer layer, and massive-GPU scale sparse transformer layer.</p>
      </li>
      <li>
        <p><strong>ZeRO-Inference</strong>: A heterogeneous solution that leverages CPU and NVMe memory in addition to GPU memory to enable high inference throughput with large models that don’t fit in aggregate GPU memory.</p>
      </li>
    </ol>

    <p>Their results demonstrate significant improvements:</p>

    <ul>
      <li>Reduces latency by up to 7.3× over state-of-the-art for latency-oriented scenarios</li>
      <li>Increases throughput by over 1.5x for throughput-oriented scenarios</li>
      <li>Enables trillion-parameter scale inference under real-time latency constraints</li>
      <li>Can inference 25× larger models than GPU-only solutions while delivering high throughput</li>
    </ul>

    <p>The paper addresses specific challenges in transformer inference related to memory bandwidth, throughput, and resource constraints, providing a comprehensive solution for the increasingly diverse landscape of transformer models.</p>

  </div>
</details>
<p><br /></p>

<h3 id="claude-1">Claude 1</h3>

<ul>
  <li>Initial release focusing on helpfulness and harmlessness</li>
</ul>

<h3 id="flan-fine-tuned-language-net-google">FLAN (Fine-tuned LAnguage Net) (Google)</h3>

<p><a href="https://arxiv.org/abs/2109.01652">paper</a></p>

<ul>
  <li>Instruction tuning across multiple tasks</li>
  <li>Improved zero-shot performance</li>
</ul>

<details>

<summary>Quick Summary</summary>
<div>

    <p>This 2022 ICLR paper by Jason Wei and colleagues at Google Research introduces “instruction tuning” - a simple yet effective approach to improve zero-shot learning capabilities of large language models. The authors:</p>

    <ol>
      <li>Take a 137B parameter pretrained language model</li>
      <li>Finetune it on 60+ NLP datasets that are described via natural language instructions</li>
      <li>Call this instruction-tuned model “FLAN” (Finetuned Language Net)</li>
      <li>Evaluate FLAN on unseen task types using a careful methodology</li>
    </ol>

    <p>The key findings are impressive:</p>

    <ul>
      <li>FLAN significantly outperforms zero-shot performance of the base model</li>
      <li>FLAN surpasses GPT-3’s zero-shot performance on 20 of 25 datasets evaluated</li>
      <li>FLAN even outperforms few-shot GPT-3 on several datasets (ANLI, RTE, BoolQ, etc.)</li>
    </ul>

    <p>Through ablation studies, they identify three critical factors for successful instruction tuning:</p>

    <ul>
      <li>Number of finetuning datasets (more is better)</li>
      <li>Model scale (benefits only emerge at sufficient scale)</li>
      <li>Natural language instructions (essential for cross-task transfer)</li>
    </ul>

    <p>This paper represents an important step in making large language models more capable of following natural language instructions without examples, expanding their practical utility for a wider audience.</p>

  </div>
</details>
<p><br /></p>

<h3 id="red-teaming-language-models-with-language-models">Red Teaming Language Models with Language Models</h3>

<p><a href="https://arxiv.org/abs/2202.03286">paper</a></p>

<details>

<summary>Quick Summary</summary>
<div>

    <p>This paper by Perez et al. introduces a novel approach for identifying harmful behaviors in language models (LMs) using other language models as “red team” attackers. Rather than relying on human-written test cases, which can be expensive and limited in scope, they demonstrate how to automatically generate test cases that effectively expose weaknesses in target LMs.</p>

    <p>The researchers show that their method can uncover a variety of harms in a 280B parameter chatbot, including:</p>

    <ul>
      <li>Offensive language generation</li>
      <li>Leakage of private training data</li>
      <li>Generation of inappropriate contact information</li>
      <li>Distributional biases against certain groups</li>
      <li>Escalating harmful behaviors in multi-turn dialogues</li>
    </ul>

    <p>The paper provides a significant methodological contribution by exploring several techniques for generating test cases, from zero-shot generation to reinforcement learning, and demonstrates that LM-based red teaming can complement manual testing approaches.</p>

  </div>
</details>
<p><br /></p>

<h3 id="helm-holistic-evaluation-of-language-models">HELM (Holistic Evaluation of Language Models)</h3>

<p><a href="https://arxiv.org/abs/2211.09110">paper</a></p>

<p>[its 170 pages, I am not reading it]</p>

<p>Comprehensive benchmark suite for LLMs
Standardized evaluation metrics</p>

<details>

<summary>Quick Summary</summary>
<div>

  </div>
</details>
<p><br /></p>

<h3 id="dall-e-2-openai">DALL-E 2 (OpenAI)</h3>

<ul>
  <li>Released in April 2022</li>
  <li>Significant improvement over original DALL-E</li>
  <li>Demonstrated remarkably detailed text-to-image generation</li>
  <li>Maintained controlled access with gradual rollout</li>
</ul>

<h3 id="stable-diffusion-stability-ai">Stable Diffusion (Stability AI)</h3>

<ul>
  <li>Released in August 2022 as “a deep learning, text-to-image model” that became “the premier product of Stability AI”</li>
  <li>Open-source alternative to DALL-E 2</li>
  <li>Democratized access to high-quality image generation</li>
  <li>Trained on LAION-5B dataset</li>
</ul>

<h3 id="gptq">GPTQ</h3>

<p><a href="https://arxiv.org/abs/2210.17323">paper</a></p>

<p>[also add how multi gpu inference works]</p>

<details>

<summary>Quick Summary</summary>
<div>

    <p>This paper addresses the challenge of compressing large language models (LLMs) like GPT-3 and BLOOM for more efficient inference. The key contribution is GPTQ, a novel one-shot weight quantization method that can compress models with billions of parameters down to 3-4 bits per weight with minimal accuracy loss.</p>

    <p>At a high level:</p>

    <ul>
      <li>GPTQ builds on previous work in post-training quantization, specifically adapting the Optimal Brain Quantization approach</li>
      <li>It introduces key optimizations that make quantization feasible for models with 175B+ parameters</li>
      <li>The method enables quantizing these massive models in just a few hours on a single GPU</li>
      <li>Results show GPTQ can maintain model performance while more than doubling compression compared to prior methods</li>
      <li>The authors demonstrate running a 175B parameter model on a single GPU for the first time</li>
    </ul>

    <p>The practical impact is significant: GPTQ allows large models to run with far fewer computational resources, achieving 3.25x speedups on high-end GPUs and 4.5x speedups on more cost-effective ones, making these powerful models more accessible to researchers and practitioners.</p>

  </div>
</details>
<p><br /></p>

<h3 id="beyond-the-imitation-game-quantifying-and-extrapolating-the-capabilities-of-language-models">Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models</h3>

<p><a href="https://arxiv.org/abs/2206.04615">paper</a></p>

<p>[BIG-Bench benchmark, 95 pages of benchmark ahhhhhhhh]</p>

<details>

<summary>Quick Summary</summary>
<div>

  </div>
</details>
<p><br /></p>

<h3 id="minerva">Minerva</h3>

<p><a href="https://arxiv.org/pdf/2206.14858">paper</a></p>

<details>

<summary>Quick Summary</summary>
<div>

    <p>This paper introduces <strong>Minerva</strong>, a large language model specifically designed to solve quantitative reasoning problems in mathematics and science. The key innovation is training PaLM models (8B, 62B, and 540B parameters) on a carefully curated dataset of mathematical content from arXiv papers and web pages containing LaTeX formatting.</p>

    <p><strong>Main Contributions:</strong></p>

    <ul>
      <li><strong>Dataset</strong>: 38.5B tokens of mathematical content that preserves LaTeX notation and mathematical expressions</li>
      <li><strong>Performance</strong>: Achieves state-of-the-art results on MATH dataset (50.3% with majority voting vs. previous 6.9%), GSM8k (78.5%), and MMLU-STEM (75.0%)</li>
      <li><strong>Evaluation</strong>: Introduces OCWCourses dataset with 272 undergraduate-level STEM problems</li>
      <li><strong>Method</strong>: Uses majority voting over multiple samples rather than external tools or calculators</li>
    </ul>

    <p><strong>Key Insight</strong>: By training on mathematically rich text that preserves formal notation (rather than just natural language descriptions of math), the model learns to manipulate mathematical symbols and follow step-by-step reasoning patterns effectively.</p>

    <p>The paper demonstrates that language models can achieve impressive mathematical reasoning capabilities when trained on appropriate data, though they still fall short of human expert performance and have notable limitations in verification and complex multi-step problems.</p>

  </div>
</details>
<p><br /></p>

<h3 id="chatgpt">ChatGPT</h3>

<p>The beginning of an Era</p>

<h2 id="2023-multi-modal-and-reasoning">2023: Multi-Modal and Reasoning</h2>

<h3 id="efficient-memory-management-for-large-language-model-serving-with-pagedattention">Efficient Memory Management for Large Language Model Serving with PagedAttention</h3>

<p><a href="https://arxiv.org/abs/2309.06180">paper</a></p>

<details>

<summary>Quick Summary</summary>
<div>

    <p>This paper introduces “PagedAttention,” a novel attention algorithm for efficiently serving Large Language Models (LLMs), and “vLLM,” a system built on this algorithm. The key innovation is inspired by virtual memory and paging techniques from operating systems - they divide the key-value (KV) cache memory into fixed-size blocks that can be stored non-contiguously, rather than requiring contiguous memory allocation.</p>

    <p>The KV cache is a significant memory bottleneck in LLM serving, often consuming around 30% of GPU memory. Traditional systems like FasterTransformer and Orca suffer from both internal and external memory fragmentation because they allocate contiguous memory chunks based on maximum possible sequence length, resulting in significant memory waste.</p>

    <p>PagedAttention significantly improves memory efficiency by:</p>

    <ol>
      <li>Reducing fragmentation through block-level memory management</li>
      <li>Enabling flexible sharing of KV cache within and across requests</li>
      <li>Supporting dynamic memory allocation as sequences grow</li>
    </ol>

    <p>Their experiments show vLLM improves throughput by 2-4× compared to state-of-the-art systems while maintaining the same latency, with even greater improvements for longer sequences, larger models, and complex decoding algorithms like beam search.</p>

  </div>
</details>
<p><br /></p>

<h3 id="qlora-efficient-finetuning-of-quantized-llms">QLoRA: Efficient Finetuning of Quantized LLMs</h3>

<p><a href="https://arxiv.org/abs/2305.14314">paper</a></p>

<details>

<summary>Quick Summary</summary>
<div>

    <p>This paper introduces QLORA (Quantized Low-Rank Adaptation), a significant advancement in efficient fine-tuning of large language models (LLMs). The key innovation is allowing 4-bit quantized models to be fine-tuned without performance degradation compared to full 16-bit precision models.</p>

    <p>The main contributions include:</p>

    <ol>
      <li><strong>4-bit NormalFloat (NF4)</strong>: A new data type optimized for normally distributed weights</li>
      <li><strong>Double Quantization</strong>: A technique to reduce memory footprint by quantizing the quantization constants</li>
      <li><strong>Paged Optimizers</strong>: A method to manage memory spikes during training</li>
    </ol>

    <p>These innovations collectively allow fine-tuning of a 65B parameter model on a single 48GB GPU while maintaining full 16-bit fine-tuning performance. This represents a dramatic improvement in accessibility, reducing the memory requirements from over 780GB to under 48GB.</p>

    <p>The authors demonstrate QLORA’s effectiveness by developing Guanaco, a family of models fine-tuned on the OASST1 dataset that performs competitively with ChatGPT on benchmark tests while requiring only 24 hours of training on a single GPU.</p>

  </div>
</details>
<p><br /></p>

<h3 id="parameter-efficient-fine-tuning-methods-for-pretrained-language-models-a-critical-review-and-assessment">Parameter-Efficient Fine-Tuning Methods for Pretrained Language Models: A Critical Review and Assessment</h3>

<p><a href="https://arxiv.org/abs/2312.12148">paper</a></p>

<p>https://huggingface.co/blog/diffusers-quantization</p>

<details>

<summary>Quick Summary</summary>
<div>

    <p>This paper provides a comprehensive review and assessment of Parameter-Efficient Fine-Tuning (PEFT) methods for pretrained language models (PLMs). The authors address an important challenge in modern NLP: as language models grow increasingly larger (from BERT’s 110M parameters to Falcon’s 180B parameters), traditional fine-tuning becomes computationally prohibitive for many practitioners.</p>

    <p>The paper categorizes PEFT methods into five main types:</p>

    <ol>
      <li>Additive fine-tuning (adding new parameters)</li>
      <li>Partial fine-tuning (updating only a subset of original parameters)</li>
      <li>Reparameterized fine-tuning (using low-rank decomposition)</li>
      <li>Hybrid fine-tuning (combining different PEFT approaches)</li>
      <li>Unified fine-tuning (proposing a unified framework)</li>
    </ol>

    <p>The authors conduct experiments with 11 representative PEFT methods across various NLP tasks to evaluate parameter efficiency and memory usage. Their analysis shows that most PEFT methods significantly reduce trainable parameters while maintaining performance comparable to full fine-tuning, with some methods even outperforming it.</p>

    <p>The paper also discusses applications of PEFT methods in multi-task learning, cross-lingual transfer, and backdoor attack/defense, concluding with future research directions in this rapidly evolving field.</p>

  </div>
</details>
<p><br /></p>

<h3 id="flashattention-2-faster-attention-with-better-parallelism-and-work-partitioning">FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning</h3>

<p><a href="https://arxiv.org/abs/2307.08691">paper</a></p>

<details>

<summary>Quick Summary</summary>
<div>

    <p>This paper presents FlashAttention-2, an improved algorithm for implementing attention mechanisms in Transformer models that significantly enhances computational efficiency. Building on the original FlashAttention work, FlashAttention-2 introduces better parallelism and work partitioning strategies that achieve approximately 2× speedup over its predecessor.</p>

    <p>The key innovations include:</p>

    <ol>
      <li>Algorithm optimizations to reduce non-matrix multiplication operations</li>
      <li>Improved parallelization across sequence length dimensions</li>
      <li>Better work distribution between GPU thread blocks and warps to minimize communication overhead</li>
    </ol>

    <p>The results are impressive - reaching 50-73% of theoretical maximum FLOPs/s on A100 GPUs and achieving up to 225 TFLOPs/s when used in end-to-end GPT model training (72% model FLOPs utilization).</p>

    <p>This advancement directly addresses the challenge of scaling Transformers to longer sequence lengths, which is critical for applications like processing long documents, high-resolution images, and video data.</p>

  </div>
</details>
<p><br /></p>

<h3 id="awq-activation-aware-weight-quantization-for-llm-compression-and-acceleration">AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration</h3>

<p><a href="https://arxiv.org/abs/2306.00978">paper</a></p>

<details>

<summary>Quick Summary</summary>
<div>

    <p>This paper addresses the challenge of deploying large language models (LLMs) directly on edge devices, which is important for privacy, offline usage, and reduced operational costs. The authors propose Activation-aware Weight Quantization (AWQ), a hardware-friendly approach for low-bit weight-only quantization of LLMs.</p>

    <p>Key contributions:</p>

    <ol>
      <li>The observation that not all weights in an LLM are equally important - protecting just 1% of salient weights can greatly reduce quantization error</li>
      <li>The insight that salient weight channels should be identified based on activation distribution rather than weight values</li>
      <li>A mathematical derivation showing that scaling up salient channels can reduce quantization error without using mixed-precision</li>
      <li>Implementation of TinyChat, an efficient inference framework for 4-bit LLMs on edge devices</li>
    </ol>

    <p>Their method demonstrates superior performance over existing quantization approaches across various language model benchmarks, including instruction-tuned LMs and multi-modal LMs. The TinyChat implementation achieves more than 3× speedup over Huggingface FP16 implementations on both desktop and mobile GPUs, enabling even 70B parameter models to run on mobile GPUs.</p>

  </div>
</details>
<p><br /></p>

<h3 id="generative-agents-interactive-simulacra-of-human-behavior">Generative Agents: Interactive Simulacra of Human Behavior</h3>

<p>Now we have started to get into the region of AI agents. I will recommend checking my blog on the topic for a beginner friendly introduction to the topic.</p>

<p><a href="https://arxiv.org/abs/2304.03442">paper</a></p>

<details>

<summary>Quick Summary</summary>
<div>

    <p>This paper introduces “generative agents” - computational agents powered by large language models that simulate believable human behavior in interactive environments. The authors present an architecture that extends language models to:</p>

    <ol>
      <li>Store comprehensive records of agents’ experiences using natural language</li>
      <li>Synthesize memories into higher-level reflections</li>
      <li>Retrieve relevant information dynamically to plan behavior</li>
    </ol>

    <p>The paper demonstrates this approach by creating a small town populated with 25 agents in a sandbox environment inspired by The Sims. These agents exhibit both individual behaviors (waking up, cooking, working) and emergent social dynamics (spreading information, forming relationships, coordinating activities).</p>

    <p>A key example highlighted is how, from a single prompt about one agent wanting to throw a Valentine’s Day party, the agents autonomously spread invitations, form new acquaintances, coordinate attendance, and even arrange dates to the party.</p>

    <p>The authors evaluate their system through controlled experiments and an end-to-end simulation, showing that their architecture components (observation, planning, and reflection) each contribute significantly to the believability of agent behavior.</p>

    <p>This work represents an interesting intersection of large language models, interactive systems, and human behavior simulation with potential applications in virtual environments, social prototyping, and training scenarios.</p>

  </div>
</details>
<p><br /></p>

<h3 id="voyager-an-open-ended-embodied-agent-with-large-language-models">Voyager: An Open-Ended Embodied Agent with Large Language Models</h3>

<p><a href="https://arxiv.org/abs/2305.16291">paper</a></p>

<details>

<summary>Quick Summary</summary>
<div>

    <p>This paper introduces VOYAGER, a novel AI agent that uses Large Language Models (specifically GPT-4) to enable lifelong learning and exploration in the open-world environment of Minecraft without human intervention.</p>

    <p>The key innovations of VOYAGER include:</p>

    <ol>
      <li>
        <p><strong>Automatic Curriculum</strong>: A self-driven goal-setting system that proposes appropriate tasks based on the agent’s current skills and environment state, maximizing exploration.</p>
      </li>
      <li>
        <p><strong>Skill Library</strong>: A repository of executable code for storing and retrieving complex behaviors, allowing the agent to build increasingly sophisticated skills over time.</p>
      </li>
      <li>
        <p><strong>Iterative Prompting Mechanism</strong>: A system that incorporates environment feedback, execution errors, and self-verification to improve program generation.</p>
      </li>
    </ol>

    <p>VOYAGER outperforms previous state-of-the-art approaches by obtaining 3.3× more unique items, traveling 2.3× longer distances, and unlocking key tech tree milestones up to 15.3× faster. It can also transfer learned skills to new Minecraft worlds to solve novel tasks, demonstrating strong generalization capabilities.</p>

    <p>The approach is particularly interesting because it creates a lifelong learning agent that operates through code generation rather than traditional reinforcement learning methods, with no need for model parameter fine-tuning.</p>

  </div>
</details>
<p><br /></p>

<h3 id="universal-and-transferable-adversarial-attacks-on-aligned-language-models">Universal and Transferable Adversarial Attacks on Aligned Language Models</h3>

<p><a href="https://arxiv.org/abs/2307.15043">paper</a></p>

<details>

<summary>Quick Summary</summary>
<div>

    <p>This 2023 paper by Zou et al. demonstrates a concerning vulnerability in aligned language models (LLMs) such as GPT, Claude, and others that have been fine-tuned not to produce harmful content. The researchers develop a surprisingly effective method called “Greedy Coordinate Gradient” (GCG) to generate adversarial prompts that can reliably make these models generate harmful, objectionable content despite their alignment training.</p>

    <p>The key findings include:</p>

    <ol>
      <li>The researchers can automatically generate adversarial suffixes that, when attached to harmful prompts, convince LLMs to respond affirmatively rather than refusing</li>
      <li>These attacks transfer remarkably well between models - suffixes trained on smaller open-source models like Vicuna work effectively against commercial models like GPT-3.5, GPT-4, and to a lesser extent Claude</li>
      <li>The attack success rates are quite high - up to 88% on the models they directly targeted, and as high as 84% transfer success rate to commercial models</li>
      <li>The method significantly outperforms previous approaches for automated adversarial prompting</li>
    </ol>

    <p>This represents a significant advancement in understanding vulnerabilities in LLM safety measures and raises important questions about current alignment techniques.</p>

    <p>I’m ready to explore any specific aspects of this paper that interest you, from the mathematical formulation of their attack method to the implications for language model safety.</p>

  </div>
</details>
<p><br /></p>

<h3 id="tree-of-thoughts-deliberate-problem-solving-with-large-language-models">Tree of Thoughts: Deliberate Problem Solving with Large Language Models</h3>

<p><a href="https://arxiv.org/abs/2305.10601">paper</a></p>

<details>

<summary>Quick Summary</summary>
<div>

    <p>This paper introduces “Tree of Thoughts” (ToT), a framework that enhances large language models’ (LLMs) problem-solving abilities by enabling more deliberate reasoning and exploration. Unlike standard autoregressive text generation or even Chain of Thought prompting, ToT allows LLMs to:</p>

    <ol>
      <li>Generate multiple intermediate “thoughts” (coherent text units that represent steps toward a solution)</li>
      <li>Evaluate these thoughts using the model’s own reasoning capabilities</li>
      <li>Explore different reasoning paths systematically using search algorithms (breadth-first or depth-first search)</li>
      <li>Use backtracking and lookahead to make more global decisions</li>
    </ol>

    <p>The authors demonstrate significant improvements on three challenging tasks:</p>

    <ul>
      <li>Game of 24 (mathematical reasoning): ToT achieved 74% success vs. 4% for GPT-4 with chain-of-thought</li>
      <li>Creative Writing (coherent multi-paragraph construction)</li>
      <li>Mini Crosswords (constraint satisfaction with linguistic knowledge)</li>
    </ul>

    <p>This framework represents an interesting bridge between classical AI problem-solving methods (tree search) and modern LLMs, adding a more deliberate “System 2” thinking process to complement the associative “System 1” capabilities of LLMs.</p>

  </div>
</details>
<p><br /></p>

<h3 id="mpt">Mpt</h3>

<p><a href="https://www.databricks.com/blog/mpt-7b">blog</a></p>

<details>

<summary>Quick Summary</summary>
<div>

    <p>MosaicML introduced MPT-7B (MosaicML Pretrained Transformer), a 7 billion parameter language model that addresses key limitations in open-source LLMs. The model’s key features include:</p>

    <ol>
      <li><strong>Commercial usability</strong> - Licensed under Apache-2.0, unlike models like LLaMA</li>
      <li><strong>Extensive training</strong> - Trained on 1 trillion tokens of text and code</li>
      <li><strong>Long context handling</strong> - Can process inputs up to 65k tokens (and even 84k in some cases) thanks to ALiBi</li>
      <li><strong>Optimized performance</strong> - Uses FlashAttention and FasterTransformer for improved training and inference</li>
      <li><strong>Competitive quality</strong> - Matches LLaMA-7B on standard benchmarks</li>
    </ol>

    <p>They released four variants:</p>

    <ul>
      <li>MPT-7B Base (general foundation model)</li>
      <li>MPT-7B-StoryWriter-65k+ (for long-form creative writing)</li>
      <li>MPT-7B-Instruct (for instruction following)</li>
      <li>MPT-7B-Chat (for conversational interactions)</li>
    </ul>

    <p>The model was trained in 9.5 days on 440 A100 GPUs at a cost of around $200,000, with zero human intervention required during training.</p>

  </div>
</details>
<p><br /></p>

<h3 id="wizardlm-empowering-large-language-models-to-follow-complex-instructions">WizardLM: Empowering Large Language Models to Follow Complex Instructions</h3>

<p><a href="https://arxiv.org/abs/2304.12244">paper</a></p>

<details>

<summary>Quick Summary</summary>
<div>

    <p>This paper introduces “Evol-Instruct,” a novel method for automatically generating complex instruction data to improve large language models’ (LLMs) instruction-following capabilities. Rather than relying on expensive and time-consuming human annotation of complex instructions, the authors propose using LLMs themselves to evolve simple instructions into more complex ones through systematic operations.</p>

    <p>The key contributions include:</p>

    <ol>
      <li>
        <p>An evolutionary approach to instruction generation that can produce increasingly complex instructions through “In-depth Evolving” (making instructions more complex) and “In-breadth Evolving” (creating diverse new instructions)</p>
      </li>
      <li>
        <p>WizardLM, a model created by fine-tuning LLaMA with these evolved instructions that shows impressive performance compared to other instruction-tuned models</p>
      </li>
      <li>
        <p>Evidence that AI-generated complex instructions can be superior to human-created ones for training LLMs, especially for handling complex queries</p>
      </li>
    </ol>

    <p>Their experiments show that WizardLM outperforms models like Alpaca and Vicuna on various benchmarks, and even outperforms ChatGPT on high-complexity tasks according to human evaluations.</p>

    <p>Is there a specific aspect of the paper you’d like to explore further? I’m happy to delve into the Evol-Instruct methodology, the evaluation approach, the mathematical components, or any other elements you find interesting.</p>

  </div>
</details>
<p><br /></p>

<h3 id="deepspeed-chat-easy-fast-and-affordable-rlhf-training-of-chatgpt-like-models-at-all-scales">DeepSpeed-Chat: Easy, Fast and Affordable RLHF Training of ChatGPT-like Models at All Scales</h3>

<p><a href="https://arxiv.org/abs/2308.01320">paper</a></p>

<details>

<summary>Quick Summary</summary>
<div>

    <p>This 2023 paper from Microsoft’s DeepSpeed team introduces DeepSpeed-Chat, a system designed to democratize the training of ChatGPT-like models using Reinforcement Learning from Human Feedback (RLHF). The system addresses three key challenges in the current landscape: accessibility, efficiency, and affordability of RLHF training, particularly for models with billions of parameters.</p>

    <p>DeepSpeed-Chat offers three main capabilities:</p>

    <ol>
      <li>An easy-to-use training and inference experience for ChatGPT-like models</li>
      <li>A DeepSpeed-RLHF pipeline that replicates the three-step training pipeline from InstructGPT (supervised fine-tuning, reward model training, and RLHF)</li>
      <li>A unified “Hybrid Engine” that optimizes both training and inference phases</li>
    </ol>

    <p>The paper demonstrates impressive efficiency gains - up to 15x faster than existing systems - making RLHF training both faster and more affordable. For example, they show training an OPT-13B model in just 9 hours for about $290 on Azure, and scaling to train a 175B parameter model in under a day. The system also enables training of much larger models on limited hardware, such as running a 13B parameter model on a single GPU.</p>

  </div>
</details>
<p><br /></p>

<h3 id="gpt-4">GPT-4</h3>

<p><a href="https://arxiv.org/abs/2303.08774">paper</a></p>

<ul>
  <li>Multi-modal encoders</li>
  <li>System prompting</li>
  <li>Advanced reasoning capabilities</li>
  <li>Tool use</li>
</ul>

<details>

<summary>Quick Summary</summary>
<div>

  </div>
</details>
<p><br /></p>

<h3 id="mistral-7b">Mistral 7b</h3>

<p><a href="https://arxiv.org/abs/2310.06825">paper</a></p>

<details>

<summary>Quick Summary</summary>
<div>

    <p>This paper introduces Mistral 7B, a 7-billion-parameter language model that achieves impressive efficiency and performance. The authors demonstrate that Mistral 7B outperforms larger models like Llama 2 (13B) across all benchmarks tested, and even surpasses Llama 1 (34B) in reasoning, mathematics, and code generation domains.</p>

    <p>The key architectural innovations include:</p>

    <ol>
      <li>Grouped-query attention (GQA) for faster inference and reduced memory requirements</li>
      <li>Sliding window attention (SWA) to handle arbitrary sequence lengths with lower computational costs</li>
      <li>A rolling buffer cache mechanism to maintain efficiency with long sequences</li>
    </ol>

    <p>The paper also presents Mistral 7B-Instruct, a fine-tuned version that outperforms Llama 2 13B-chat on both human and automated benchmarks. All models are released under the Apache 2.0 license.</p>

    <p>This work challenges conventional scaling laws by showing that careful architecture design can achieve better performance with fewer parameters, suggesting new directions for efficient LLM development.</p>

  </div>
</details>
<p><br /></p>

<h3 id="llama">LLaMA</h3>

<p><a href="https://arxiv.org/abs/2302.13971">paper</a></p>

<ul>
  <li>Efficient scaling</li>
  <li>Flash Attention-2</li>
  <li>Chat templates</li>
  <li>RLHF improvements</li>
</ul>

<p>”””
Link: https://huggingface.co/docs/transformers/main/model_doc/llama
Family: Transformer
Pretraining Architecture: Decoder
Pretraining Task: LM
Extension: LLaMA uses a Transformer architecture, and with extensions: Pre-normalization, SwiGLU activations, RoPE embeddings, reduced memory usage and runtime through efficient implementation of the causal multi-head attention, checkpointing to reduce the amount of activations that are recomputed during the backward pass, model and sequence parallelism to reduce memory usage of the model, and uses 1.4T BPE tokens after tokenization.
Application: Zero and few shot Commonsense reasoning, Question answering, Code generation and Reading comprehension.
Date (of first known publication): 02/2023
Num. Params: 7B, 13B, 33B and 65B
Corpus: English CommonCrawl + C4 + Github + Wikipedia + Gutenberg and Books3 + ArXiv + Stack Exchange
License: Limited, Non-commercial bespoke license
Lab: Meta
“””</p>

<details>

<summary>Quick Summary</summary>
<div>

    <p>This paper from Meta AI introduces LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. The key contributions include:</p>

    <ol>
      <li>Creating state-of-the-art models trained exclusively on publicly available datasets (unlike many competitors that use proprietary data)</li>
      <li>Demonstrating that smaller models trained on more tokens can outperform larger models (e.g., LLaMA-13B outperforms GPT-3 175B on most benchmarks)</li>
      <li>Making these models available to the research community</li>
    </ol>

    <p>The researchers focus on optimizing for inference efficiency rather than just training efficiency. They train their models on trillions of tokens (more than typically used) and implement architectural improvements including pre-normalization, SwiGLU activation functions, and rotary positional embeddings.</p>

    <p>The paper also examines performance across various benchmarks including common sense reasoning, question answering, reading comprehension, mathematical reasoning, and code generation.</p>

  </div>
</details>
<p><br /></p>

<h3 id="mixtral-8x7b">Mixtral 8x7B</h3>

<p><a href="https://arxiv.org/pdf/2401.04088">paper</a></p>

<details>

<summary>Quick Summary</summary>
<div>

    <p>This paper from Mistral AI introduces Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model that represents a significant advancement in efficient language model architecture. Let me provide a concise overview of the key contributions:</p>

    <p>The Mixtral 8x7B model:</p>

    <ul>
      <li>Builds on the Mistral 7B architecture but replaces the feedforward blocks with Mixture-of-Experts (MoE) layers</li>
      <li>Contains 8 expert networks per layer, with each token dynamically routed to 2 experts</li>
      <li>Has 47B total parameters but only activates 13B parameters per token (improving efficiency)</li>
      <li>Trained with a 32k token context window on multilingual data</li>
      <li>Outperforms Llama 2 70B and GPT-3.5 on most benchmarks despite using fewer active parameters</li>
      <li>Shows particular strength in mathematics, code generation, and multilingual tasks</li>
      <li>Available in both a base version and an instruction-tuned version (Mixtral 8x7B - Instruct)</li>
      <li>Released under the Apache 2.0 license for both academic and commercial use</li>
    </ul>

    <p>The instruction-tuned version (Mixtral 8x7B - Instruct) performs exceptionally well, surpassing GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B chat models on human evaluation benchmarks.</p>

  </div>
</details>
<p><br /></p>

<h3 id="llama-2">LLaMA 2</h3>

<p>https://yeokhengmeng.com/2025/04/llama2-llm-on-dos/</p>

<p><a href="https://arxiv.org/abs/2307.09288">paper</a></p>

<details>

<summary>Quick Summary</summary>
<div>

    <p>This paper introduces Llama 2, Meta’s updated collection of large language models (LLMs) ranging from 7 billion to 70 billion parameters. The paper focuses on two main offerings:</p>

    <ol>
      <li>
        <p><strong>Llama 2</strong> - Base pretrained models that improve upon Llama 1 with more training data (2 trillion tokens), longer context length (4096 tokens), and architectural improvements.</p>
      </li>
      <li>
        <p><strong>Llama 2-Chat</strong> - Fine-tuned versions optimized specifically for dialogue applications, using supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF).</p>
      </li>
    </ol>

    <p>The authors detail their comprehensive approach to safety alignment and provide extensive evaluation metrics showing that Llama 2-Chat models outperform most open-source alternatives and are competitive with some proprietary models like ChatGPT on helpfulness and safety benchmarks.</p>

    <p>Key contributions include detailed methodologies for the fine-tuning process, safety mechanisms, and a transparent discussion of potential limitations and ethical considerations. The models have been released for both commercial and research use.</p>

  </div>
</details>
<p><br /></p>

<h3 id="vicuna-lmsys">Vicuna (LMSYS)</h3>

<p><a href="https://lmsys.org/blog/2023-03-30-vicuna/">paper</a></p>

<ul>
  <li>Fine-tuned LLaMA</li>
  <li>Open-source conversational agent</li>
</ul>

<details>

<summary>Quick Summary</summary>
<div>

    <p>This blog post introduces Vicuna-13B, an open-source chatbot developed by fine-tuning Meta’s LLaMA model on approximately 70,000 user-shared conversations collected from ShareGPT. According to their preliminary evaluation using GPT-4 as a judge, Vicuna-13B achieves more than 90% of the quality of OpenAI’s ChatGPT and Google’s Bard while outperforming other open-source models like base LLaMA and Stanford Alpaca in over 90% of test cases.</p>

    <p>Key highlights:</p>

    <ol>
      <li><strong>Training approach</strong>: Fine-tuned LLaMA on user-shared conversations from ShareGPT with improved handling of multi-turn conversations and longer sequences</li>
      <li>
        <p><strong>Cost-efficiency</strong>: Training cost was approximately $300 for the 13B model</p>
      </li>
      <li>
        <p><strong>Novel evaluation method</strong>: Used GPT-4 as a judge to evaluate response quality compared to other chatbots</p>
      </li>
      <li>
        <p><strong>Performance</strong>: Achieved competitive results against proprietary models while significantly outperforming other open-source alternatives</p>
      </li>
      <li><strong>Availability</strong>: Code, weights, and an online demo released for non-commercial use</li>
    </ol>

    <p>The authors acknowledge limitations in mathematical reasoning, factual accuracy, and safety, noting that this represents an open starting point for future research.</p>

  </div>
</details>
<p><br /></p>

<h3 id="alpaca">Alpaca</h3>

<p><a href="https://crfm.stanford.edu/2023/03/13/alpaca.html">paper</a></p>

<ul>
  <li>Efficient fine-tuning approach</li>
  <li>Instruction-tuned LLaMA
“””
Link: https://github.com/tatsu-lab/stanford_alpaca
Family: LLaMA
Pretraining Architecture: Decoder
Fine-tuning Task: human instructions
Extension: Alpaca is fine-tuned from a 7B LLaMA model.
Application: Evaluated on a variety of text generation and classification tasks.
Date (of first known publication): 03/2023
Num. Params: 7B
Corpus: 52K instruction-following data generated using self-instruct mechanism, from 175 human-written instruction-output pairs.
License: Limited, Non-commercial bespoke license
Lab: Stanford
“””</li>
</ul>

<details>

<summary>Quick Summary</summary>
<div>

    <p>This blog post introduces Alpaca 7B, a model fine-tuned from Meta’s LLaMA 7B using 52,000 instruction-following demonstrations. Stanford researchers created Alpaca to provide the academic community with an accessible instruction-following model comparable to OpenAI’s text-davinci-003, but at a fraction of the cost (under $600 to reproduce).</p>

    <p>Key highlights:</p>

    <ol>
      <li>
        <p><strong>Training approach</strong>: Used the “self-instruct” method, starting with 175 human-written instruction-output pairs and prompting text-davinci-003 to generate additional examples, resulting in 52K unique instructions</p>
      </li>
      <li>
        <p><strong>Cost efficiency</strong>: Data generation cost less than $500 using OpenAI’s API, and fine-tuning took 3 hours on 8 A100 GPUs (under $100)</p>
      </li>
      <li>
        <p><strong>Performance</strong>: In preliminary human evaluation, Alpaca matched text-davinci-003 performance (winning 90 vs 89 comparisons in pairwise evaluation)</p>
      </li>
      <li>
        <p><strong>Academic focus</strong>: Explicitly designed for academic research only - commercial use prohibited due to LLaMA’s license and OpenAI’s terms of use</p>
      </li>
      <li>
        <p><strong>Known limitations</strong>: Exhibits hallucination, toxicity, and can generate misinformation (authors provide specific examples)</p>
      </li>
      <li>
        <p><strong>Safety measures</strong>: Implemented content filtering via OpenAI’s moderation API and watermarking for the demo</p>
      </li>
    </ol>

    <p>The work aimed to democratize access to instruction-following models for academic research while acknowledging the risks and implementing appropriate safeguards.</p>

  </div>
</details>
<p><br /></p>

<h3 id="direct-preference-optimization-dpo">Direct Preference Optimization (DPO)</h3>

<p><a href="https://arxiv.org/abs/2305.18290">paper</a></p>

<details>

<summary>Quick Summary</summary>
<div>

    <p>This paper introduces <strong>Direct Preference Optimization (DPO)</strong>, a revolutionary approach that eliminates the need for reinforcement learning in training language models from human preferences, making the process dramatically simpler while maintaining or improving performance.</p>

    <p><strong>Core Innovation: Bypassing Reinforcement Learning</strong></p>

    <p><strong>The Problem with RLHF</strong>: Traditional Reinforcement Learning from Human Feedback (RLHF) is complex and unstable, requiring:</p>

    <ol>
      <li>Training a reward model on preference data</li>
      <li>Using RL algorithms (like PPO) to optimize the language model</li>
      <li>Careful hyperparameter tuning and sampling during training</li>
    </ol>

    <p><strong>DPO’s Breakthrough</strong>: The paper shows that this two-stage process can be replaced with a single, simple classification loss that directly optimizes the language model on preference data.</p>

    <p><strong>Key Mathematical Insight</strong></p>

    <p>The central theoretical contribution is recognizing that <strong>your language model is secretly a reward model</strong>. Specifically:</p>

    <ul>
      <li><strong>Standard approach</strong>: Learn reward function r(x,y), then use RL to find optimal policy π*</li>
      <li>
        <table>
          <tbody>
            <tr>
              <td><strong>DPO insight</strong>: Any reward function can be reparameterized as r(x,y) = β log π(y</td>
              <td>x)/π_ref(y</td>
              <td>x) + β log Z(x)</td>
            </tr>
          </tbody>
        </table>
      </li>
      <li><strong>Key observation</strong>: In the Bradley-Terry preference model, the partition function Z(x) cancels out when comparing preferences</li>
      <li><strong>Result</strong>: You can directly optimize the policy using a simple binary cross-entropy loss</li>
    </ul>

    <p><strong>The DPO Algorithm</strong></p>

    <p>Instead of optimizing a complex RL objective, DPO uses:</p>

    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>L_DPO = -E[(x,y_w,y_l)~D] log σ(β log π_θ(y_w|x)/π_ref(y_w|x) - β log π_θ(y_l|x)/π_ref(y_l|x))
</code></pre></div>    </div>

    <p>Where:</p>

    <ul>
      <li>σ is the sigmoid function</li>
      <li>y_w and y_l are preferred and dispreferred completions</li>
      <li>β controls the KL penalty strength</li>
      <li>π_ref is the reference model (typically the SFT model)</li>
    </ul>

    <p><strong>Experimental Results</strong></p>

    <p><strong>Performance</strong>: DPO matches or exceeds PPO-based RLHF across three tasks:</p>

    <ul>
      <li><strong>Sentiment control</strong>: Better reward/KL trade-off than PPO</li>
      <li><strong>Summarization</strong>: 61% win rate vs PPO’s 57% on TL;DR dataset</li>
      <li><strong>Dialogue</strong>: Only method to improve over baseline on Anthropic-HH dataset</li>
    </ul>

    <p><strong>Simplicity</strong>: Eliminates the need for:</p>

    <ul>
      <li>Reward model training</li>
      <li>RL optimization loops</li>
      <li>Sampling during training</li>
      <li>Extensive hyperparameter tuning</li>
    </ul>

    <p><strong>Theoretical Contributions</strong></p>

    <ol>
      <li><strong>Equivalence proof</strong>: Shows DPO optimizes the same objective as RLHF</li>
      <li><strong>Completeness</strong>: Proves any reward function class can be represented with their reparameterization</li>
      <li><strong>Stability analysis</strong>: Explains why actor-critic methods (like PPO) can be unstable due to high-variance gradients</li>
    </ol>

    <p><strong>Significance</strong></p>

    <p>This work represents a paradigm shift in preference learning by showing that the seemingly necessary complexity of RLHF can be completely avoided. DPO makes training language models from human preferences accessible to a much broader range of practitioners while providing better or equivalent results.</p>

    <p><strong>What makes this particularly impactful</strong> is that it challenges a fundamental assumption in the field - that you need reinforcement learning to learn from preferences - and provides both theoretical justification and empirical validation for a much simpler alternative.</p>

  </div>
</details>
<p><br /></p>

<h3 id="constitutional-ai">Constitutional AI</h3>

<p><a href="https://arxiv.org/pdf/2212.08073">paper</a></p>

<p><a href="https://www.anthropic.com/research/constitutional-ai-harmlessness-from-ai-feedback">blog</a></p>

<details>

<summary>Quick Summary</summary>
<div>

  </div>
</details>
<p><br /></p>

<h3 id="toy-models-of-superposition">Toy Models of Superposition</h3>

<p><a href="https://transformer-circuits.pub/2022/toy_model/index.html">blog</a></p>

<details>

<summary>Quick Summary</summary>
<div>

  </div>
</details>
<p><br /></p>

<h3 id="towards-monosemanticity-decomposing-language-models-with-dictionary-learning">Towards Monosemanticity: Decomposing Language Models With Dictionary Learning</h3>

<p><a href="https://www.anthropic.com/research/towards-monosemanticity-decomposing-language-models-with-dictionary-learning">paper</a></p>

<details>

<summary>Quick Summary</summary>
<div>

  </div>
</details>
<p><br /></p>

<h3 id="palm-2">PaLM 2</h3>

<p><a href="https://arxiv.org/abs/2305.10403">paper</a></p>

<ul>
  <li>Improved multilingual capabilities</li>
  <li>Enhanced reasoning</li>
</ul>

<details>

<summary>Quick Summary</summary>
<div>

    <p><strong>Core Contribution</strong></p>

    <p>PaLM 2 achieves <strong>better performance than its much larger predecessor</strong> while being significantly more compute-efficient. This challenges the “bigger is always better” paradigm in Language Modeling.</p>

    <p><strong>Key Technical Insights</strong></p>

    <p><strong>1. Scaling Laws Validation</strong></p>

    <ul>
      <li>Independently confirms Hoffmann et al.’s findings that model parameters (N) and training tokens (D) should scale roughly 1:1</li>
      <li>This differs from earlier scaling approaches that prioritized model size over data</li>
    </ul>

    <p><strong>2. Three-Pronged Improvement Strategy</strong></p>

    <ul>
      <li><strong>Better data mixture</strong>: More multilingual, diverse, and higher-quality training data</li>
      <li><strong>Improved architecture</strong>: Uses a mixture of training objectives (not just standard Language Modeling)</li>
      <li><strong>Compute-optimal scaling</strong>: Smaller model trained on more tokens rather than just scaling up parameters</li>
    </ul>

    <p><strong>3. Multilingual Excellence</strong></p>

    <ul>
      <li>Strong performance across hundreds of languages</li>
      <li>Passes advanced language proficiency exams (C2 level) in multiple languages</li>
      <li>Significant improvements on translation tasks</li>
    </ul>

    <p><strong>Performance Highlights</strong></p>

    <ul>
      <li>Outperforms the much larger PaLM 540B on most benchmarks</li>
      <li>Achieves state-of-the-art results on reasoning tasks (78.1% on BIG-Bench Hard)</li>
      <li>Strong coding capabilities across multiple programming languages</li>
      <li>Reduced memorization compared to PaLM</li>
    </ul>

    <p><strong>Responsible AI Focus</strong></p>

    <ul>
      <li>Extensive evaluation of potential harms and biases across languages</li>
      <li>Analysis of memorization and privacy implications</li>
      <li>Inference-time toxicity control mechanisms</li>
    </ul>

    <p><strong>What makes this particularly interesting</strong> is that it demonstrates that careful data curation, architectural improvements, and compute-optimal training can be more effective than simply scaling up model size - a finding with significant implications for the field’s resource requirements and accessibility.</p>

  </div>
</details>
<p><br /></p>

<h3 id="laion-5b-laion">LAION-5B (LAION)</h3>

<p>I was conflicted about whether I should put it here or not. But this is one of the best works that made advancements in multi-modality possible.</p>

<p><a href="https://arxiv.org/abs/2210.08402">paper</a></p>

<ul>
  <li>Large-scale image-text dataset</li>
  <li>Enabled better multimodal training</li>
</ul>

<details>

<summary>Quick Summary</summary>
<div>

    <p>This paper introduces <strong>LAION-5B</strong>, a massive open dataset containing 5.85 billion image-text pairs designed for training large-scale multimodal models like CLIP and text-to-image generators. Here’s the high-level picture:</p>

    <p><strong>Key Contribution</strong></p>

    <p>The authors address a critical bottleneck in multimodal AI research: while models like CLIP and DALL-E demonstrated the power of training on billions of image-text pairs, the datasets used were proprietary and unavailable to the broader research community. LAION-5B democratizes access to large-scale multimodal training data.</p>

    <p><strong>Dataset Composition</strong></p>

    <ul>
      <li><strong>2.32B English</strong> image-text pairs</li>
      <li><strong>2.26B multilingual</strong> pairs (100+ languages)</li>
      <li><strong>1.27B “unknown language”</strong> pairs (short-form text, product names, etc.)</li>
    </ul>

    <p><strong>Collection Methodology</strong></p>

    <ol>
      <li>Started with <strong>Common Crawl</strong> web archives</li>
      <li>Extracted images with alt-text from HTML</li>
      <li><strong>CLIP-filtered</strong> pairs using cosine similarity thresholds (0.28 for English, 0.26 for others)</li>
      <li>Added safety tags for NSFW content, watermarks, and inappropriate material</li>
    </ol>

    <p><strong>Validation Results</strong></p>

    <p>The authors demonstrate LAION’s utility by successfully reproducing CLIP model performance and training state-of-the-art text-to-image models (Stable Diffusion, GLIDE variants).</p>

    <p><strong>Significance</strong></p>

    <p>This represents the first openly available dataset at the scale needed for training foundation multimodal models, potentially accelerating research in vision-language AI while enabling transparency and bias auditing.</p>

  </div>
</details>
<p><br /></p>

<h3 id="lima">LIMA</h3>

<p><a href="https://arxiv.org/abs/2305.11206">paper</a></p>

<p>Demonstrated efficiency of small high-quality datasets
1,000 examples for alignment</p>

<details>

<summary>Quick Summary</summary>
<div>

    <p>This paper presents <strong>LIMA</strong>, a striking demonstration that effective language model alignment can be achieved with remarkably little data. The core finding challenges conventional wisdom about the scale of instruction tuning needed for high-quality conversational AI.</p>

    <p><strong>Key Contribution</strong></p>

    <p>The authors propose the <strong>Superficial Alignment Hypothesis</strong>: that a model’s knowledge and capabilities are learned almost entirely during pretraining, while alignment primarily teaches the model which response format and style to use when interacting with users.</p>

    <p><strong>Experimental Design</strong></p>

    <ul>
      <li>Started with <strong>LLaMa 65B</strong> (pretrained base model)</li>
      <li>Fine-tuned on only <strong>1,000 carefully curated</strong> prompt-response pairs</li>
      <li>No reinforcement learning from human feedback (RLHF)</li>
      <li>No massive instruction datasets (unlike typical approaches using millions of examples)</li>
    </ul>

    <p><strong>Dataset Composition (1,000 examples total)</strong></p>

    <ul>
      <li><strong>750 examples</strong> from community Q&amp;A (Stack Exchange, wikiHow, Reddit)</li>
      <li><strong>250 manually authored</strong> examples by the research team</li>
      <li>Emphasis on <strong>quality and diversity</strong> over quantity</li>
    </ul>

    <p><strong>Results</strong></p>

    <p><strong>Human preference study</strong> across 300 test prompts showed LIMA:</p>

    <ul>
      <li><strong>Outperforms</strong> DaVinci003 (RLHF-trained) and Alpaca 65B (52K examples)</li>
      <li>Produces <strong>equivalent or better responses</strong> than GPT-4 in 43% of cases</li>
      <li><strong>58% win/tie rate</strong> against Bard, 65% against DaVinci003</li>
    </ul>

    <p><strong>Key Insights</strong></p>

    <ol>
      <li><strong>Quality » Quantity</strong>: Diminishing returns from scaling data without improving diversity/quality</li>
      <li><strong>Emergent capabilities</strong>: Zero-shot dialogue ability that improves dramatically with just 30 dialogue examples</li>
      <li><strong>Pretraining power</strong>: Most knowledge acquisition happens during pretraining, not instruction tuning</li>
    </ol>

    <p>This work has profound implications for understanding what makes language models helpful and could democratize access to high-quality conversational AI by dramatically reducing the data requirements for alignment.</p>

  </div>
</details>
<p><br /></p>

<h3 id="mamba">Mamba</h3>

<p>https://tridao.me/blog/</p>

<p><a href="https://arxiv.org/abs/2312.00752">paper</a></p>

<ul>
  <li>State space model for sequence modeling</li>
  <li>Linear scaling with sequence length</li>
</ul>

<details>

<summary>Quick Summary</summary>
<div>

    <p>This paper introduces <strong>Mamba</strong>, a novel neural network architecture that aims to replace Transformers for sequence modeling tasks. Here’s the high-level story:</p>

    <p><strong>The Core Problem</strong></p>

    <p>Transformers dominate modern AI but have a fundamental limitation: their attention mechanism scales quadratically with sequence length, making them computationally expensive for long sequences (think processing entire books, long DNA sequences, or extended audio).</p>

    <p><strong>The Proposed Solution</strong></p>

    <p>The authors develop <strong>Selective State Space Models (SSMs)</strong> - a new approach that:</p>

    <ul>
      <li>Scales <strong>linearly</strong> with sequence length (much more efficient)</li>
      <li>Introduces a novel “selection mechanism” that allows the model to selectively focus on or ignore parts of the input sequence</li>
      <li>Achieves performance comparable to Transformers while being significantly faster</li>
    </ul>

    <p><strong>Key Innovation: Selection Mechanism</strong></p>

    <p>Traditional SSMs are “linear time-invariant” - they process all inputs the same way. Mamba’s breakthrough is making the model parameters <strong>input-dependent</strong>, allowing it to:</p>

    <ul>
      <li>Remember important information indefinitely</li>
      <li>Forget irrelevant details</li>
      <li>Adapt its behavior based on context</li>
    </ul>

    <p><strong>Empirical Results</strong></p>

    <p>The paper demonstrates that Mamba:</p>

    <ul>
      <li>Matches or exceeds Transformer performance on Language Modeling</li>
      <li>Handles sequences up to 1 million tokens</li>
      <li>Achieves 5× higher inference throughput than Transformers</li>
      <li>Works well across multiple domains (language, DNA, audio)</li>
    </ul>

    <p>This represents a significant step toward more efficient sequence models that could handle much longer contexts than current Transformers. The mathematical foundation combines classical control theory (state space models) with modern deep learning innovations.</p>

  </div>
</details>
<p><br /></p>

<h3 id="llava-visual-instruction-tuning">LLaVA (Visual Instruction Tuning)</h3>

<p><a href="https://arxiv.org/abs/2304.08485">paper</a></p>

<ul>
  <li>Released in April 2023 LLaVA was among the first vision-language models created using visual instruction tuning</li>
  <li>Combined vision encoders with language models</li>
  <li>Pioneered efficient visual instruction tuning</li>
  <li>Set foundation for open-source multimodal models</li>
</ul>

<details>

<summary>Quick Summary</summary>
<div>

    <p>This paper introduces <strong>visual instruction tuning</strong> - the first attempt to extend instruction-following capabilities from language-only models to multimodal vision-language tasks. The key contributions are:</p>

    <p><strong>Core Innovation</strong>: Using GPT-4 to automatically generate multimodal instruction-following data by converting image-text pairs into conversational format, then training a model that connects a vision encoder (CLIP) with a language model (Vicuna).</p>

    <p><strong>Technical Approach</strong>:</p>

    <ul>
      <li>A simple but effective architecture: CLIP vision encoder → linear projection → LLM</li>
      <li>Two-stage training: (1) feature alignment pre-training, (2) end-to-end instruction tuning</li>
      <li>158K generated instruction-following samples across conversations, detailed descriptions, and complex reasoning</li>
    </ul>

    <p><strong>Key Results</strong>:</p>

    <ul>
      <li>Achieves 85.1% relative performance compared to GPT-4 on synthetic benchmarks</li>
      <li>Sets new SOTA on ScienceQA (92.53%) when combined with GPT-4</li>
      <li>Demonstrates strong generalization to unseen visual concepts and tasks</li>
    </ul>

    <p><strong>Significance</strong>: This work essentially brought the “ChatGPT moment” to multimodal AI by showing that instruction tuning - which revolutionized language models - could be successfully adapted to vision-language tasks using generated data rather than expensive human annotation.</p>

    <p>I’m ready to dive deeper into any aspect you’d like to explore - whether that’s the mathematical formulations, training procedures, data generation pipeline, or architectural choices. What interests you most about this approach?</p>

  </div>
</details>
<p><br /></p>

<h3 id="claude-1claude-2">Claude 1/Claude 2</h3>

<ul>
  <li>Released in March 2023 (Claude 1) and July 2023 (Claude 2)</li>
  <li>Focused on constitutional AI approach</li>
  <li>Enhanced safety and alignment</li>
  <li>Specialized in long-form content generation</li>
</ul>

<h3 id="gemini">Gemini</h3>

<p><a href="https://arxiv.org/pdf/2312.11805">paper</a></p>

<ul>
  <li>Announced initially in May 2023, fully released in December Described as “a family of multimodal large language models developed by Google DeepMind, and the successor to LaMDA and PaLM 2”</li>
  <li>Designed from the ground up as a multimodal model</li>
  <li>Positioned as Google’s answer to GPT-4</li>
</ul>

<details>

<summary>Quick Summary</summary>
<div>

    <p>This is Google’s technical report introducing <strong>Gemini</strong>, a family of multimodal AI models that can process and understand text, images, audio, and video simultaneously. The paper presents three model sizes:</p>

    <ul>
      <li><strong>Gemini Ultra</strong>: The most capable model for complex reasoning tasks</li>
      <li><strong>Gemini Pro</strong>: Balanced performance and efficiency for scalable deployment</li>
      <li><strong>Gemini Nano</strong>: Optimized for on-device applications</li>
    </ul>

    <p><strong>Key Highlights:</strong></p>

    <p><strong>Breakthrough Performance</strong>: Gemini Ultra achieves state-of-the-art results on 30 of 32 benchmarks tested, becoming the first model to surpass human expert performance on MMLU (90.04% vs 89.8% human expert threshold).</p>

    <p><strong>Native Multimodality</strong>: Unlike models that combine separate systems, Gemini is trained from the ground up to understand multiple modalities together, enabling sophisticated cross-modal reasoning.</p>

    <p><strong>Technical Innovation</strong>: Built on enhanced Transformer architecture, trained on Google’s TPU infrastructure with novel approaches to handle massive scale (97% training efficiency despite unprecedented resource usage).</p>

    <p><strong>Responsible Deployment</strong>: Extensive safety evaluations, red-teaming, and responsible AI practices integrated throughout development.</p>

    <p>The paper is quite comprehensive at 90+ pages, covering everything from architectural details and training infrastructure to extensive benchmarking and safety considerations.</p>

  </div>
</details>
<p><br /></p>

<h3 id="qwen">Qwen</h3>

<p><a href="https://arxiv.org/pdf/2309.16609">paper</a></p>

<details>

<summary>Quick Summary</summary>
<div>

    <p>This paper introduces <strong>QWEN</strong>, a comprehensive series of large language models developed by Alibaba’s Qwen Team. Here’s the high-level picture:</p>

    <p><strong>What they built</strong>: A family of language models ranging from 1.8B to 14B parameters, including:</p>

    <ul>
      <li>Base pretrained models (QWEN)</li>
      <li>Chat-aligned models (QWEN-CHAT)</li>
      <li>Specialized variants for coding (CODE-QWEN) and mathematics (MATH-QWEN)</li>
    </ul>

    <p><strong>Key contributions</strong>:</p>

    <ol>
      <li><strong>Scale &amp; Performance</strong>: Models trained on up to 3 trillion tokens, demonstrating competitive performance against much larger models</li>
      <li><strong>Multilingual Focus</strong>: Strong emphasis on Chinese-English bilingual capabilities with an optimized tokenizer</li>
      <li><strong>Specialized Training</strong>: Domain-specific fine-tuning for coding and mathematical reasoning</li>
      <li><strong>Comprehensive Alignment</strong>: Full pipeline from supervised fine-tuning (SFT) to reinforcement learning from human feedback (RLHF)</li>
    </ol>

    <p><strong>Technical highlights</strong>: The paper covers the complete model development lifecycle - from pretraining data curation and architectural choices to alignment techniques and specialized model variants. They achieve impressive results, with QWEN-14B outperforming many larger open-source models.</p>

    <p>What makes this particularly interesting is their systematic approach to building not just one model, but an entire ecosystem of specialized variants, all while maintaining strong multilingual capabilities.</p>

    <p>What aspects of this work would you like to dive deeper into? I’m ready to explore the mathematical foundations, training methodologies, or any specific techniques that caught your attention!</p>

  </div>
</details>
<p><br /></p>

<h3 id="qwen-vl">Qwen-VL</h3>

<p><a href="https://arxiv.org/abs/2308.12966">paper</a></p>

<details>

<summary>Quick Summary</summary>
<div>

    <p>This paper introduces <strong>Qwen-VL</strong>, a series of large-scale vision-language models that can process both images and text. The key contribution is creating models that go beyond basic image captioning and visual question answering to include <strong>fine-grained capabilities</strong> like:</p>

    <ul>
      <li><strong>Visual grounding</strong> (localizing objects with bounding boxes)</li>
      <li><strong>Text reading</strong> (OCR capabilities)</li>
      <li><strong>Multi-image conversations</strong></li>
      <li><strong>Multilingual support</strong> (English and Chinese)</li>
    </ul>

    <p><strong>Architecture Overview:</strong></p>

    <ul>
      <li>Built on the Qwen-7B language model foundation</li>
      <li>Uses a Vision Transformer (ViT) as the visual encoder</li>
      <li>Introduces a novel “position-aware vision-language adapter” that compresses visual features while preserving spatial information</li>
      <li>Total model size: ~9.6B parameters</li>
    </ul>

    <p><strong>Training Pipeline:</strong>
The authors use a carefully designed 3-stage training approach:</p>

    <ol>
      <li><strong>Pre-training</strong> on 1.4B image-text pairs (frozen LLM)</li>
      <li><strong>Multi-task pre-training</strong> on 7 different vision-language tasks simultaneously</li>
      <li><strong>Supervised fine-tuning</strong> for instruction-following and chat capabilities</li>
    </ol>

    <p><strong>Key Results:</strong>
Qwen-VL achieves state-of-the-art performance across multiple benchmarks compared to similar-scale models, particularly excelling in text-oriented tasks and fine-grained visual understanding.</p>

  </div>
</details>
<p><br /></p>

<h3 id="phi-1">Phi-1</h3>

<p><a href="https://arxiv.org/pdf/2306.11644">paper</a></p>

<details>

<summary>Quick Summary</summary>
<div>

    <p>This paper introduces <strong>phi-1</strong>, a remarkably compact 1.3B parameter language model for code generation that achieves impressive performance despite being orders of magnitude smaller than competing models. The key insight is that <strong>data quality trumps data quantity</strong> - they achieved 50.6% pass@1 accuracy on HumanEval and 55.5% on MBPP using only 7B tokens of carefully curated “textbook quality” data.</p>

    <p><strong>Core Innovation</strong>: Instead of following traditional scaling laws (bigger models + more data = better performance), they focused on three high-quality datasets:</p>

    <ol>
      <li><strong>Filtered code</strong> from The Stack/StackOverflow (~6B tokens) using GPT-4-based quality classification</li>
      <li><strong>Synthetic textbooks</strong> generated by GPT-3.5 (&lt;1B tokens)</li>
      <li><strong>Synthetic exercises</strong> for fine-tuning (~180M tokens)</li>
    </ol>

    <p><strong>Key Results</strong>: phi-1 outperforms much larger models like StarCoder (15.5B parameters, 1T tokens) while being trained 100x faster and using 100x less data.</p>

    <p><strong>Broader Implications</strong>: This work suggests that the “bitter lesson” of simply scaling compute might not be the only path forward - careful data curation can dramatically improve learning efficiency.</p>

  </div>
</details>
<p><br /></p>

<h3 id="reinforced-self-training-rest-for-language-modeling">Reinforced Self-Training (ReST) for Language Modeling</h3>

<p><a href="https://arxiv.org/pdf/2308.08998">paper</a></p>

<details>

<summary>Quick Summary</summary>
<div>

    <summary>Quick Summary</summary>

  </div>
</details>
<p><br /></p>

<h3 id="the-era-of-1-bit-llms-all-large-language-models-are-in-158-bits">The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits</h3>

<p><a href="https://arxiv.org/pdf/2402.17764">paper</a></p>

<details>

<summary>Quick Summary</summary>
<div>

    <summary>Quick Summary</summary>

  </div>
</details>
<p><br /></p>

<h2 id="2024-efficiency-and-performance">2024: Efficiency and Performance</h2>

<p>It’s June 2025 currently while I am writing this, and I cannot say for certain if the innovations mentioned in this and the next year will have huge impacts in the future, I just went with the papers and models that have caught “attention” in this time period. As well as ideas that I found unconventional and interesting.</p>

<h3 id="gemma">Gemma</h3>

<p><a href="https://arxiv.org/abs/2403.08295">paper</a></p>

<details>

<summary>Quick Summary</summary>
<div>

    <p><strong>Gemma: Open Models Based on Gemini Research and Technology</strong> introduces a family of open-source language models derived from Google’s Gemini research. This represents a significant contribution to the open AI ecosystem, as it makes state-of-the-art capabilities more accessible to researchers and developers.</p>

    <p><strong>Key Contributions:</strong></p>

    <ul>
      <li><strong>Two model sizes</strong>: 2B and 7B parameters, designed for different computational constraints</li>
      <li><strong>Dual releases</strong>: Both pre-trained base models and instruction-tuned chat variants</li>
      <li><strong>Strong performance</strong>: Outperforms similarly-sized open models on 11 out of 18 benchmarks</li>
      <li><strong>Responsible AI focus</strong>: Comprehensive safety evaluations and responsible deployment practices</li>
    </ul>

    <p><strong>Architecture Highlights:</strong></p>

    <ul>
      <li>Built on transformer decoder architecture with modern improvements</li>
      <li>Uses <strong>RoPE embeddings</strong> for positional encoding</li>
      <li><strong>GeGLU activations</strong> instead of standard ReLU</li>
      <li><strong>Multi-query attention</strong> for the 2B model, multi-head for 7B</li>
      <li>Trained on up to 6T tokens of primarily English text</li>
    </ul>

    <p><strong>Training Pipeline:</strong></p>

    <ol>
      <li><strong>Pre-training</strong> on web documents, mathematics, and code</li>
      <li><strong>Supervised Fine-Tuning (SFT)</strong> on instruction-response pairs</li>
      <li><strong>RLHF</strong> using human preference data</li>
    </ol>

    <p><strong>Notable Results:</strong></p>

    <ul>
      <li>Gemma 7B achieves 64.3% on MMLU and 44.4% on MBPP</li>
      <li>Strong performance on mathematics (46.4% on GSM8K) and coding tasks</li>
      <li>Comprehensive safety evaluations show competitive performance on responsibility benchmarks</li>
    </ul>

  </div>
</details>
<p><br /></p>

<h3 id="gemma-2">Gemma 2</h3>

<details>

<summary>Quick Summary</summary>
<div>

    <p>This paper introduces <strong>Gemma 2</strong>, Google DeepMind’s next-generation family of open language models ranging from 2B to 27B parameters. Here’s the key story:</p>

    <p><strong>Core Innovation: Knowledge Distillation at Scale</strong></p>

    <p>The paper’s main contribution is demonstrating that <strong>knowledge distillation</strong> - where smaller “student” models learn from larger “teacher” models - can dramatically improve performance when applied at massive scale (training on 50× more tokens than typically considered optimal).</p>

    <p><strong>Key Technical Advances</strong></p>

    <ul>
      <li><strong>Architectural improvements</strong>: Interleaving local sliding window attention with global attention, grouped-query attention (GQA), and logit soft-capping</li>
      <li><strong>Training methodology</strong>: Using distillation instead of standard next-token prediction for the 2B and 9B models</li>
      <li><strong>Responsible deployment</strong>: Extensive safety evaluations and responsible AI toolkit development</li>
    </ul>

    <p><strong>Performance Highlights</strong></p>

    <ul>
      <li>The models achieve state-of-the-art performance for their size class</li>
      <li>Gemma 2-27B competes with models 2-3× larger (like LLaMA-3 70B)</li>
      <li>Strong results on the LMSYS Chatbot Arena, with Gemma 2-27B ranking higher than LLaMA-3 70B</li>
    </ul>

    <p><strong>Broader Impact</strong></p>

    <p>The paper provides evidence that <strong>smaller, more efficiently trained models</strong> can challenge the “bigger is always better” paradigm in LLMs, which has important implications for democratizing access to capable AI systems.</p>

  </div>
</details>
<p><br /></p>

<h3 id="chatbot-arena-an-open-platform-for-evaluating-llms-by-human-preference">Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference</h3>

<p><a href="https://arxiv.org/abs/2403.04132">paper</a></p>

<details>

<summary>Quick Summary</summary>
<div>

    <p>This paper introduces <strong>Chatbot Arena</strong>, a crowdsourced platform for evaluating Large Language Models (LLMs) through human preferences rather than traditional static benchmarks. Here are the key contributions:</p>

    <p><strong>Core Innovation</strong>: The platform uses a “battle” format where users interact with two anonymous LLMs simultaneously, then vote for which response they prefer. This creates a live, dynamic evaluation system that captures real-world usage patterns.</p>

    <p><strong>Scale &amp; Impact</strong>: Since April 2023, they’ve collected over 240K votes from 90K+ users across 100+ languages, making it one of the most referenced LLM leaderboards in the field.</p>

    <p><strong>Mathematical Framework</strong>: The paper employs sophisticated statistical methods to convert pairwise comparisons into reliable rankings:</p>

    <ul>
      <li><strong>Bradley-Terry model</strong> for estimating win probabilities</li>
      <li><strong>Active sampling algorithms</strong> to efficiently select model pairs for comparison</li>
      <li><strong>Confidence intervals</strong> and anomaly detection for robust evaluation</li>
    </ul>

    <p><strong>Key Findings</strong>:</p>

    <ul>
      <li>Crowdsourced votes show high agreement (72-83%) with expert evaluations</li>
      <li>The diverse, user-generated prompts effectively discriminate between model capabilities</li>
      <li>Their ranking system provides statistically valid confidence intervals for model performance</li>
    </ul>

    <p><strong>Why It Matters</strong>: This addresses critical limitations of static benchmarks (contamination, lack of human alignment, limited diversity) by creating a continuously updating, human-preference-based evaluation system that better reflects real-world LLM usage.</p>

  </div>
</details>
<p><br /></p>

<h3 id="tinyllama-an-open-source-small-language-model">TinyLlama: An Open-Source Small Language Model</h3>

<p><a href="https://arxiv.org/abs/2401.02385">paper</a></p>

<details>

<summary>Quick Summary</summary>
<div>

    <p>This paper presents <strong>TinyLlama</strong>, a remarkably compact 1.1B parameter language model that challenges conventional wisdom about the relationship between model size and performance. The key insight is exploring what happens when you train a small model on far more data than traditional scaling laws suggest - specifically training on up to 3 trillion tokens (later reduced to 2T in v1.1).</p>

    <p><strong>Core Contributions:</strong></p>

    <ul>
      <li><strong>Architecture</strong>: Built on Llama 2’s foundation but optimized for efficiency with techniques like FlashAttention and grouped-query attention</li>
      <li><strong>Training Strategy</strong>: Demonstrates that smaller models can achieve competitive performance when trained on significantly more data than scaling laws recommend</li>
      <li><strong>Multi-stage Training</strong>: Introduces a three-phase approach (basic pretraining → domain-specific continual pretraining → cooldown) that creates specialized variants</li>
      <li><strong>Performance</strong>: Outperforms comparable models like OPT-1.3B and Pythia-1.4B across multiple benchmarks despite being smaller</li>
    </ul>

    <p><strong>Key Insight</strong>: Rather than following compute-optimal scaling (Chinchilla scaling laws), they pursue <em>inference-optimal</em> scaling - training smaller models longer to achieve better performance per parameter during deployment.</p>

    <p>The work is particularly valuable for democratizing language model research, enabling applications on resource-constrained devices, and providing a strong foundation for experimentation.</p>

  </div>
</details>
<p><br /></p>

<h3 id="mordernbert">MordernBert</h3>

<p><a href="https://arxiv.org/pdf/2412.13663">paper</a></p>

<details>

<summary>Quick Summary</summary>
<div>
    <p>This paper introduces <strong>ModernBERT</strong>, a significant modernization of the classic BERT encoder architecture. The key insight is that while decoder-only models like GPT have seen tremendous advances, encoder-only models have remained largely stagnant since BERT’s original release in 2019.</p>

    <p><strong>Core Contributions:</strong></p>

    <ol>
      <li><strong>Architectural Modernization</strong>: Incorporates proven techniques from recent transformer research (RoPE positional embeddings, GeGLU activations, alternating local/global attention)</li>
      <li><strong>Scale &amp; Data</strong>: Trained on 2 trillion tokens with modern data mixtures including code</li>
      <li><strong>Hardware-Aware Design</strong>: Optimized for inference efficiency on common GPUs</li>
      <li><strong>Long Context</strong>: Native 8192 sequence length (vs BERT’s 512)</li>
    </ol>

    <p><strong>Key Results:</strong></p>

    <ul>
      <li>State-of-the-art performance across classification, retrieval, and code tasks</li>
      <li>~2x faster inference than previous long-context encoders</li>
      <li>First encoder to beat DeBERTaV3 on GLUE since 2021</li>
      <li>Strong performance on both single-vector and multi-vector retrieval</li>
    </ul>

    <p><strong>Why This Matters:</strong>
Encoders remain crucial for production systems doing retrieval, classification, and RAG pipelines where efficiency matters more than generation capability. ModernBERT shows there’s still significant room for improvement in this “mature” architecture family.</p>

  </div>
</details>
<p><br /></p>

<h3 id="jamba-a-hybrid-transformer-mamba-language-model">Jamba: A Hybrid Transformer-Mamba Language Model</h3>

<p><a href="https://arxiv.org/abs/2403.19887">paper</a></p>

<details>

<summary>Quick Summary</summary>
<div>

    <p>This paper introduces <strong>Jamba</strong>, a novel hybrid architecture that combines three key components:</p>

    <ol>
      <li><strong>Transformer layers</strong> (the standard attention mechanism we know)</li>
      <li><strong>Mamba layers</strong> (a recent state-space model that’s more memory-efficient)</li>
      <li><strong>Mixture-of-Experts (MoE)</strong> (to scale model capacity while keeping compute manageable)</li>
    </ol>

    <p><strong>Key Innovation</strong></p>

    <p>Instead of using pure Transformer or pure Mamba architectures, Jamba interleaves these different layer types in a flexible pattern. The released model uses a 1:7 ratio (1 attention layer for every 7 Mamba layers) and achieves:</p>

    <ul>
      <li><strong>52B total parameters</strong> but only <strong>12B active parameters</strong></li>
      <li><strong>256K token context length</strong> support</li>
      <li><strong>8x smaller KV cache</strong> compared to vanilla Transformers</li>
      <li><strong>3x better throughput</strong> than Mixtral on long contexts</li>
      <li>Fits in a <strong>single 80GB GPU</strong></li>
    </ul>

    <p><strong>The Core Problem It Solves</strong></p>

    <p>Transformers struggle with long contexts due to quadratic memory scaling and large KV caches. Mamba is more efficient but historically underperforms Transformers. Jamba gets the best of both worlds by combining them strategically.</p>

    <p><strong>Particularly Interesting Finding</strong></p>

    <p>The paper shows that pure Mamba struggles with in-context learning (following formats in few-shot examples), but adding just a few attention layers restores this capability - suggesting attention mechanisms play a crucial role in pattern copying and format adherence.</p>

  </div>
</details>
<p><br /></p>

<h3 id="claude-3">Claude 3</h3>

<p><a href="https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf">Technical Report</a></p>

<ul>
  <li>Multi-modal understanding</li>
  <li>Tool use capabilities</li>
  <li>Advanced reasoning</li>
  <li>Constitutional AI improvements</li>
</ul>

<details>

<summary>Quick Summary</summary>
<div>

  </div>
</details>
<p><br /></p>

<h3 id="llama-3">LLaMA 3</h3>

<p>https://github.com/naklecha/llama3-from-scratch</p>

<p><a href="https://arxiv.org/abs/2407.21783">paper</a></p>

<details>

<summary>Quick Summary</summary>
<div>

    <p>This is Meta’s comprehensive technical report on <strong>Llama 3</strong>, their latest family of foundation models. Here’s a high-level summary:</p>

    <p><strong>What They Built</strong></p>

    <ul>
      <li><strong>Three model sizes</strong>: 8B, 70B, and 405B parameters</li>
      <li><strong>Flagship model</strong>: Llama 3 405B - a dense transformer with 128K context window</li>
      <li><strong>Multimodal extensions</strong>: Vision, video, and speech capabilities (experimental)</li>
      <li><strong>Performance</strong>: Competitive with GPT-4, Claude 3.5 Sonnet on many benchmarks</li>
    </ul>

    <p><strong>Key Technical Contributions</strong></p>

    <p><strong>Scale &amp; Data</strong>:</p>

    <ul>
      <li>405B model trained on 15.6T tokens (vs 1.8T for Llama 2)</li>
      <li>50× more compute than largest Llama 2 model</li>
      <li>Extensive data curation and quality filtering pipelines</li>
    </ul>

    <p><strong>Infrastructure Innovation</strong>:</p>

    <ul>
      <li>4D parallelism for training at 16K GPU scale</li>
      <li>Novel pipeline parallelism improvements</li>
      <li>FP8 quantization for efficient inference</li>
    </ul>

    <p><strong>Safety &amp; Alignment</strong>:</p>

    <ul>
      <li>Comprehensive safety evaluation across capabilities</li>
      <li>Llama Guard 3 for system-level safety</li>
      <li>Extensive red teaming and adversarial testing</li>
    </ul>

    <p><strong>Mathematical Foundation</strong></p>

    <p>The paper is built on established scaling laws but pushes them to new extremes, with careful attention to compute-optimal training and the tension between model size and training duration.</p>

  </div>
</details>
<p><br /></p>

<h3 id="claude-3-1">Claude 3</h3>

<p>Opus, Sonnet, and Haiku variants
Improved reasoning and multimodal capabilities</p>

<h3 id="gemini-15">Gemini 1.5</h3>

<p><a href="https://arxiv.org/pdf/2403.05530">paper</a></p>

<details>

<summary>Quick Summary</summary>
<div>

    <p>This paper introduces <strong>Gemini 1.5 Pro and Gemini 1.5 Flash</strong> - a new generation of multimodal AI models that represent a major leap in <strong>long-context understanding</strong>. The standout achievement is extending context windows from the typical 32K-200K tokens to <strong>up to 10 million tokens</strong> across text, images, video, and audio modalities.</p>

    <p><strong>Key Innovations:</strong></p>

    <ul>
      <li><strong>Architecture</strong>: Gemini 1.5 Pro uses a sparse mixture-of-experts (MoE) Transformer, while Flash is a distilled dense model</li>
      <li><strong>Unprecedented Scale</strong>: Can process ~7 million words, 107 hours of audio, or 10.5 hours of video in a single context</li>
      <li><strong>Multimodal Excellence</strong>: Near-perfect recall (&gt;99%) on “needle-in-haystack” tasks across all modalities</li>
      <li><strong>Practical Capabilities</strong>: Learn new languages from single grammar books, analyze entire codebases, reason over hour-long videos</li>
    </ul>

    <p><strong>Performance Highlights:</strong></p>

    <ul>
      <li>Outperforms Gemini 1.0 Ultra on most benchmarks despite using less training compute</li>
      <li>Achieves state-of-the-art on many multimodal reasoning tasks</li>
      <li>Demonstrates remarkable in-context learning (e.g., translating Kalamang, a language with &lt;200 speakers, from documentation alone)</li>
    </ul>

    <p>The paper also includes extensive safety evaluations and introduces novel benchmarks for long-context evaluation, addressing a key challenge in evaluating such capable models.</p>

    <p><strong>Mathematical/Technical Depth</strong>: The paper contains rich technical content around scaling laws, architectural innovations, and evaluation methodologies that we could dive deep into.</p>

  </div>
</details>
<p><br /></p>

<h3 id="qwen-2">Qwen 2</h3>

<details>

<summary>Quick Summary</summary>
<div>

    <p>This paper introduces <strong>Qwen2</strong>, a comprehensive family of large language models ranging from 0.5B to 72B parameters, including both dense models and a Mixture-of-Experts (MoE) variant. Here are the key highlights:</p>

    <p><strong>Main Contributions:</strong></p>

    <ul>
      <li><strong>Model Family</strong>: Five model sizes (0.5B, 1.5B, 7B, 57B-A14B MoE, 72B) designed for different deployment scenarios</li>
      <li><strong>Performance</strong>: The flagship Qwen2-72B achieves strong results across benchmarks (84.2 MMLU, 64.6 HumanEval, 89.5 GSM8K)</li>
      <li><strong>Multilingual</strong>: Supports ~30 languages with robust capabilities</li>
      <li><strong>Long Context</strong>: Extended context length up to 131K tokens using YARN and Dual Chunk Attention</li>
    </ul>

    <p><strong>Technical Innovations:</strong></p>

    <ul>
      <li><strong>Architecture</strong>: Grouped Query Attention (GQA) for efficient inference</li>
      <li><strong>MoE Design</strong>: Fine-grained experts with shared + routing-specific expert structure</li>
      <li><strong>Training</strong>: 7T tokens for dense models, enhanced data quality and distribution</li>
      <li><strong>Post-training</strong>: Scalable alignment with minimal human annotation using automated synthesis</li>
    </ul>

    <p><strong>Key Strengths:</strong></p>

    <ul>
      <li>Outperforms most open-weight models including Qwen1.5</li>
      <li>Competitive with proprietary models across diverse benchmarks</li>
      <li>Strong performance in coding, mathematics, and reasoning tasks</li>
      <li>Comprehensive safety and contamination analysis</li>
    </ul>

    <p>The paper represents a significant step forward in open-weight language models, with particular attention to practical deployment considerations and rigorous evaluation methodology.</p>

  </div>
</details>
<p><br /></p>

<h3 id="phi-2phi-3">phi-2/phi-3</h3>

<p>Small but powerful models
High performance with limited training data</p>

<details>

<summary>Quick Summary</summary>
<div>

  </div>
</details>
<p><br /></p>

<h3 id="openai-o1">OpenAI o1</h3>

<p>First specialized reasoning model
Advanced mathematical problem-solving</p>

<details>

<summary>Quick Summary</summary>
<div>

  </div>
</details>
<p><br /></p>

<h3 id="rso-reinforced-self-training-with-online-feedback">RSO (Reinforced Self-training with Online feedback)</h3>

<ul>
  <li>Self-improvement through AI evaluation</li>
  <li>Reduced human annotation needs</li>
</ul>

<details>

<summary>Quick Summary</summary>
<div>

  </div>
</details>
<p><br /></p>

<h3 id="spin-self-played-improvement-narration">SPIN (Self-Played Improvement Narration)</h3>

<ul>
  <li>Self-correction capabilities</li>
  <li>Improved factual accuracy</li>
</ul>

<details>

<summary>Quick Summary</summary>
<div>

  </div>
</details>
<p><br /></p>

<h3 id="dbrx">DBRX</h3>

<p><a href="https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm">blog</a></p>

<details>

<summary>Quick Summary</summary>
<div>

  </div>
</details>
<p><br /></p>

<h3 id="flashattention-3-fast-and-accurate-attention-with-asynchrony-and-low-precision">FlashAttention-3: Fast and Accurate Attention with Asynchrony and Low-precision</h3>

<p><a href="https://arxiv.org/abs/2407.08608">paper</a></p>

<details>

<summary>Quick Summary</summary>
<div>

  </div>
</details>
<p><br /></p>

<h3 id="qwen-25-alibaba">Qwen 2.5 (Alibaba)</h3>

<ul>
  <li>Released in September 2024 as “the latest addition to the Qwen family,” which the developers called “the largest opensource release in history”</li>
  <li>Specialized variants for coding and mathematics</li>
  <li>Sizes ranging from 1.5B to 72B parameters</li>
  <li>Strong multilingual capabilities</li>
</ul>

<details>

<summary>Quick Summary</summary>
<div>

  </div>
</details>
<p><br /></p>

<h3 id="deepseek-25-deepseek">DeepSeek 2.5 (DeepSeek)</h3>

<ul>
  <li>Released in September 2024 combining “DeepSeek-V2-Chat and DeepSeek-Coder-V2-Instruct” as an “upgraded version”</li>
  <li>Competitive code generation capabilities</li>
  <li>Cost-effective alternative to larger models</li>
  <li>128K token context window</li>
</ul>

<details>

<summary>Quick Summary</summary>
<div>

  </div>
</details>
<p><br /></p>

<h3 id="claude-35-sonnet-anthropic">Claude 3.5 Sonnet (Anthropic)</h3>

<ul>
  <li>Released in October 2024 featuring improved performance “in undergraduate knowledge, graduate-level reasoning, general reasoning, and code generation”</li>
  <li>Advanced reasoning and coding capabilities</li>
  <li>Introduces Artifacts for interactive content creation</li>
  <li>Significant improvements over Claude 3 Opus</li>
</ul>

<details>

<summary>Quick Summary</summary>
<div>

  </div>
</details>
<p><br /></p>

<h3 id="deepseek-r1-deepseek">DeepSeek-R1 (DeepSeek)</h3>

<ul>
  <li>Specialized reasoning model released in December 2024</li>
  <li>Focus on mathematical and logical reasoning</li>
  <li>Designed to compete with OpenAI’s o1</li>
  <li>Significantly faster inference than o1</li>
</ul>

<details>

<summary>Quick Summary</summary>
<div>

  </div>
</details>
<p><br /></p>

<h3 id="phi-3">Phi 3</h3>

<p><a href="https://arxiv.org/pdf/2404.14219">paper</a></p>

<details>

<summary>Quick Summary</summary>
<div>
    <p>This paper introduces the <strong>Phi-3 family</strong> of compact yet highly capable language models, with the flagship <strong>phi-3-mini</strong> (3.8B parameters) achieving performance competitive with much larger models like Mixtral 8x7B and GPT-3.5. The key breakthrough is that phi-3-mini can run locally on a phone while achieving 69% on MMLU and 8.38 on MT-bench.</p>

    <p><strong>Key Innovations from phi-1:</strong></p>

    <ul>
      <li><strong>Massive scale-up</strong>: From 1.3B to 3.8B parameters, trained on 3.3T tokens (vs 7B tokens for phi-1)</li>
      <li><strong>Enhanced data recipe</strong>: Evolved the “textbook quality” approach with more sophisticated filtering and synthetic data generation</li>
      <li><strong>Model family</strong>: phi-3-small (7B), phi-3-medium (14B), and phi-3.5 variants including MoE and Vision models</li>
      <li><strong>Production ready</strong>: Full safety alignment, chat formatting, and mobile deployment</li>
    </ul>

    <p><strong>The “Data Optimal” Philosophy</strong>: Rather than following traditional compute-optimal scaling laws, they focus on curating the highest quality training data for a given model size - essentially asking “what’s the best data diet for a 4B parameter model?” rather than “how big should we make the model?”</p>

    <p><strong>Remarkable Results</strong>: A 3.8B model that fits on a phone outperforming models 10-25x larger, suggesting we may be far from the efficiency frontier that traditional scaling laws suggest.</p>

  </div>
</details>
<p><br /></p>

<h3 id="phi-4">Phi 4</h3>

<p><a href="https://arxiv.org/pdf/2412.08905">paper</a></p>

<details>

<summary>Quick Summary</summary>
<div>

    <p>This paper introduces <strong>Phi-4</strong>, a 14-billion parameter language model that achieves remarkable performance through a data-centric approach rather than simply scaling model size. Here are the key highlights:</p>

    <p><strong>Core Innovation: Quality over Scale</strong></p>

    <ul>
      <li><strong>Central Philosophy</strong>: Strategic use of high-quality synthetic data throughout training, moving beyond traditional web-scraped datasets</li>
      <li><strong>Architecture</strong>: 14B parameters with minimal changes from Phi-3, proving that data quality can rival compute scaling</li>
      <li><strong>Performance</strong>: Outperforms much larger models (even its teacher GPT-4o) on STEM reasoning tasks like GPQA and MATH</li>
    </ul>

    <p><strong>Three Pillars of Development</strong></p>

    <ol>
      <li><strong>Synthetic Data Generation</strong>: ~400B tokens using diverse techniques including multi-agent prompting, self-revision workflows, and instruction reversal</li>
      <li><strong>Curated Organic Data</strong>: Meticulous filtering of web content, books, and code repositories to extract high-reasoning seeds</li>
      <li><strong>Advanced Post-Training</strong>: Novel “Pivotal Token Search” (PTS) method for DPO that targets the most impactful tokens in reasoning chains</li>
    </ol>

    <p><strong>Standout Results</strong></p>

    <ul>
      <li><strong>Fresh Evaluation</strong>: Strong performance on November 2024 AMC-10/12 math competitions (completely contamination-proof)</li>
      <li><strong>STEM Excellence</strong>: Surpasses teacher model GPT-4o on graduate-level STEM (GPQA) and math competition problems (MATH)</li>
      <li><strong>Efficiency</strong>: Achieves frontier-model reasoning capabilities at a fraction of the computational cost</li>
    </ul>

    <p><strong>Mathematical Innovation</strong></p>

    <p>The paper introduces <strong>Pivotal Token Search</strong>, a fascinating approach that identifies tokens with the highest impact on solution success probability, creating more targeted preference optimization data.</p>

    <hr />

    <p><strong>What would you like to explore first?</strong> I’d be happy to dive deeper into:</p>

    <ul>
      <li>The mathematical formulation of Pivotal Token Search</li>
      <li>The synthetic data generation techniques and their theoretical foundations</li>
      <li>The training dynamics and curriculum design</li>
      <li>The contamination-proof evaluation methodology</li>
      <li>Any specific technical aspects that caught your attention</li>
    </ul>

  </div>
</details>
<p><br /></p>

<h3 id="self-play-fine-tuning-converts-weak-language-models-to-strong-language-models">Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models</h3>

<p><a href="https://arxiv.org/pdf/2401.01335">paper</a></p>

<details>

<summary>Quick Summary</summary>

</details>
<p><br /></p>

<h2 id="2025">2025</h2>

<h3 id="gemma-3">Gemma 3</h3>

<p><a href="https://arxiv.org/pdf/2503.19786">paper</a></p>

<details>

<summary>Quick Summary</summary>
<div>

    <p><strong>What it is</strong>: Gemma 3 represents a significant evolution in the Gemma family, introducing multimodal capabilities (vision + text), extended context length (128K tokens), and improved multilingual support while maintaining the lightweight, open-model philosophy.</p>

    <p><strong>Key Technical Innovations</strong>:</p>

    <ul>
      <li><strong>Hybrid attention architecture</strong>: A 5:1 ratio of local-to-global attention layers to manage long context memory efficiently</li>
      <li><strong>Multimodal integration</strong>: SigLIP vision encoder with Pan &amp; Scan method for flexible image resolutions</li>
      <li><strong>Enhanced post-training</strong>: Novel distillation and reinforcement learning techniques (BOND, WARM, WARP)</li>
    </ul>

    <p><strong>Model Sizes</strong>: 1B, 4B, 12B, and 27B parameters, designed for consumer hardware deployment</p>

    <p><strong>Performance Highlights</strong>: The 27B instruction-tuned model achieves an ELO score of 1338 on Chatbot Arena, ranking among top-10 models while being significantly smaller than competitors like LLaMA 3 405B.</p>

    <p><strong>Mathematical Focus Areas</strong>: The paper contains rich material on attention mechanisms, knowledge distillation formulations, memory optimization techniques, and scaling laws for multimodal training.</p>

    <p>This paper offers excellent opportunities to explore modern transformer architectures, efficient attention patterns, multimodal fusion techniques, and post-training optimization methods.</p>

  </div>
</details>
<p><br /></p>

<h3 id="llama-4">Llama 4</h3>

<h3 id="qwen25">Qwen2.5</h3>

<details>

<summary>Quick Summary</summary>
<div>

    <p>This paper introduces <strong>Qwen2.5</strong>, a comprehensive series of large language models representing a significant advancement over previous iterations. Here are the key highlights:</p>

    <p><strong>Core Improvements</strong></p>

    <ul>
      <li><strong>Massive data scaling</strong>: Pre-training dataset expanded from 7 trillion to <strong>18 trillion tokens</strong></li>
      <li><strong>Enhanced post-training</strong>: Sophisticated supervised fine-tuning with 1M+ samples plus multi-stage reinforcement learning (DPO + GRPO)</li>
      <li><strong>Extended capabilities</strong>: Generation length increased from 2K to 8K tokens, better structured data handling</li>
    </ul>

    <p><strong>Model Family</strong></p>

    <ul>
      <li><strong>Open-weight models</strong>: 0.5B, 1.5B, 3B, 7B, 14B, 32B, and 72B parameters</li>
      <li><strong>Proprietary MoE variants</strong>: Qwen2.5-Turbo and Qwen2.5-Plus for API services</li>
      <li><strong>Long-context version</strong>: Qwen2.5-Turbo supports up to 1M tokens</li>
    </ul>

    <p><strong>Performance Achievements</strong></p>

    <ul>
      <li><strong>Qwen2.5-72B-Instruct</strong> matches Llama-3-405B-Instruct performance while being ~5× smaller</li>
      <li>Strong improvements across mathematics, coding, reasoning, and multilingual tasks</li>
      <li>Competitive with GPT-4o-mini and GPT-4o for the MoE variants</li>
    </ul>

    <p><strong>Technical Innovations</strong></p>

    <ul>
      <li>Progressive context length scaling during training</li>
      <li>Advanced long-context techniques (YARN + DCA)</li>
      <li>Two-stage reinforcement learning (offline + online RL)</li>
      <li>Sophisticated data filtering using Qwen2-Instruct models</li>
    </ul>

    <p>The paper demonstrates how careful scaling of both data and training techniques can achieve remarkable efficiency gains, making state-of-the-art performance accessible with significantly fewer parameters.</p>

  </div>
</details>
<p><br /></p>

<h3 id="qwen-25-1m">Qwen 2.5-1M</h3>

<p><a href="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-1M/Qwen2_5_1M_Technical_Report.pdf">paper</a></p>

<details>

<summary>Quick Summary</summary>
<div>

  </div>
</details>
<p><br /></p>

<h3 id="qwen25-omni">Qwen2.5-Omni</h3>

<p><a href="https://github.com/QwenLM/Qwen2.5-Omni/blob/main/assets/Qwen2.5_Omni.pdf">paper</a></p>

<details>

<summary>Quick Summary</summary>
<div>

  </div>
</details>
<p><br /></p>

<h3 id="qwen-3">Qwen 3</h3>

<p><a href="https://arxiv.org/pdf/2505.09388">paper</a></p>

<details>

<summary>Quick Summary</summary>
<div>

  </div>
</details>
<p><br /></p>

<h3 id="grok">Grok</h3>

<ul>
  <li>Open-source model</li>
  <li>314B parameters</li>
  <li>
    <details>

</details>
  </li>
</ul>
<summary>Quick Summary</summary>
<div>

</div>
<p>&lt;/details&gt;
<br /></p>

<h3 id="pixtral">Pixtral</h3>

<p><a href="https://arxiv.org/abs/2410.07073">paper</a></p>

<ul>
  <li>Multimodal capabilities</li>
  <li>12B parameters</li>
</ul>

<details>

<summary>Quick Summary</summary>
<div>

  </div>
</details>
<p><br /></p>

<h3 id="large-language-diffusion-models">Large Language Diffusion Models</h3>

<p><a href="https://arxiv.org/pdf/2502.09992">paper</a></p>

<details>

<summary>Quick Summary</summary>
<div>

  </div>
</details>
<p><br /></p>

<p>https://hkunlp.github.io/blog/2025/dream/</p>

<p>https://arxiv.org/pdf/2502.09992</p>

<h2 id="rlvr">RLVR</h2>

<h2 id="kimi-ai">Kimi AI</h2>

<p>Talk about optimizers here, Muon and stuff</p>

<p>https://magazine.sebastianraschka.com/p/the-big-llm-architecture-comparison</p>

<h2 id="its-all-about-deepseek">It’s all about DeepSeek</h2>

<p>https://www.alphaxiv.org/abs/2505.09343</p>

<p>This whole section is dedicated just to the geniuses that are DeepSeek</p>

<p>Consider reading all their paper from this list https://huggingface.co/collections/Presidentlin/deepseek-papers-674c536aa6acddd9bc98c2ac</p>

<details>

<summary>Quick Summary</summary>
<div>

  </div>
</details>
<p><br /></p>

<h3 id="some-honorable-blogs-and-mentions-that-i-believe-you-should-definitely-check-out">Some Honorable blogs and mentions that I believe you should definitely check out:</h3>

<p><a href="https://www.darioamodei.com/essay/machines-of-loving-grace">blog</a></p>

<h2 id="self-notes-and-misc-stuff">SELF NOTES AND MISC STUFF</h2>

<p>Include Large Text diffusion models</p>

<p>Add performance charts showing scaling laws
Include architecture diagrams for key innovations
Create a “family tree” showing model lineage</p>

<p>NOTES TO SELF</p>

<ul>
  <li>Add a note for hardware, not in the scope of this blog but should not be ignored [DONE]</li>
  <li>Quick note about benchmark, Not hear to explain these but these are the major ones that are used mostly.[DONE]</li>
  <li>Under each paper, add the image of the paper with the name of the authors as well as the abstract[DONE]</li>
  <li>Train a hand drawn sketch LORA in flux dev for images</li>
  <li>Add a reference section in the end which redirects to the papers, Like latex reference and stuff.[DONE]</li>
</ul>

<p>[add prerequisites section, summary section, skip to section]</p>

<h2 id="a-short-introduction-to-llms">A short introduction to LLMs</h2>

<p>This part is highly influenced by this <a href="https://www.youtube.com/watch?v=7xTGNNLPyMI">video</a> by andrej karpathy</p>

<p>A paper on pretraining <a href="https://arxiv.org/pdf/2003.08271">paper</a></p>

<h3 id="training">Training</h3>

<p>As I mentioned there are 3 kinds of architectures when it comes to LLM, hence there is a different way of training each. Even though they share few similarities. Each has a different objective. Let us begin by first understanding the most popular LLM architecture, that being the decoder only architecture.</p>

<h4 id="decoder-only">Decoder Only</h4>

<h5 id="pretraining">Pretraining</h5>

<p><strong>Step 1: Data Collection</strong></p>

<p>We know Decoder based LLMs are able to answer a variety of answer to questions about science, food, mathematics, facts etc. That is because they have been trained on a huge plethora of the data.</p>

<p>So the first step is to collect the said data. One important thing to keep in mind is that these models are able to answer based on what they have seen. They are not performing any kind of reasoning. It is given a distribution, what is the most likely token that is to appear.</p>

<p>So if you want a coding LLM you will collect a lot of code related data from publically available places like github.</p>

<p>If you want a cooking LLM you will collected a lot of recipes and so on.</p>

<p>Most general purpose LLMs are trained on data collected from various sources, hence they are able to answer a lot of question.</p>

<p>A lot of filtering steps also goes behind it</p>

<p>”
Garbage in, garbage out
“</p>

<p>Most of the internet when crawled has data which looks something like this</p>

<div class="language-html highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nt">&lt;html&gt;</span>
  {add shit}
<span class="nt">&lt;/html&gt;</span>
</code></pre></div></div>

<p>Hence it needs to be processed into a more human readable form, You can imagine how humongous of a task it must be to clean huge datasets with GBs of data.</p>

<p>Now there are other filterning that also needs to be done, How do you take care of profanity? What about fake news and so on</p>

<p><strong>Step 2: Tokenization</strong></p>

<p>{maybe talk about vocabulary and token length}</p>

<p>When we talked about transformers, we skipped talking about tokenization, but as it is a vital piece of LLM training. We shall spend some time talking about it here.</p>

<p>Sentence level tokenization</p>

<p>My first question was, why do we need to break down words. Why not give the entire sentence. Heck why not give the whole paragraph as an input.</p>

<p>While in practice it can be done, it is not wise. Because if we go back to our basic principle about LLMs.</p>

<p>“They are next token predictors”</p>

<p>So in given any large enough paragraph or even an essay. The likelihood of a sentence repeating is very low. So if we think it in terms of machines, if we transform each sentence into a token, we will have a vocabular with a lot of numbers, which do not relate to each other at all. So we can never predict what sentence will come after any given sentence.</p>

<p>Word level tokenization</p>

<p>A simple work around this seems to be, well why not just tokenize the words, instead of sentences. Because in any large enough paragraph or essay. Words repeat and they follow a logical sequence of what is to appear next. For example</p>

<p>“Water is ___”, if you gave me this sentence word by word, I will assume the next word is wet. Whereas if you gave me a sentence</p>

<p>“Water is wet, This is a debatable topic.” I will have no clue what can be said after this sentence, Maybe someone raises a point, maybe someone says something else.</p>

<p>So word level helps us retain the logical sequence, and words have meanings to them too. But there is still one big issue. There can be millions of words, some have way higher representation in usual text and some are highly unlikely to occur in common place.</p>

<p>There are words which are commonplace in one industry and rare in another.</p>

<p>So we will have a huge vocabulary.</p>

<p>If you think for a moment we may come to the conclusion that, why not use character level tokenization to solve this problem, this will reduce the vocabulary drastically.</p>

<p>Here the problem would lie in the fact that characters by themselves do not hold much meaning (atleast in the english lexicon)</p>

<p><strong>Step 3: Training the network</strong></p>

<h5 id="fine-tuning">Fine-Tuning</h5>

<h4 id="encoder-only">Encoder Only</h4>

<p>Now let’s understand how a usual Encoder is trained, We will talk about BERT here</p>

<h4 id="encoder-decoder">Encoder Decoder</h4>

<p>Now let’s do the same for T5</p>

<h3 id="inference">Inference</h3>

<p>[Expand SENTENCE PIECE WHEN IT MAKES SENE]</p>

  <div class="voting-buttons" data-post-id="blogs-evolution-of-llms">
    <button class="vote-btn upvote" aria-label="Upvote">
      <i class="vote-icon fas fa-arrow-up"></i>
      <span class="vote-count upvote-count">0</span>
    </button>
    <button class="vote-btn downvote" aria-label="Downvote">
      <i class="vote-icon fas fa-arrow-down"></i>
      <span class="vote-count downvote-count">0</span>
    </button>
  </div>

  <!-- Social sharing section -->
  <div class="share-section mt-4">
    <h5 class="share-title">Share this post with your friends!!</h5>
    <div class="share-buttons">
      <a href="https://twitter.com/intent/tweet?url=https%3A%2F%2Fgoyalpramod.github.io%2Fblogs%2Fevolution_of_LLMs%2F&text=Evolution+of+LLMs" 
         class="share-btn" 
         target="_blank"
         rel="noopener noreferrer">
        <i class="fab fa-twitter"></i> Twitter
      </a>
      
      <a href="https://reddit.com/submit?url=https%3A%2F%2Fgoyalpramod.github.io%2Fblogs%2Fevolution_of_LLMs%2F&title=Evolution+of+LLMs" 
         class="share-btn" 
         target="_blank"
         rel="noopener noreferrer">
        <i class="fab fa-reddit"></i> Reddit
      </a>
      
      <a href="https://www.linkedin.com/sharing/share-offsite/?url=https%3A%2F%2Fgoyalpramod.github.io%2Fblogs%2Fevolution_of_LLMs%2F" 
         class="share-btn" 
         target="_blank"
         rel="noopener noreferrer">
        <i class="fab fa-linkedin"></i> LinkedIn
      </a>
      
      <a href="https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fgoyalpramod.github.io%2Fblogs%2Fevolution_of_LLMs%2F" 
         class="share-btn" 
         target="_blank"
         rel="noopener noreferrer">
        <i class="fab fa-facebook"></i> Facebook
      </a>
    </div>
  </div>

  <!-- Comments section -->
  
  <div class="comments-section mt-5">
    <h5 class="comments-title mb-4">Comments</h5>
    <script src="https://utteranc.es/client.js"
            repo="goyalpramod/goyalpramod.github.io"
            issue-term="pathname"
            label="💬 comments"
            theme="github-light"
            crossorigin="anonymous"
            async>
    </script>
  </div>
  

  <button id="back-to-top" class="back-to-top" aria-label="Back to top">
    <i class="fas fa-arrow-up"></i>
  </button>
</article>
      </div>
    </main><footer class="site-footer">
  <div class="wrapper">
    <div class="footer-row">
      <!-- Left side description -->
      <div class="footer-description">
        <p>A space for Machine Learning, Philosophy, Life, Food or whatever I fancy at the moment.</p>
      </div>

      <!-- Right side social links -->
      <div class="social-links">
        
        <a href="https://github.com/goyalpramod" target="_blank" aria-label="GitHub">
          <i class="fa-brands fa-github"></i>
        </a>
        
        
        
        <a href="https://twitter.com/goyal__pramod" target="_blank" aria-label="Twitter">
          <i class="fa-brands fa-x-twitter"></i>
        </a>
        
        
        
        <a href="https://linkedin.com/in/goyalpramod" target="_blank" aria-label="LinkedIn">
          <i class="fa-brands fa-linkedin"></i>
        </a>
        
        
        
        <a href="https://youtube.com/@goyal_pramod" target="_blank" aria-label="YouTube">
          <i class="fa-brands fa-youtube"></i>
        </a>
        
      </div>
    </div>

    <div class="footer-copyright">
      <p>&copy; 2025 Pramod's Blog</p>
    </div>
  </div>
</footer><!-- Bootstrap JS -->
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
    <!-- D3.js for visualizations -->
    <script src="https://d3js.org/d3.v7.min.js"></script>
    <!-- Chart.js for charts -->
    <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
    <!-- Copy Code Script -->
    <script src="/assets/scripts/copyCode.js"></script>
    <!-- Back to Top Script -->
    <script src="/assets/scripts/backToTop.js"></script>
    <!-- Table of Contents Script -->
    <script src="/assets/scripts/toc.js"></script>
    <script src="/assets/scripts/dark-mode.js"></script>

    <!-- Firebase --><script type="module">
    // Import the functions you need from the SDKs you need
    import { initializeApp } from "https://www.gstatic.com/firebasejs/11.2.0/firebase-app.js";
    import { getAnalytics } from "https://www.gstatic.com/firebasejs/11.2.0/firebase-analytics.js";
    // TODO: Add SDKs for Firebase products that you want to use
    // https://firebase.google.com/docs/web/setup#available-libraries
  
    // Your web app's Firebase configuration
    // For Firebase JS SDK v7.20.0 and later, measurementId is optional
    const firebaseConfig = {
      apiKey: "AIzaSyBJBUmmGWVR0KUYBiSJF-hLmWdJV1n3JX8",
      authDomain: "blog-5c804.firebaseapp.com",
      projectId: "blog-5c804",
      storageBucket: "blog-5c804.firebasestorage.app",
      messagingSenderId: "800555725459",
      appId: "1:800555725459:web:b9bfbb95b2cf89f3e0d366",
      measurementId: "G-3E5CXE5JTX"
    };
  
    // Initialize Firebase
    const app = initializeApp(firebaseConfig);
    const analytics = getAnalytics(app);

    // Export the initialized app
    window.firebaseApp = app;
</script><!-- Add voting.js script -->
    <script type="module" src="/assets/scripts/voting.js"></script>

    <script>
      document.addEventListener('DOMContentLoaded', function() {
        // More specific selector to target all links in content areas
        const contentLinks = document.querySelectorAll(`
          .note a,
          .latest-post-card a,
          .blog-post a,
          .thought-post a,
          article a
        `);
        
        contentLinks.forEach(link => {
          // Check if the link is pointing to an external site
          const isExternalLink = link.hostname !== window.location.hostname;
          
          // Only add target="_blank" to external links within content
          if (isExternalLink) {
            link.setAttribute('target', '_blank');
            link.setAttribute('rel', 'noopener noreferrer');
            link.classList.add('content-link');
          }
        });
      });
    </script>

  </body>

</html>