<!-- ---
layout: blog
title: Future blogs to write
date: 2025-01-3 12:00:00 +0530
categories: [personal, technology]
image: [add image]
---

- Evolution of LLMs
  - [Flash Attention Blog](https://gordicaleksa.medium.com/eli5-flash-attention-5c44017022ad)
  - [Flash Attention 3](https://tridao.me/blog/2024/flash3/)
  - [Mamba](https://tridao.me/blog/)
  - [Paper on fine-tuning](https://arxiv.org/abs/2408.13296v1)
  - [Scaling laws](https://arxiv.org/pdf/2001.08361)
  - [Fine-tune lora on CPU](https://rentry.org/cpu-lora)


- CUDA & optimising GPUs
  - [Understanding triton](https://isamu-website.medium.com/understanding-the-triton-tutorials-part-1-6191b59ba4c)
  - [Triton Documentation](https://triton-lang.org/main/getting-started/tutorials/01-vector-add.html#sphx-glr-getting-started-tutorials-01-vector-add-py)
  - [Reddit post on triton](https://www.reddit.com/r/OpenAI/comments/18nf310/openai_triton_coursetutorial_recommendations/)
  - [Daniels video on GPU mode](https://www.youtube.com/watch?v=hfb_AIhDYnA&ab_channel=GPUMODE)
  - [Blog series on CUDA by maharish](https://maharshi.bearblog.dev/blog/)
  - [Beating cuBLASS blog](https://salykova.github.io/)
  - [Lectures on CUDA and GPU stuff](https://www.youtube.com/@GPUMODE/videos)
  - 
- Superfast inference with vLLMs, triton etc/ quantization
  - [Fast lora implementation](https://github.dev/unslothai/unsloth)
  - [Code for different optimization from unsloth](https://github.com/unslothai/unsloth/tree/main/unsloth/kernels)
  - [4 bit flux](https://github.com/HighCWu/flux-4bit/blob/main/model.py)
  - [Quantized flux inference](https://gist.github.com/sayakpaul/05afd428bc089b47af7c016e42004527)
  - [unsloth wiki](https://github.com/unslothai/unsloth/wiki)
- Guide for distributed training and training multiple GPUs

- A guide to hacking LLMs

- Building a 2B model from scratch

- Building a vision model

- How do video gen models work

- unsupervised learning and RL, the dark horse of ML

- An MLE's guide to WEB DEVELOPMENT -->
